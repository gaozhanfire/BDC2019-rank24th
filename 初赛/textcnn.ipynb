{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"19F91B6AE5D148B28EFC30B340D0A989"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"44A9A480F69F4C2685239D0295F6DC3A"}},{"outputs":[],"execution_count":null,"source":"!killall python","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"B745C56B62784E62913E3C574DBD70C2","scrolled":false}},{"metadata":{"id":"9677A9A1AC1B4AAB9AD5D0797CAB746D","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[],"source":"import numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\n\r\n\r\ndef loader(train=r\"test_data_hava_query_title.csv\"):##################就是很简单的实现，先read_csv,然后填充缺失值为每列均值\r\n    all_data=pd.read_csv(train)\r\n    all_data=all_data[['query','title','title_count','title_ctr','label']]\r\n    mean_val = all_data['title_count'].mean()\r\n    all_data['title_count'].fillna(mean_val, inplace=True)\r\n    mean_val = all_data['title_ctr'].mean()\r\n    all_data['title_ctr'].fillna(mean_val, inplace=True)\r\n    data_split=StratifiedShuffleSplit(n_splits=2,test_size=0.02,random_state=1)\r\n    train_index,vaild_index=data_split.split(all_data['label'],all_data['label']).__next__()\r\n    train_data=all_data.iloc[train_index]\r\n    vaild_data=all_data.iloc[vaild_index]\r\n    return train_data,vaild_data\r\n\r\n############下面的函数返回一个迭代器，迭代一次产生一个batch\r\ndef batch_iter(data_x,data_y, batch_size, num_epochs, shuffle=True):#这个就是产生batch的，可以直接用\r\n    \"\"\"\r\n    Generates a batch iterator for a dataset.\r\n    \"\"\"\r\n    \r\n    data=list(zip(data_x,data_y))\r\n    data=np.array(data)\r\n    data_size = len(data)\r\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\r\n    \r\n    for epoch in range(num_epochs):\r\n        # Shuffle the data at each epoch\r\n        if shuffle:#如果为true 则代表允许随机取数据\r\n            shuffle_indices = np.random.permutation(np.arange(data_size))\r\n            shuffled_data = data[shuffle_indices]#随机取数据\r\n        else:\r\n            shuffled_data = data\r\n        for batch_num in range(num_batches_per_epoch):\r\n            start_index = batch_num * batch_size\r\n            end_index = min((batch_num + 1) * batch_size, data_size)\r\n            yield shuffled_data[start_index:end_index]#而不用一个个加到列表里了。这是一个batch，嗯哼\r\n            #取到的维度为（batchsize,2） y为(batchsize,1) 里面存的是序号\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nimport tensorflow as tf\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom gensim.models import Word2Vec\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.1, shape = shape)\r\n    return tf.Variable(initial)\r\ndef weight_variable(shape,name):\r\n    with tf.variable_scope('model',reuse=False):\r\n        initial = tf.get_variable(name , shape , initializer = tf.contrib.layers.variance_scaling_initializer())\r\n    return initial\r\ndef conv2d(x, W):\r\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = 'VALID')\r\n    # tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\r\n    # x(input)  : [batch, in_height, in_width, in_channels]\r\n    # W(filter) : [filter_height, filter_width, in_channels, out_channels]\r\n    # strides   : The stride of the sliding window for each dimension of input.\r\n    #             For the most common case of the same horizontal and vertices strides, strides = [1, stride, \r\ndef max_pool_2x2(x,ksize_input):\r\n    return tf.nn.max_pool(x, ksize =ksize_input,\r\n                          strides = [1, 1, 1, 1], padding = 'VALID')\r\n\r\n\r\ndef batchnorm( Ylogits, offset, convolutional=False):\r\n        \"\"\"batchnormalization.\r\n        Args:\r\n                Ylogits: 1D向量或者是3D的卷积结果。\r\n                num_updates: 迭代的global_step\r\n                offset：表示beta，全局均值；在 RELU 激活中一般初始化为 0.1。\r\n                scale：表示lambda，全局方差；在 sigmoid 激活中需要，这 RELU 激活中作用不大。\r\n                m: 表示batch均值；v:表示batch方差。\r\n                bnepsilon：一个很小的浮点数，防止除以 0.\r\n        Returns:\r\n                Ybn: 和 Ylogits 的维度一样，就是经过 Batch Normalization 处理的结果。\r\n                update_moving_everages：更新mean和variance，主要是给最后的 test 使用。\r\n        \"\"\"\r\n        \r\n        exp_moving_avg = tf.train.ExponentialMovingAverage(0.999,\r\n                                                                global_step)  # adding the iteration prevents from averaging across non-existing iterations\r\n        bnepsilon = 1e-5\r\n        if convolutional:\r\n                mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\r\n        else:\r\n                mean, variance = tf.nn.moments(Ylogits, [0])\r\n        update_moving_everages = exp_moving_avg.apply([mean, variance])\r\n        m = tf.cond(tst, lambda: exp_moving_avg.average(mean), lambda: mean)\r\n        v = tf.cond(tst, lambda: exp_moving_avg.average(variance), lambda: variance)\r\n        Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\r\n        return Ybn, update_moving_everages\r\n\r\n\r\nglobal_step=tf.Variable(0,name='global_step',trainable=False)\r\ndef ln_model(data) :\r\n    ###############输入是两维度，1、ctr 2、count\r\n    #得到数据的长度，即每个样例有多少个输入\r\n    len = 2#######################################这里需要改成你特征的维度数，即你特征的个数\r\n\r\n    #随机初始化参数\r\n    tf.set_random_seed(1)\r\n    with tf.variable_scope('ln_model',reuse=False):\r\n        W1 = tf.get_variable(\"W1\" , [len , 8] , initializer = tf.contrib.layers.variance_scaling_initializer())\r\n        b1 = bias_variable([8])\r\n        W2 = tf.get_variable(\"W2\" , [8 , 1] , initializer = tf.contrib.layers.variance_scaling_initializer())\r\n        b2 = bias_variable([1])\r\n\r\n    #前向传播：\r\n    Z1 = tf.nn.bias_add(tf.matmul(data , W1) , b1)\r\n    Z2 = tf.nn.bias_add(tf.matmul(Z1 , W2) , b2)\r\n    A = tf.nn.relu(Z2)\r\n\r\n\r\n    # 输出每个样本经过计算的值\r\n    output = tf.reshape(A, [-1, 1])\r\n    return output################出来一个[1,1]\r\n\r\n\r\ndef get_w2v_array(word_list,max_len):##################初始化一个固定长度的全零矩阵，超过这个长度就截断，\r\n    array = np.zeros((max_len, embedding_size))\r\n    if len(word_list)<=max_len:\r\n        for i in range(len(word_list)):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    else:\r\n        for i in range(max_len):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    return array\r\n\r\n###########################\r\nquery_len=6###################就是query和title 需要取一个固定长度。\r\ntitle_len=19\r\nembedding_size=120\r\n\r\nfilter_sizes = [2, 3, 4, 5]\r\n\r\nnum_filters=256\r\nfc_hidden_size=1024\r\n\r\nlr=0.006\r\ndecay_step=5000\r\n\r\nupdate_emas=[]\r\n\r\ninput_x_query=tf.placeholder(tf.float32,[None,query_len,embedding_size])#h r\r\ninput_x_title=tf.placeholder(tf.float32,[None,title_len,embedding_size])# r\r\ninput_ctr_fea=tf.placeholder(tf.float32,[None,2])# ctr特征\r\ninput_y=tf.placeholder(tf.float32,[None,1])#就是 标签\r\nkeep_prob = tf.placeholder(tf.float32, [])\r\ntst = tf.placeholder(tf.bool)\r\n\r\n# W_h = tf.Variable(\r\n#     tf.random_uniform([vocab_size_e, embedding_size], -1.0, 1.0),\r\n#     )#[None，1，10]\r\n# W_r = tf.Variable(\r\n#     tf.random_uniform([vocab_size_r, embedding_size], -1.0, 1.0),\r\n#     )#[None，1，10] h和r\r\n#########################################词向量部分 会替换成预训练词向量矩阵\r\n\r\n\r\nembedded_x_query = tf.expand_dims(input_x_query, -1)#-1代表最后一维\r\n\r\nembedded_x_title = tf.expand_dims(input_x_title, -1)#-1代表最后一维\r\n#####[None,title_len,embedding_size,1]##最后一维是通道\r\n##############################################################这上面分别换成query和title\r\n#过六个大小为[1,embedding_size,1,3]的卷积核\r\npooled_all=[]\r\n\r\n\r\nfor i,filter_size in enumerate(filter_sizes):\r\n    with tf.name_scope(\"conv1-query-%s\" % i):\r\n        filter_shape = [filter_size, embedding_size, 1, num_filters]\r\n        W=weight_variable(filter_shape,name=\"w_query\"+str(i))\r\n        b = bias_variable([num_filters])\r\n        print(embedded_x_query)\r\n        print(W,\"**********\")\r\n        conv=conv2d(embedded_x_query,W)#→[none,1,1,num_filters]\r\n        conv_bn, update_ema = batchnorm(conv, b, convolutional=True)  # 在激活层前面加 BN\r\n        update_emas.append(update_ema)\r\n        query_relu = tf.nn.relu(conv_bn, name=\"relu\")\r\n        ksize=[1,query_len - filter_size + 1,1,1]\r\n        pooled=max_pool_2x2(query_relu,ksize)\r\n        pooled_all.append(pooled)\r\nquery_pool = tf.concat(pooled_all,3)#→[none,1,1,num_filters*len(filter_sizes)]\r\nquery_pool_flat = tf.reshape(query_pool, [-1, len(filter_sizes)*num_filters])\r\npooled_all=[]\r\n\r\n\r\nfor i,filter_size in enumerate(filter_sizes):\r\n    with tf.name_scope(\"conv1-title-%s\" % i):\r\n        filter_shape = [filter_size, embedding_size, 1, num_filters]\r\n        W=weight_variable(filter_shape,name=\"w_title\"+str(i))\r\n        b = bias_variable([num_filters])\r\n        conv=conv2d(embedded_x_title,W)\r\n        conv_bn, update_ema = batchnorm(conv, b, convolutional=True)\r\n        update_emas.append(update_ema)\r\n        title_relu = tf.nn.relu(conv_bn, name=\"relu\")\r\n        ksize=[1,title_len - filter_size + 1,1,1]\r\n        pooled=max_pool_2x2(title_relu,ksize)\r\n        pooled_all.append(pooled)\r\ntitle_pool=tf.concat(pooled_all,axis=3)\r\ntitle_pool_flat = tf.reshape(title_pool, [-1, len(filter_sizes)*num_filters])\r\n\r\n\r\noutput = tf.concat([query_pool_flat, title_pool_flat], axis=1)\r\nW_fc = weight_variable([len(filter_sizes)*num_filters * 2, fc_hidden_size],name=\"fc\")\r\nh_fc = tf.matmul(output, W_fc, name='h_fc')\r\nbeta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[fc_hidden_size], name=\"beta_fc\"))\r\nfc_bn, update_ema_fc = batchnorm(h_fc, beta_fc, convolutional=False)\r\nupdate_emas.append(update_ema_fc)\r\nfc_bn_relu = tf.nn.relu(fc_bn, name=\"relu\")\r\nfc_bn_drop = tf.nn.dropout(fc_bn_relu, keep_prob)\r\n\r\n\r\nwith tf.variable_scope('out_layer'):\r\n    W_out = weight_variable([fc_hidden_size, 1],name=\"output\")\r\n    b_out = bias_variable([1])\r\n    _y_pred_nn = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name='y_pred')  # 每个类别的分数 scores\r\n    _y_pred_ln = ln_model(input_ctr_fea)###########data还得写\r\n    _y_pred_nn_ln=tf.add(_y_pred_nn , _y_pred_ln)\r\n    central_bias = tf.Variable([np.random.randn()], name=\"central_bias\")###初始化还得想想！！！！\r\n    _y_pred = tf.multiply(_y_pred_nn_ln, central_bias)\r\n\r\nwith tf.name_scope('loss'):\r\n\r\n    loss = tf.reduce_mean(\r\n        tf.nn.sigmoid_cross_entropy_with_logits(logits=_y_pred, labels=input_y))\r\n    \r\n    learning_rate = tf.train.exponential_decay(lr, global_step, decay_step,\r\n        0.8, staircase=True)\r\n    optimizer=tf.train.AdamOptimizer(learning_rate)\r\n    grads_and_vars=optimizer.compute_gradients(loss)\r\n    train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)\r\n\r\n    ###############################输入的名字 _y_inputs也得换 之类的！！！\r\n\r\n######剩下的，还需要看截断维度！！！！！！！！\r\n\r\n\r\n\r\nsess = tf.Session() #启动创建的模型\r\nsess.run(tf.initialize_all_variables())\r\n\r\n\r\n\r\n\r\ntrain_batches,vaild_data=loader()\r\ntrain_batches=np.array(train_batches)\r\ntrain_batches_y=train_batches[:,4]\r\ntrain_batches_x=np.delete(train_batches, 4, axis=1)######这里是除了label以外的维数\r\n\r\nbatches=batch_iter(train_batches_x,train_batches_y,1024,4)\r\n\r\nvaild_batches=np.array(vaild_data)\r\nvaild_batches_y=vaild_batches[:,4]\r\nvaild_batches_x=np.delete(vaild_batches, 4, axis=1)\r\nvaild_data=batch_iter(vaild_batches_x,vaild_batches_y,5000,100)\r\n\r\nprint(\"train starting\")\r\nw2v_model = Word2Vec.load('w2v_model/w2v_all_data_model.txt')\r\nword_wv= w2v_model.wv\r\n\r\n\r\nper_train_loss_avg_list=[]\r\nper_vaild_loss_avg_list=[]\r\nfor batch in batches:\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=pd.DataFrame(batch_x)\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],title_len),axis=1,arr=batch_x)\r\n\r\n    ######x[0]暂时代表query的位置 x[1]表示title的位置\r\n    batch_x_ctr_fea=batch_x[:,2:]######这里需要到时候调整，按label的位置和特征位置\r\n    \r\n    feed_dict={\r\n            input_x_query:batch_x_query,\r\n            input_x_title:batch_x_title,\r\n            input_ctr_fea:batch_x_ctr_fea,\r\n            input_y:batch_y,#这个就是label\r\n            tst:False,\r\n            keep_prob:0.8\r\n    }\r\n    \r\n    _,step,loss_out=sess.run([train_op,global_step,loss],feed_dict)\r\n    print(\"step {},loss:{:g}\".format(step,loss_out))\r\n    per_train_loss_avg_list.append(loss_out)\r\n    if step%(int(5900000/1024))==0:\r\n        print(\"batch ended\")\r\n    if step%50==0:\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        #####[None,title_len,embedding_size]\r\n        #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n        #query   title   title_count         title_ctr      label\r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],title_len),axis=1,arr=vaild_x)\r\n    \r\n        ######x[0]暂时代表query的位置 x[1]表示title的位置\r\n        vaild_x_ctr_fea=vaild_x[:,2:]######这里需要到时候调整，按label的位置和特征位置\r\n        feed_dict={\r\n                input_x_query:vaild_x_query,\r\n                input_x_title:vaild_x_title,\r\n                input_ctr_fea:vaild_x_ctr_fea,\r\n                input_y:vaild_y,#这个就是label\r\n                tst:False,\r\n                keep_prob:1\r\n        }\r\n        step,loss_out=sess.run([global_step,loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%500==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n        saver = tf.train.Saver(max_to_keep = 3)\r\n        saver = saver.save(sess, \"tf_model/model.ckpt\", global_step=step)\r\n\r\n    \r\n\r\n","execution_count":1},{"metadata":{"id":"DE5E0648EF0E4C748E0106610B006737","mdEditEnable":false},"cell_type":"markdown","source":"# 这个是如果上面代码跑着跑着看不到输出了，你可以直接执行下面的代码，接着上面的运行"},{"metadata":{"id":"D6B03D2B49334546B625595BC2689F03","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[],"source":"\r\n\r\nfor batch in batches:\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=pd.DataFrame(batch_x)\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],title_len),axis=1,arr=batch_x)\r\n\r\n    ######x[0]暂时代表query的位置 x[1]表示title的位置\r\n    batch_x_ctr_fea=batch_x[:,2:]######这里需要到时候调整，按label的位置和特征位置\r\n    \r\n    feed_dict={\r\n            input_x_query:batch_x_query,\r\n            input_x_title:batch_x_title,\r\n            input_ctr_fea:batch_x_ctr_fea,\r\n            input_y:batch_y,#这个就是label\r\n            tst:False,\r\n            keep_prob:0.8\r\n    }\r\n    \r\n    _,step,loss_out=sess.run([train_op,global_step,loss],feed_dict)\r\n    print(\"step {},loss:{:g}\".format(step,loss_out))\r\n    per_train_loss_avg_list.append(loss_out)\r\n    if step%(int(10000000/1024))==0:\r\n        print(\"batch ended\")\r\n    if step%50==0:\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        #####[None,title_len,embedding_size]\r\n        #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n        #query   title   title_count         title_ctr      label\r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],title_len),axis=1,arr=vaild_x)\r\n    \r\n        ######x[0]暂时代表query的位置 x[1]表示title的位置\r\n        vaild_x_ctr_fea=vaild_x[:,2:]######这里需要到时候调整，按label的位置和特征位置\r\n        feed_dict={\r\n                input_x_query:vaild_x_query,\r\n                input_x_title:vaild_x_title,\r\n                input_ctr_fea:vaild_x_ctr_fea,\r\n                input_y:vaild_y,#这个就是label\r\n                tst:False,\r\n                keep_prob:1\r\n        }\r\n        step,loss_out=sess.run([global_step,loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%500==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n        saver = tf.train.Saver(max_to_keep = 3)\r\n        saver = saver.save(sess, \"tf_model_1000W/model-1000W.ckpt\", global_step=step)\r\n\r\n    ","execution_count":null},{"metadata":{"id":"08274582C06E4B3285BA1858A191DA4D"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}