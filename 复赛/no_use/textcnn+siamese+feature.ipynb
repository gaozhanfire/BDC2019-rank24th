{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"FB18B3A525B045DC8FBA0B8C74A545E1"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"9A666D1D2B9C47DCAAD30E6EC18C06F1"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"D5A06C7E86444D65A20AF69984996D08"}},{"outputs":[],"execution_count":2,"source":"import pandas as pd\npd.read_pickle(\"/home/kesci/work/first_zzp/test3.pickle\")","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"52DEC4F28DC3456C8CF5451ED4D641A9","scrolled":false}},{"metadata":{"id":"3080251D16B84A31894A605255D3C568","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# -*- coding:utf-8 -*-\r\n\r\nimport os\r\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import rnn\r\n\r\nimport tensorflow.contrib.layers as layers\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\nfrom sklearn import preprocessing\r\nimport tensorflow as tf\r\nimport gc\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom gensim.models import Word2Vec\r\nfrom time import time\r\nimport numpy as np\r\n\r\n\r\n\r\nclass Settings(object):\r\n\r\n    def __init__(self):\r\n\r\n\r\n        self.model_name = 'wd_1_2_cnn_max'\r\n        self.query_len = 20\r\n        self.title_len = 20\r\n        self.filter_sizes = [1,2, 3, 4, 6]\r\n        self.n_filter = 350\r\n        self.fc_hidden_size = 1024\r\n        self.n_class = 1\r\n        self.train_batch_size=1024\r\n        self.vaild_batch_size=1024\r\n        self.lr=0.0015\r\n        self.decay_step=20000\r\n        self.decay_rate=0.8\r\n        self.rnn_hidden_units=170\r\n        self.dense_feature_num=0\r\n        self.ln_hidden_size = 128\r\n\r\n\r\n\r\n\r\n\r\nclass TEXTCNN(object):\r\n\r\n    def __init__(self,  settings):\r\n\r\n\r\n        self.model_name = settings.model_name\r\n        self.query_len = settings.query_len\r\n        self.title_len = settings.title_len\r\n        self.filter_sizes = settings.filter_sizes\r\n        self.n_filter = settings.n_filter\r\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\r\n        self.n_class = settings.n_class\r\n        self.rnn_hidden_units=settings.rnn_hidden_units\r\n        self.fc_hidden_size = settings.fc_hidden_size\r\n        self._global_step = tf.Variable(0, trainable=False, name='Global_Step')\r\n        self.update_emas = list()\r\n        self.dense_feature_num = settings.dense_feature_num\r\n        self.ln_hidden_size = settings.ln_hidden_size\r\n        # placeholders\r\n        self.is_training = tf.placeholder(tf.bool)\r\n        self._keep_prob = tf.placeholder(tf.float32, [])\r\n        self._batch_size = tf.placeholder(tf.int32, [])\r\n        self.embedding_size=100\r\n        \r\n        self.dnn_all=[]\r\n\r\n        with tf.name_scope('Inputs'):\r\n            self.embedded_x_query=tf.placeholder(tf.float32,[None,self.query_len,self.embedding_size])#h r\r\n            self.embedded_x_title=tf.placeholder(tf.float32,[None,self.title_len,self.embedding_size])# r\r\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name='y_input')\r\n            self.kv_features=tf.placeholder(shape=(None,self.dense_feature_num), dtype=tf.float32)\r\n\r\n\r\n        with tf.name_scope(\"ln_output\"):\r\n            output_ln = self.ln_foward(self.kv_features)\r\n            self.dnn_all.append(output_ln)\r\n\r\n        with tf.name_scope(\"cnn_output\"):\r\n            output_title = self.cnn_inference(self.embedded_x_query, self.query_len ,'query_bn')\r\n            output_title = tf.expand_dims(output_title, 0)\r\n\r\n            output_content = self.cnn_inference(self.embedded_x_title, self.title_len ,'title_bn')\r\n            output_content = tf.expand_dims(output_content, 0)\r\n            output_cnn = tf.concat([output_title, output_content], axis=0)\r\n            output_cnn = tf.reduce_max(output_cnn, axis=0)\r\n            self.dnn_all.append(output_cnn)\r\n\r\n        with tf.name_scope(\"rnn_output\"):\r\n            self.out1=self.BiRNN(self.embedded_x_query, self._keep_prob, \"side1\", self.embedding_size, self.query_len, self.rnn_hidden_units)\r\n            self.out2=self.BiRNN(self.embedded_x_title, self._keep_prob, \"side2\", self.embedding_size, self.title_len, self.rnn_hidden_units)\r\n            self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.out1,self.out2)),1,keep_dims=True))\r\n            self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.out1),1,keep_dims=True)),tf.sqrt(tf.reduce_sum(tf.square(self.out2),1,keep_dims=True))))\r\n            self.distance = tf.reshape(self.distance, [-1,1], name=\"distance\")\r\n\r\n        with tf.variable_scope('fc-bn-layer'):\r\n            self.dnn_all = tf.concat(self.dnn_all, axis=1)\r\n            dnn_bn = self.bn_layer_top(self.dnn_all,'dnn_bn_',self.is_training)\r\n\r\n            fn_input_size = int(dnn_bn.shape[-1])\r\n            W_fc = self.weight_variable([fn_input_size, self.fc_hidden_size], name='Weight_fc')\r\n            b_fc = self.bias_variable([self.fc_hidden_size], name='fc_bias')\r\n            fc_out = tf.nn.xw_plus_b(dnn_bn, W_fc, b_fc, name='y_pred')\r\n            fc_relu = tf.nn.relu(fc_out, name=\"fc_relu\")\r\n            fc_bn = self.bn_layer_top(fc_relu,'fc_bn_',self.is_training)\r\n            fc_drop = tf.nn.dropout(fc_bn, self.keep_prob)\r\n            \r\n            \r\n        with tf.variable_scope('out_layer'):\r\n            W_out = self.weight_variable([int(fc_drop.shape[-1]), self.n_class], name='Weight_out')\r\n            b_out = self.bias_variable([self.n_class], name='bias_out')\r\n            self._y_pred = tf.nn.xw_plus_b(fc_bn, W_out, b_out, name='y_pred')  # 每个类别的分数 scores\r\n            self._y_pred = self._y_pred+self.distance\r\n\r\n        with tf.name_scope('loss'):\r\n            self._loss = tf.reduce_mean(\r\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\r\n            self._y_pred = tf.sigmoid(self._y_pred )\r\n\r\n    @property\r\n    def tst(self):\r\n        return self._tst\r\n\r\n    @property\r\n    def keep_prob(self):\r\n        return self._keep_prob\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self._batch_size\r\n\r\n    @property\r\n    def global_step(self):\r\n        return self._global_step\r\n\r\n    @property\r\n    def X1_inputs(self):\r\n        return self._X1_inputs\r\n\r\n    @property\r\n    def X2_inputs(self):\r\n        return self._X2_inputs\r\n\r\n    @property\r\n    def y_inputs(self):\r\n        return self._y_inputs\r\n\r\n    @property\r\n    def y_pred(self):\r\n        return self._y_pred\r\n\r\n    @property\r\n    def loss(self):\r\n        return self._loss\r\n\r\n    def weight_variable(self, shape, name):\r\n        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial, name=name)\r\n\r\n    def bias_variable(self, shape, name):\r\n        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n        initial = tf.constant(0.1, shape=shape)\r\n        return tf.Variable(initial, name=name)\r\n\r\n\r\n    def cnn_inference(self, X_inputs, n_step ,bn_name):\r\n        \"\"\"TextCNN 模型。\r\n        Args:\r\n            X_inputs: tensor.shape=(batch_size, n_step)\r\n        Returns:\r\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\r\n        \"\"\"\r\n        inputs = X_inputs\r\n        inputs = tf.expand_dims(inputs, -1)\r\n        pooled_outputs = list()\r\n        for i, filter_size in enumerate(self.filter_sizes):\r\n            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\r\n                W_filter = self.weight_variable(shape=filter_shape, name='W_filter')\r\n                beta = self.bias_variable(shape=[self.n_filter], name='beta_filter')\r\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\r\n                conv = self.bn_layer_top(conv,bn_name,self.is_training)\r\n                h = tf.nn.relu(conv, name=\"relu\")\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\r\n                                        strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n        h_pool = tf.concat(pooled_outputs, 3)\r\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\r\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\r\n\r\n    def ln_foward(self,kv_features):\r\n        kv_features=self.bn_layer(kv_features,'ln_bn_',self.is_training)\r\n        input_size=int(kv_features.shape[-1])\r\n        with tf.variable_scope('ln_model',reuse=False):\r\n            W1 = tf.get_variable(\"W1\" , [input_size , self.ln_hidden_size] , initializer = tf.contrib.layers.variance_scaling_initializer())\r\n        Z1 = tf.matmul(kv_features , W1)\r\n        A1 = tf.nn.relu(Z1)\r\n        return A1\r\n\r\n\r\n\r\n    def BiRNN(self, x, dropout, scope, embedding_size, sequence_length, hidden_units):\r\n        n_hidden=hidden_units\r\n        n_layers=3\r\n        # Prepare data shape to match `static_rnn` function requirements\r\n        x = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))\r\n        # Define lstm cells with tensorflow\r\n        # Forward direction cell\r\n        with tf.name_scope(\"fw\"+scope),tf.variable_scope(\"fw\"+scope):\r\n            stacked_rnn_fw = []\r\n            for _ in range(n_layers):\r\n                fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\r\n                lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell,output_keep_prob=dropout)\r\n                stacked_rnn_fw.append(lstm_fw_cell)\r\n            lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\r\n\r\n        with tf.name_scope(\"bw\"+scope),tf.variable_scope(\"bw\"+scope):\r\n            stacked_rnn_bw = []\r\n            for _ in range(n_layers):\r\n                bw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\r\n                lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell,output_keep_prob=dropout)\r\n                stacked_rnn_bw.append(lstm_bw_cell)\r\n            lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\r\n        # Get lstm cell output\r\n\r\n        with tf.name_scope(\"bw\"+scope),tf.variable_scope(\"bw\"+scope):\r\n            outputs, _, _ = tf.nn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, x, dtype=tf.float32)\r\n        return outputs[-1]\r\n\r\n\r\n    def bn_layer(self,x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\r\n        with tf.variable_scope(scope, reuse=reuse):\r\n            shape = x.get_shape().as_list()\r\n            gamma = tf.get_variable(scope+\"_gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\r\n            beta = tf.get_variable(scope+\"_beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\r\n            moving_avg = tf.get_variable(scope+\"_moving_mean\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\r\n            moving_var = tf.get_variable(scope+\"_moving_variance\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\r\n            if is_training is not None:\r\n                avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\r\n                avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\r\n                var=tf.reshape(var, [var.shape.as_list()[-1]])\r\n                update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\r\n                update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\r\n                control_inputs = [update_moving_avg, update_moving_var]\r\n            else:\r\n                avg = moving_avg\r\n                var = moving_var\r\n                control_inputs = []\r\n            with tf.control_dependencies(control_inputs):\r\n                output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\r\n        return output\r\n\r\n    def bn_layer_top(self,x, scope, is_training, epsilon=0.001, decay=0.99):\r\n        return tf.cond(\r\n            is_training,\r\n            lambda: self.bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None),\r\n            lambda: self.bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True),\r\n        )\r\n\r\n\r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef loader(data_store_path=''):############\r\n    # all_data=pd.read_csv(data_store_path+\"data_sets/train_feature_second_no_query_and_title_data.csv\")\r\n    # query_title=pd.read_csv(data_store_path+\"data_sets/train_data.csv\",usecols=['query','title'])\r\n\r\n    print(\"开始加载数据，有ctr\")\r\n    # train_data=pd.read_pickle(data_store_path+\"train_feature_first_no_query_and_title_data.pickle\")[train_start_index:train_end_index].reset_index(drop=True)\r\n    train_data=pd.read_pickle(\"/home/kesci/train_temp.pickle\")\r\n    query_title=pd.read_pickle(\"/home/kesci/query_title_temp.pickle\")\r\n\r\n    train_data=train_data.drop(['label'],axis=1)\r\n    train_data=pd.concat([query_title,train_data],axis=1).reset_index(drop=True)\r\n    data_split=StratifiedShuffleSplit(n_splits=2,test_size=0.001,random_state=1)\r\n    \r\n    train_index,vaild_index=data_split.split(train_data['label'],train_data['label']).__next__()\r\n    train_data2=train_data.iloc[train_index]\r\n    vaild_data2=train_data.iloc[vaild_index]\r\n    print(\"加载数据完成\")\r\n    return train_data2,vaild_data2\r\n\r\ndef batch_iter(data_x,data_y, batch_size, num_epochs, shuffle=True):#这个就是产生batch的，可以直接用\r\n    \"\"\"\r\n    Generates a batch iterator for a dataset.\r\n    \"\"\"\r\n    # data_x=data_x.tolist()\r\n    data=list(zip(data_x,data_y))\r\n    data=np.array(data)\r\n    data_size = len(data)\r\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\r\n    for epoch in range(num_epochs):\r\n        # Shuffle the data at each epoch\r\n        if shuffle:#如果为true 则代表允许随机取数据\r\n            shuffle_indices = np.random.permutation(np.arange(data_size))\r\n            shuffled_data = data[shuffle_indices]#随机取数据\r\n        else:\r\n            shuffled_data = data\r\n        for batch_num in range(num_batches_per_epoch):\r\n            start_index = batch_num * batch_size\r\n            end_index = min((batch_num + 1) * batch_size, data_size)\r\n            yield shuffled_data[start_index:end_index]#而不用一个个加到列表里了。这是一个batch，嗯哼\r\n            #取到的维度为（batchsize,2） y为(batchsize,1) 里面存的是序号\r\n\r\ndef get_w2v_array(word_list,max_len):\r\n    array = np.zeros((max_len, 100))\r\n    if len(word_list)<=max_len:\r\n        for i in range(len(word_list)):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    else:\r\n        for i in range(max_len):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    return array\r\n\r\n\r\nsettings = Settings()\r\n\r\ntrain_batches,vaild_data=loader()\r\n\r\ntrain_batches=np.array(train_batches)\r\ntrain_batches_y=train_batches[:,2]\r\ntrain_batches_x=np.delete(train_batches, 2, axis=1)######这里是除了label以外的维数\r\n\r\n\r\nbatches=batch_iter(train_batches_x,train_batches_y,settings.train_batch_size,4)\r\n\r\nvaild_batches=np.array(vaild_data)\r\nvaild_batches_y=vaild_batches[:,2]\r\nvaild_batches_x=np.delete(vaild_batches, 2, axis=1)\r\nvaild_data=batch_iter(vaild_batches_x,vaild_batches_y,settings.vaild_batch_size,1000)\r\n\r\nprint(\"train starting\")\r\nw2v_model = Word2Vec.load('w2v_model3/w2v_all_data_model.txt')\r\nword_wv= w2v_model.wv\r\n\r\n\r\nsettings = Settings()\r\n\r\nsettings.dense_feature_num=train_batches_x.shape[1]-2###########自动设置数值型特征数量\r\nmodel = TEXTCNN( settings)\r\n\r\n\r\n\r\n\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    learning_rate = tf.train.exponential_decay(settings.lr, model.global_step, settings.decay_step,\r\n                                        settings.decay_rate, staircase=True)\r\n    optimizer = tf.train.AdamOptimizer(learning_rate)\r\n    grads_and_vars=optimizer.compute_gradients(model.loss)\r\n    train_op=optimizer.apply_gradients(grads_and_vars,global_step=model.global_step)\r\n\r\n\r\n\r\nconfig = tf.ConfigProto() \r\nconfig.gpu_options.allow_growth = True \r\nsess = tf.Session(config=config) #启动创建的模型\r\nvar_list = tf.trainable_variables()\r\ng_list = tf.global_variables()\r\nbn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]\r\nbn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]\r\nglobal_step_var=[g for g in g_list if 'Global_Step' in g.name]\r\nvar_list += bn_moving_vars\r\nvar_list += global_step_var\r\nsaver = tf.train.Saver(var_list)\r\nsess.run(tf.global_variables_initializer())\r\nsaver.restore(sess, tf.train.latest_checkpoint('/home/kesci/work/tf_model_textcnn_siamese_ln3/'))\r\n\r\nper_train_loss_avg_list=[]\r\nper_vaild_loss_avg_list=[]\r\nparts_time=[]\r\nfrom time import time\r\nfor batch in batches:\r\n    time1=time()\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=[i.tolist() for i in batch_x]\r\n    batch_x=pd.DataFrame(batch_x)\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\r\n    batch_x_ctr_fea=batch_x[:,2:]\r\n    \r\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \r\n    feed_dict = {model.embedded_x_query: batch_x_query,\r\n                 model.embedded_x_title: batch_x_title,\r\n                 model.y_inputs: batch_y,\r\n                 model.batch_size: batch_x.shape[0], \r\n                 model.is_training: True, \r\n                 model.keep_prob: 0.8,\r\n                 model.kv_features :batch_x_ctr_fea}\r\n\r\n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\r\n    time2=time()\r\n    parts_time.append(time2-time1)\r\n    if step%10==0:\r\n        print(\"step {},loss:{:g}\".format(step,loss_out))              \r\n    per_train_loss_avg_list.append(loss_out)\r\n    \r\n    \r\n    if step%(int(50000000/1004))==0:\r\n        print(\"batch ended\")\r\n    if step%500==0:\r\n        time_single_part=sum(parts_time)\r\n        ed_time=(50000000/settings.train_batch_size-step)/500*time_single_part\r\n        print(\"距离训练完一整轮剩余时间：\",ed_time/60,\" 分钟\")\r\n        parts_time=[]\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=[i.tolist() for i in vaild_x]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\r\n\r\n        vaild_x_ctr_fea=vaild_x[:,2:]\r\n\r\n        feed_dict = {model.embedded_x_query: vaild_x_query,\r\n                     model.embedded_x_title: vaild_x_title,\r\n                     model.y_inputs: vaild_y,\r\n                     model.batch_size: vaild_x.shape[0], \r\n                     model.is_training: False, \r\n                     model.keep_prob: 1,\r\n                     model.kv_features :vaild_x_ctr_fea}\r\n    \r\n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%5000==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n\r\n        var_list = tf.trainable_variables()\r\n        g_list = tf.global_variables()\r\n        bn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]\r\n        bn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]\r\n        global_step_var=[g for g in g_list if 'Global_Step' in g.name]\r\n        var_list += global_step_var\r\n        var_list += bn_moving_vars\r\n        saver = tf.train.Saver(var_list=var_list)\r\n        \r\n        saver = saver.save(sess, \"/home/kesci/work/tf_model_textcnn_siamese_ln3/rnn_fea\", global_step=step)\r\n\r\n\r\n\r\n\r\n","execution_count":1},{"metadata":{"id":"E35A56C972C24F76ADF2F9B24BD89A1B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"for batch in batches:\r\n    time1=time()\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=[i.tolist() for i in batch_x]\r\n    batch_x=pd.DataFrame(batch_x)\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\r\n    batch_x_ctr_fea=batch_x[:,2:]\r\n    \r\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \r\n    feed_dict = {model.embedded_x_query: batch_x_query,\r\n                 model.embedded_x_title: batch_x_title,\r\n                 model.y_inputs: batch_y,\r\n                 model.batch_size: batch_x.shape[0], \r\n                 model.is_training: True, \r\n                 model.keep_prob: 0.8,\r\n                 model.kv_features :batch_x_ctr_fea}\r\n\r\n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\r\n    time2=time()\r\n    parts_time.append(time2-time1)\r\n    if step%10==0:\r\n        print(\"step {},loss:{:g}\".format(step,loss_out))              \r\n    per_train_loss_avg_list.append(loss_out)\r\n    \r\n    \r\n    if step%(int(50000000/1004))==0:\r\n        print(\"batch ended\")\r\n    if step%500==0:\r\n        time_single_part=sum(parts_time)\r\n        ed_time=(50000000/settings.train_batch_size-step)/500*time_single_part\r\n        print(\"距离训练完一整轮剩余时间：\",ed_time/60,\" 分钟\")\r\n        parts_time=[]\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=[i.tolist() for i in vaild_x]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\r\n\r\n        vaild_x_ctr_fea=vaild_x[:,2:]\r\n\r\n        feed_dict = {model.embedded_x_query: vaild_x_query,\r\n                     model.embedded_x_title: vaild_x_title,\r\n                     model.y_inputs: vaild_y,\r\n                     model.batch_size: vaild_x.shape[0], \r\n                     model.is_training: False, \r\n                     model.keep_prob: 1,\r\n                     model.kv_features :vaild_x_ctr_fea}\r\n    \r\n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%5000==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n\r\n        var_list = tf.trainable_variables()\r\n        g_list = tf.global_variables()\r\n        bn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]\r\n        bn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]\r\n        global_step_var=[g for g in g_list if 'Global_Step' in g.name]\r\n        var_list += global_step_var\r\n        var_list += bn_moving_vars\r\n        saver = tf.train.Saver(var_list=var_list)\r\n        \r\n        saver = saver.save(sess, \"/home/kesci/work/tf_model_textcnn_siamese_ln2/rnn_fea\", global_step=step)\r\n\r\n\r\n","execution_count":1},{"metadata":{"id":"7BC6024508D340958738C96C0EA2C680","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[],"source":"# -*- coding:utf-8 -*-\r\n\r\nimport os\r\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import rnn\r\n\r\nimport tensorflow.contrib.layers as layers\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\nfrom sklearn import preprocessing\r\nimport tensorflow as tf\r\nimport gc\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom gensim.models import Word2Vec\r\nfrom time import time\r\nimport numpy as np\r\n\r\n\r\n\r\n\r\nclass Settings(object):\r\n\r\n    def __init__(self):\r\n\r\n\r\n        self.model_name = 'wd_1_2_cnn_max'\r\n        self.query_len = 20\r\n        self.title_len = 20\r\n        self.filter_sizes = [1,2, 3, 4, 6]\r\n        self.n_filter = 256\r\n        self.fc_hidden_size = 1024\r\n        self.n_class = 1\r\n        self.train_batch_size=1024\r\n        self.vaild_batch_size=1024\r\n        self.lr=0.0015\r\n        self.decay_step=20000\r\n        self.decay_rate=0.8\r\n        self.rnn_hidden_units=120\r\n        self.dense_feature_num=0\r\n        self.ln_hidden_size = 512\r\n\r\n\r\n\r\n\r\n\r\nclass TEXTCNN(object):\r\n\r\n    def __init__(self,  settings):\r\n\r\n\r\n        self.model_name = settings.model_name\r\n        self.query_len = settings.query_len\r\n        self.title_len = settings.title_len\r\n        self.filter_sizes = settings.filter_sizes\r\n        self.n_filter = settings.n_filter\r\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\r\n        self.n_class = settings.n_class\r\n        self.rnn_hidden_units=settings.rnn_hidden_units\r\n        self.fc_hidden_size = settings.fc_hidden_size\r\n        self._global_step = tf.Variable(0, trainable=False, name='Global_Step')\r\n        self.update_emas = list()\r\n        self.dense_feature_num = settings.dense_feature_num\r\n        self.ln_hidden_size = settings.ln_hidden_size\r\n        # placeholders\r\n        self.is_training = tf.placeholder(tf.bool)\r\n        self._keep_prob = tf.placeholder(tf.float32, [])\r\n        self._batch_size = tf.placeholder(tf.int32, [])\r\n        self.embedding_size=100\r\n        \r\n        self.dnn_all=[]\r\n\r\n        with tf.name_scope('Inputs'):\r\n            self.embedded_x_query=tf.placeholder(tf.float32,[None,self.query_len,self.embedding_size])#h r\r\n            self.embedded_x_title=tf.placeholder(tf.float32,[None,self.title_len,self.embedding_size])# r\r\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name='y_input')\r\n            self.kv_features=tf.placeholder(shape=(None,self.dense_feature_num), dtype=tf.float32)\r\n\r\n\r\n        with tf.name_scope(\"ln_output\"):\r\n            output_ln = self.ln_foward(self.kv_features)\r\n            self.dnn_all.append(output_ln)\r\n\r\n        with tf.name_scope(\"cnn_output\"):\r\n            output_title = self.cnn_inference(self.embedded_x_query, self.query_len ,'query_bn')\r\n            output_title = tf.expand_dims(output_title, 0)\r\n\r\n            output_content = self.cnn_inference(self.embedded_x_title, self.title_len ,'title_bn')\r\n            output_content = tf.expand_dims(output_content, 0)\r\n            output_cnn = tf.concat([output_title, output_content], axis=0)\r\n            output_cnn = tf.reduce_max(output_cnn, axis=0)\r\n            self.dnn_all.append(output_cnn)\r\n\r\n        with tf.name_scope(\"rnn_output\"):\r\n            self.out1=self.BiRNN(self.embedded_x_query, self._keep_prob, \"side1\", self.embedding_size, self.query_len, self.rnn_hidden_units)\r\n            self.out2=self.BiRNN(self.embedded_x_title, self._keep_prob, \"side2\", self.embedding_size, self.title_len, self.rnn_hidden_units)\r\n            self.distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(self.out1,self.out2)),1,keep_dims=True))\r\n            self.distance = tf.div(self.distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(self.out1),1,keep_dims=True)),tf.sqrt(tf.reduce_sum(tf.square(self.out2),1,keep_dims=True))))\r\n            self.distance = tf.reshape(self.distance, [-1,1], name=\"distance\")\r\n\r\n        with tf.variable_scope('fc-bn-layer'):\r\n            self.dnn_all = tf.concat(self.dnn_all, axis=1)\r\n            dnn_bn = self.bn_layer_top(self.dnn_all,'dnn_bn_',self.is_training)\r\n\r\n            fn_input_size = int(dnn_bn.shape[-1])\r\n            W_fc = self.weight_variable([fn_input_size, self.fc_hidden_size], name='Weight_fc')\r\n            b_fc = self.bias_variable([self.fc_hidden_size], name='fc_bias')\r\n            fc_out = tf.nn.xw_plus_b(dnn_bn, W_fc, b_fc, name='y_pred')\r\n            fc_relu = tf.nn.relu(fc_out, name=\"fc_relu\")\r\n            fc_bn = self.bn_layer_top(fc_relu,'fc_bn_',self.is_training)\r\n            fc_drop = tf.nn.dropout(fc_bn, self.keep_prob)\r\n            \r\n            \r\n        with tf.variable_scope('out_layer'):\r\n            W_out = self.weight_variable([int(fc_drop.shape[-1]), self.n_class], name='Weight_out')\r\n            b_out = self.bias_variable([self.n_class], name='bias_out')\r\n            self._y_pred = tf.nn.xw_plus_b(fc_bn, W_out, b_out, name='y_pred')  # 每个类别的分数 scores\r\n            self._y_pred = self._y_pred+self.distance\r\n\r\n        with tf.name_scope('loss'):\r\n            self._loss = tf.reduce_mean(\r\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\r\n            self._y_pred = tf.sigmoid(self._y_pred )\r\n\r\n    @property\r\n    def tst(self):\r\n        return self._tst\r\n\r\n    @property\r\n    def keep_prob(self):\r\n        return self._keep_prob\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self._batch_size\r\n\r\n    @property\r\n    def global_step(self):\r\n        return self._global_step\r\n\r\n    @property\r\n    def X1_inputs(self):\r\n        return self._X1_inputs\r\n\r\n    @property\r\n    def X2_inputs(self):\r\n        return self._X2_inputs\r\n\r\n    @property\r\n    def y_inputs(self):\r\n        return self._y_inputs\r\n\r\n    @property\r\n    def y_pred(self):\r\n        return self._y_pred\r\n\r\n    @property\r\n    def loss(self):\r\n        return self._loss\r\n\r\n    def weight_variable(self, shape, name):\r\n        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial, name=name)\r\n\r\n    def bias_variable(self, shape, name):\r\n        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n        initial = tf.constant(0.1, shape=shape)\r\n        return tf.Variable(initial, name=name)\r\n\r\n\r\n    def cnn_inference(self, X_inputs, n_step ,bn_name):\r\n        \"\"\"TextCNN 模型。\r\n        Args:\r\n            X_inputs: tensor.shape=(batch_size, n_step)\r\n        Returns:\r\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\r\n        \"\"\"\r\n        inputs = X_inputs\r\n        inputs = tf.expand_dims(inputs, -1)\r\n        pooled_outputs = list()\r\n        for i, filter_size in enumerate(self.filter_sizes):\r\n            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\r\n                W_filter = self.weight_variable(shape=filter_shape, name='W_filter')\r\n                beta = self.bias_variable(shape=[self.n_filter], name='beta_filter')\r\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\r\n                conv = self.bn_layer_top(conv,bn_name,self.is_training)\r\n                h = tf.nn.relu(conv, name=\"relu\")\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\r\n                                        strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n        h_pool = tf.concat(pooled_outputs, 3)\r\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\r\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\r\n\r\n    def ln_foward(self,kv_features):\r\n        kv_features=self.bn_layer(kv_features,'ln_bn_',self.is_training)\r\n        input_size=int(kv_features.shape[-1])\r\n        with tf.variable_scope('ln_model',reuse=False):\r\n            W1 = tf.get_variable(\"W1\" , [input_size , self.ln_hidden_size] , initializer = tf.contrib.layers.variance_scaling_initializer())\r\n        Z1 = tf.matmul(kv_features , W1)\r\n        A1 = tf.nn.relu(Z1)\r\n        return A1\r\n\r\n\r\n\r\n    def BiRNN(self, x, dropout, scope, embedding_size, sequence_length, hidden_units):\r\n        n_hidden=hidden_units\r\n        n_layers=3\r\n        # Prepare data shape to match `static_rnn` function requirements\r\n        x = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))\r\n        # Define lstm cells with tensorflow\r\n        # Forward direction cell\r\n        with tf.name_scope(\"fw\"+scope),tf.variable_scope(\"fw\"+scope):\r\n            stacked_rnn_fw = []\r\n            for _ in range(n_layers):\r\n                fw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\r\n                lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(fw_cell,output_keep_prob=dropout)\r\n                stacked_rnn_fw.append(lstm_fw_cell)\r\n            lstm_fw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_fw, state_is_tuple=True)\r\n\r\n        with tf.name_scope(\"bw\"+scope),tf.variable_scope(\"bw\"+scope):\r\n            stacked_rnn_bw = []\r\n            for _ in range(n_layers):\r\n                bw_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\r\n                lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(bw_cell,output_keep_prob=dropout)\r\n                stacked_rnn_bw.append(lstm_bw_cell)\r\n            lstm_bw_cell_m = tf.nn.rnn_cell.MultiRNNCell(cells=stacked_rnn_bw, state_is_tuple=True)\r\n        # Get lstm cell output\r\n\r\n        with tf.name_scope(\"bw\"+scope),tf.variable_scope(\"bw\"+scope):\r\n            outputs, _, _ = tf.nn.static_bidirectional_rnn(lstm_fw_cell_m, lstm_bw_cell_m, x, dtype=tf.float32)\r\n        return outputs[-1]\r\n\r\n\r\n    def bn_layer(self,x, scope, is_training, epsilon=0.001, decay=0.99, reuse=None):\r\n        with tf.variable_scope(scope, reuse=reuse):\r\n            shape = x.get_shape().as_list()\r\n            gamma = tf.get_variable(scope+\"_gamma\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=True)\r\n            beta = tf.get_variable(scope+\"_beta\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=True)\r\n            moving_avg = tf.get_variable(scope+\"_moving_mean\", shape[-1], initializer=tf.constant_initializer(0.0), trainable=False)\r\n            moving_var = tf.get_variable(scope+\"_moving_variance\", shape[-1], initializer=tf.constant_initializer(1.0), trainable=False)\r\n            if is_training is not None:\r\n                avg, var = tf.nn.moments(x, np.arange(len(shape)-1), keep_dims=True)\r\n                avg=tf.reshape(avg, [avg.shape.as_list()[-1]])\r\n                var=tf.reshape(var, [var.shape.as_list()[-1]])\r\n                update_moving_avg=tf.assign(moving_avg, moving_avg*decay+avg*(1-decay))\r\n                update_moving_var=tf.assign(moving_var, moving_var*decay+var*(1-decay))\r\n                control_inputs = [update_moving_avg, update_moving_var]\r\n            else:\r\n                avg = moving_avg\r\n                var = moving_var\r\n                control_inputs = []\r\n            with tf.control_dependencies(control_inputs):\r\n                output = tf.nn.batch_normalization(x, avg, var, offset=beta, scale=gamma, variance_epsilon=epsilon)\r\n        return output\r\n\r\n    def bn_layer_top(self,x, scope, is_training, epsilon=0.001, decay=0.99):\r\n        return tf.cond(\r\n            is_training,\r\n            lambda: self.bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=True, reuse=None),\r\n            lambda: self.bn_layer(x=x, scope=scope, epsilon=epsilon, decay=decay, is_training=False, reuse=True),\r\n        )\r\n\r\n\r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef loader(data_store_path=''):############\r\n    # all_data=pd.read_csv(data_store_path+\"data_sets/train_feature_second_no_query_and_title_data.csv\")\r\n    # query_title=pd.read_csv(data_store_path+\"data_sets/train_data.csv\",usecols=['query','title'])\r\n\r\n    print(\"开始加载数据，有ctr\")\r\n    # train_data=pd.read_pickle(data_store_path+\"train_feature_first_no_query_and_title_data.pickle\")[train_start_index:train_end_index].reset_index(drop=True)\r\n    train_data=pd.read_pickle(\"/home/kesci/blending/5000w-6000wall_fea_norm.pickle\")\r\n    query_title=pd.read_pickle(\"/home/kesci/blending/train_data5000w-6000w.pickle\")\r\n    \r\n\r\n    train_data=train_data.drop(['label'],axis=1)\r\n    train_data=pd.concat([query_title,train_data],axis=1).reset_index(drop=True)\r\n\r\n    print(\"加载数据完成\")\r\n    return train_data,1\r\n\r\ndef batch_iter(data_x,data_y, batch_size, num_epochs, shuffle=False):#这个就是产生batch的，可以直接用\r\n    \"\"\"\r\n    Generates a batch iterator for a dataset.\r\n    \"\"\"\r\n    # data_x=data_x.tolist()\r\n    data=list(zip(data_x,data_y))\r\n    data=np.array(data)\r\n    data_size = len(data)\r\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\r\n    for epoch in range(num_epochs):\r\n        # Shuffle the data at each epoch\r\n        if shuffle:#如果为true 则代表允许随机取数据\r\n            shuffle_indices = np.random.permutation(np.arange(data_size))\r\n            shuffled_data = data[shuffle_indices]#随机取数据\r\n        else:\r\n            shuffled_data = data\r\n        for batch_num in range(num_batches_per_epoch):\r\n            start_index = batch_num * batch_size\r\n            end_index = min((batch_num + 1) * batch_size, data_size)\r\n            yield shuffled_data[start_index:end_index]#而不用一个个加到列表里了。这是一个batch，嗯哼\r\n            #取到的维度为（batchsize,2） y为(batchsize,1) 里面存的是序号\r\n\r\ndef get_w2v_array(word_list,max_len):\r\n    array = np.zeros((max_len, 100))\r\n    if len(word_list)<=max_len:\r\n        for i in range(len(word_list)):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    else:\r\n        for i in range(max_len):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    return array\r\n\r\n\r\nsettings = Settings()\r\n\r\ntrain_batches,vaild_data=loader()\r\n\r\ntrain_batches=np.array(train_batches)\r\ntrain_batches_y=train_batches[:,2]\r\ntrain_batches_x=np.delete(train_batches, 2, axis=1)######这里是除了label以外的维数\r\n\r\n\r\nbatches=batch_iter(train_batches_x,train_batches_y,settings.train_batch_size,1)\r\n\r\n\r\n\r\nprint(\"train starting\")\r\nw2v_model = Word2Vec.load('w2v_model3/w2v_all_data_model.txt')\r\nword_wv= w2v_model.wv\r\n\r\n\r\nsettings = Settings()\r\n\r\nsettings.dense_feature_num=train_batches_x.shape[1]-2###########自动设置数值型特征数量\r\nmodel = TEXTCNN( settings)\r\n\r\n\r\n\r\n\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    learning_rate = tf.train.exponential_decay(settings.lr, model.global_step, settings.decay_step,\r\n                                        settings.decay_rate, staircase=True)\r\n    optimizer = tf.train.AdamOptimizer(learning_rate)\r\n    grads_and_vars=optimizer.compute_gradients(model.loss)\r\n    train_op=optimizer.apply_gradients(grads_and_vars,global_step=model.global_step)\r\n\r\nconfig = tf.ConfigProto() \r\nconfig.gpu_options.allow_growth = True \r\n\r\nsess = tf.Session(config=config) #启动创建的模型\r\nvar_list = tf.trainable_variables()\r\ng_list = tf.global_variables()\r\nbn_moving_vars = [g for g in g_list if 'moving_mean' in g.name]\r\nbn_moving_vars += [g for g in g_list if 'moving_variance' in g.name]\r\nglobal_step_var=[g for g in g_list if 'Global_Step' in g.name]\r\nvar_list += bn_moving_vars\r\nvar_list += global_step_var\r\nsaver = tf.train.Saver(var_list)\r\nsess.run(tf.global_variables_initializer())\r\nsaver.restore(sess, '/home/kesci/work/tf_model_textcnn_siamese_ln2/rnn_fea-220000')\r\n\r\n\r\nparts_time=[]\r\nfrom time import time\r\nresult_list=[]\r\nfor step,batch in enumerate(batches):\r\n    time1=time()\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=[i.tolist() for i in batch_x]\r\n    batch_x=pd.DataFrame(batch_x)\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\r\n    batch_x_ctr_fea=batch_x[:,2:]\r\n    \r\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \r\n    feed_dict = {model.embedded_x_query: batch_x_query,\r\n                 model.embedded_x_title: batch_x_title,\r\n                 model.batch_size: batch_x.shape[0], \r\n                 model.is_training: False, \r\n                 model.keep_prob: 1,\r\n                 model.kv_features :batch_x_ctr_fea}\r\n\r\n    result=sess.run([model._y_pred],feed_dict)\r\n    result_list.append(result)\r\n    time2=time()\r\n    parts_time.append(time2-time1)\r\n    if step%10==0:\r\n        print(result[0][0])  \r\n    if step%100==0:  \r\n        time_single_part=sum(parts_time)\r\n        ed_time=(20000000/settings.train_batch_size-step)/100*time_single_part\r\n        print(\"距预测完一整轮剩余时间：\",ed_time/60,\" 分钟\")\r\n        parts_time=[]\r\n    \r\n    ","execution_count":1},{"metadata":{"id":"2C05CB4F34FF46A3B5458CB63219A14A","collapsed":true,"scrolled":false},"cell_type":"code","outputs":[],"source":"result_list2=[pd.DataFrame(i[0]) for i in result_list]\ny_pred=pd.concat(result_list2).reset_index(drop=True)\ny_pred","execution_count":2},{"metadata":{"id":"D55665A7CCFF4EEEB4B56D924A21BCF0","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"y_pred.to_pickle(\"/home/kesci/test_old_result/siamese_fea_no_ctr.pickle\")","execution_count":4},{"metadata":{"id":"B3D05BB0ABE447D1AC710E59CE8AB74A","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[],"source":"data_r_path='/home/kesci/input/bytedance/test_final_part1.csv'\r\ncpu_num=16\r\nstart=0\r\nall_data_num=20000000\r\nnames_out=['query_id','query','query_title_id','title']\r\nusecols_out=['query_id','query_title_id']\r\nhas_head=False\r\nif has_head==False:\r\n    import multiprocessing\r\n    from time import time\r\n    import pandas as pd\r\n    all_data=[]\r\n    #test_final_part1.csv\r\n    def data_read(start,single_data_num,data_real_path):\r\n        data_out=pd.read_csv(data_real_path,header=None,names=names_out,usecols=usecols_out,skiprows=start,nrows=single_data_num)\r\n        return data_out\r\n    time1=time()\r\n    pool = multiprocessing.Pool(processes=cpu_num)\r\n    \r\n    for epoch in range(int(cpu_num)):\r\n        print(epoch)\r\n        single_data_num=int(all_data_num/cpu_num)\r\n        all_data.append(pool.apply_async(data_read, [start+single_data_num*epoch,single_data_num,data_r_path]))\r\n    # single_data_num=20000000/16\r\n    # all_data_num=20000000\r\n    # for epoch in range(int(all_data_num/single_data_num)):\r\n    #     all_data.append(pool.apply_async(data_read, [single_data_num*epoch,single_data_num,data_path+\"test_final_part1.csv\"]))\r\n    pool.close()\r\n    pool.join()\r\n    time2=time()\r\n    print(time2-time1)\r\n    all_data_pro=[single.get() for single in all_data]\r\n    time3=time()\r\n    print(time3-time2)\r\n    all_data_pro=pd.concat(all_data_pro)\r\n    test_data_pred_need=all_data_pro.reset_index(drop=True)\r\n\r\nresult=pd.concat([test_data_pred_need,y_pred],axis=1)\r\nresult.to_csv(\"first_zzp/result/sub_siamese_fea_no_ctr.csv\",header=None,index=None)#####合成三列做 最终的提交结果\r\nresult","execution_count":5},{"metadata":{"id":"351EC04489D04CF785FAA81687CD7728","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"!wget -O kesci_submit https://www.heywhale.com/kesci_submit&&chmod +x kesci_submit\n!https_proxy=\"http://klab-external-proxy\" ./kesci_submit -file /home/kesci/work/first_zzp/result/sub_t2.csv -token 02ada54c9760d3e1","execution_count":7},{"metadata":{"id":"1DDD5F8CDFC34437AA8648EC1B3E7FDE"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}