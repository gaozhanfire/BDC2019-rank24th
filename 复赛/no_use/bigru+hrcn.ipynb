{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"E793E167D8D447278D43FFCDD8EDD157"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"0176FACE99C54DAEA2991F039698929A"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"BDDAB2B6C78C4D738E8DA009875350A0"}},{"outputs":[],"execution_count":null,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"25378B40C80D4318BCA8ECA035D4264D"}},{"metadata":{"id":"212A769CC98D4710A06F56ED9B70B865","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"开始加载数据\n加载数据完成\ntrain starting\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","name":"stderr"},{"output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-1-92771f4375d4>:246: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From <ipython-input-1-92771f4375d4>:247: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\nInstructions for updating:\ndim is deprecated, use axis instead\nWARNING:tensorflow:From <ipython-input-1-92771f4375d4>:212: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\nWARNING:tensorflow:From <ipython-input-1-92771f4375d4>:121: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\nInstructions for updating:\nUse `tf.global_variables_initializer` instead.\nstep 10,loss:2.31439\nstep 20,loss:0.795758\nstep 30,loss:0.559962\nstep 40,loss:0.491966\nstep 50,loss:0.483351\nstep 60,loss:0.489116\nstep 70,loss:0.468662\nstep 80,loss:0.472852\nstep 90,loss:0.494204\nstep 100,loss:0.531546\nstep 110,loss:0.448226\nstep 120,loss:0.498237\nstep 130,loss:0.49694\nstep 140,loss:0.501911\nstep 150,loss:0.515829\nstep 160,loss:0.497761\nstep 170,loss:0.459991\nstep 180,loss:0.508972\nstep 190,loss:0.468498\nstep 200,loss:0.497193\nstep 210,loss:0.492086\nstep 220,loss:0.478903\nstep 230,loss:0.479084\nstep 240,loss:0.499323\nstep 250,loss:0.484812\nstep 260,loss:0.479765\nstep 270,loss:0.488924\nstep 280,loss:0.523876\nstep 290,loss:0.480433\nstep 300,loss:0.495011\nstep 310,loss:0.500896\nstep 320,loss:0.485072\nstep 330,loss:0.504869\nstep 340,loss:0.47691\nstep 350,loss:0.465447\nstep 360,loss:0.466243\nstep 370,loss:0.487035\nstep 380,loss:0.456379\nstep 390,loss:0.462315\nstep 400,loss:0.497852\nstep 410,loss:0.465543\nstep 420,loss:0.491394\nstep 430,loss:0.460365\nstep 440,loss:0.469206\nstep 450,loss:0.517185\nstep 460,loss:0.471313\nstep 470,loss:0.487519\nstep 480,loss:0.48542\nstep 490,loss:0.486697\nstep 500,loss:0.480611\n距离训练完一整轮剩余时间： 574.7438740242272  分钟\nper_train_loss_avg: 0.5704800606369972 **********\nvaild_loss: 0.5575615\nstep 510,loss:0.525098\nstep 520,loss:0.444067\nstep 530,loss:0.515236\nstep 540,loss:0.479249\nstep 550,loss:0.498275\nstep 560,loss:0.467316\nstep 570,loss:0.463744\nstep 580,loss:0.487751\nstep 590,loss:0.514448\nstep 600,loss:0.521993\nstep 610,loss:0.471864\nstep 620,loss:0.483943\nstep 630,loss:0.455928\nstep 640,loss:0.460099\nstep 650,loss:0.460723\nstep 660,loss:0.478114\nstep 670,loss:0.480871\nstep 680,loss:0.446967\nstep 690,loss:0.475578\nstep 700,loss:0.479594\nstep 710,loss:0.468616\nstep 720,loss:0.475507\nstep 730,loss:0.436818\nstep 740,loss:0.483778\nstep 750,loss:0.458958\nstep 760,loss:0.483967\nstep 770,loss:0.438607\nstep 780,loss:0.457959\nstep 790,loss:0.490902\nstep 800,loss:0.463551\nstep 810,loss:0.49464\nstep 820,loss:0.463093\nstep 830,loss:0.472081\nstep 840,loss:0.507527\nstep 850,loss:0.439107\nstep 860,loss:0.463042\nstep 870,loss:0.475346\nstep 880,loss:0.484402\nstep 890,loss:0.458424\nstep 900,loss:0.455179\nstep 910,loss:0.486108\nstep 920,loss:0.468831\nstep 930,loss:0.484227\nstep 940,loss:0.474932\nstep 950,loss:0.457978\nstep 960,loss:0.483825\nstep 970,loss:0.487638\nstep 980,loss:0.464029\nstep 990,loss:0.518628\nstep 1000,loss:0.488181\n距离训练完一整轮剩余时间： 554.3437070233126  分钟\nper_train_loss_avg: 0.47580706721544264 **********\nvaild_loss: 0.4922899\nstep 1010,loss:0.500196\nstep 1020,loss:0.461283\nstep 1030,loss:0.495493\nstep 1040,loss:0.493555\nstep 1050,loss:0.484971\nstep 1060,loss:0.490526\nstep 1070,loss:0.499328\nstep 1080,loss:0.451967\nstep 1090,loss:0.474147\nstep 1100,loss:0.454075\nstep 1110,loss:0.458394\nstep 1120,loss:0.451708\nstep 1130,loss:0.48063\nstep 1140,loss:0.460357\nstep 1150,loss:0.482401\nstep 1160,loss:0.47364\nstep 1170,loss:0.472022\nstep 1180,loss:0.491166\nstep 1190,loss:0.478109\nstep 1200,loss:0.495519\nstep 1210,loss:0.46325\nstep 1220,loss:0.494866\nstep 1230,loss:0.487621\nstep 1240,loss:0.468525\nstep 1250,loss:0.48932\nstep 1260,loss:0.438798\nstep 1270,loss:0.46652\nstep 1280,loss:0.43544\nstep 1290,loss:0.480581\nstep 1300,loss:0.471195\nstep 1310,loss:0.462796\nstep 1320,loss:0.449271\nstep 1330,loss:0.472778\nstep 1340,loss:0.465442\nstep 1350,loss:0.458433\nstep 1360,loss:0.479258\nstep 1370,loss:0.469083\nstep 1380,loss:0.461791\nstep 1390,loss:0.462751\nstep 1400,loss:0.496647\nstep 1410,loss:0.472917\nstep 1420,loss:0.477107\nstep 1430,loss:0.450735\nstep 1440,loss:0.489823\nstep 1450,loss:0.459256\nstep 1460,loss:0.476259\nstep 1470,loss:0.463615\nstep 1480,loss:0.509615\nstep 1490,loss:0.448005\nstep 1500,loss:0.479739\n距离训练完一整轮剩余时间： 538.409614069884  分钟\nper_train_loss_avg: 0.47377140164375303 **********\nvaild_loss: 0.46667635\nstep 1510,loss:0.501772\nstep 1520,loss:0.436907\nstep 1530,loss:0.47933\nstep 1540,loss:0.469135\nstep 1550,loss:0.463038\nstep 1560,loss:0.498894\nstep 1570,loss:0.464015\nstep 1580,loss:0.471281\nstep 1590,loss:0.5174\nstep 1600,loss:0.45468\nstep 1610,loss:0.501782\nstep 1620,loss:0.503418\nstep 1630,loss:0.453946\nstep 1640,loss:0.45774\nstep 1650,loss:0.478548\nstep 1660,loss:0.469722\nstep 1670,loss:0.452705\nstep 1680,loss:0.485788\nstep 1690,loss:0.438288\nstep 1700,loss:0.464623\nstep 1710,loss:0.4702\nstep 1720,loss:0.481894\nstep 1730,loss:0.491181\nstep 1740,loss:0.455044\nstep 1750,loss:0.454363\nstep 1760,loss:0.474977\nstep 1770,loss:0.455066\nstep 1780,loss:0.4548\nstep 1790,loss:0.475512\nstep 1800,loss:0.479007\nstep 1810,loss:0.479106\nstep 1820,loss:0.45353\nstep 1830,loss:0.465382\nstep 1840,loss:0.490317\nstep 1850,loss:0.474196\nstep 1860,loss:0.480982\nstep 1870,loss:0.461928\nstep 1880,loss:0.457275\nstep 1890,loss:0.447021\nstep 1900,loss:0.482956\nstep 1910,loss:0.466144\nstep 1920,loss:0.499785\nstep 1930,loss:0.496964\nstep 1940,loss:0.478479\nstep 1950,loss:0.459364\nstep 1960,loss:0.458886\nstep 1970,loss:0.440256\nstep 1980,loss:0.471372\nstep 1990,loss:0.467755\nstep 2000,loss:0.467162\n距离训练完一整轮剩余时间： 500.18120381869375  分钟\nper_train_loss_avg: 0.4704341569542885 **********\nvaild_loss: 0.46730825\nstep 2010,loss:0.481939\nstep 2020,loss:0.455904\nstep 2030,loss:0.461879\nstep 2040,loss:0.488114\nstep 2050,loss:0.466458\nstep 2060,loss:0.467918\nstep 2070,loss:0.471441\nstep 2080,loss:0.472447\nstep 2090,loss:0.463861\nstep 2100,loss:0.476728\nstep 2110,loss:0.436022\nstep 2120,loss:0.460139\nstep 2130,loss:0.462991\nstep 2140,loss:0.476205\nstep 2150,loss:0.483989\nstep 2160,loss:0.450607\nstep 2170,loss:0.50282\nstep 2180,loss:0.452851\nstep 2190,loss:0.469038\nstep 2200,loss:0.456896\nstep 2210,loss:0.459894\nstep 2220,loss:0.451065\nstep 2230,loss:0.470355\nstep 2240,loss:0.45827\nstep 2250,loss:0.498667\nstep 2260,loss:0.434038\nstep 2270,loss:0.503499\nstep 2280,loss:0.44066\nstep 2290,loss:0.446935\nstep 2300,loss:0.447172\nstep 2310,loss:0.480561\nstep 2320,loss:0.460081\nstep 2330,loss:0.438747\nstep 2340,loss:0.46137\nstep 2350,loss:0.440862\nstep 2360,loss:0.493746\nstep 2370,loss:0.457194\nstep 2380,loss:0.467375\nstep 2390,loss:0.432171\nstep 2400,loss:0.451282\nstep 2410,loss:0.464441\nstep 2420,loss:0.468265\nstep 2430,loss:0.468463\nstep 2440,loss:0.492003\nstep 2450,loss:0.447561\nstep 2460,loss:0.485973\nstep 2470,loss:0.465398\nstep 2480,loss:0.455026\nstep 2490,loss:0.489994\nstep 2500,loss:0.471346\n距离训练完一整轮剩余时间： 535.8368374320368  分钟\nper_train_loss_avg: 0.4687262308001518 **********\nvaild_loss: 0.4854087\nstep 2510,loss:0.473917\nstep 2520,loss:0.459888\nstep 2530,loss:0.449018\nstep 2540,loss:0.469589\nstep 2550,loss:0.468613\nstep 2560,loss:0.458\nstep 2570,loss:0.459904\nstep 2580,loss:0.459764\nstep 2590,loss:0.452934\nstep 2600,loss:0.467891\nstep 2610,loss:0.464251\nstep 2620,loss:0.43034\nstep 2630,loss:0.470831\nstep 2640,loss:0.465816\nstep 2650,loss:0.468663\nstep 2660,loss:0.456911\nstep 2670,loss:0.467861\nstep 2680,loss:0.46855\nstep 2690,loss:0.444785\nstep 2700,loss:0.486729\nstep 2710,loss:0.475283\nstep 2720,loss:0.458935\nstep 2730,loss:0.467974\nstep 2740,loss:0.458158\nstep 2750,loss:0.504516\nstep 2760,loss:0.461752\nstep 2770,loss:0.457972\nstep 2780,loss:0.507186\nstep 2790,loss:0.453816\nstep 2800,loss:0.445409\nstep 2810,loss:0.464012\nstep 2820,loss:0.480208\nstep 2830,loss:0.447193\nstep 2840,loss:0.520782\nstep 2850,loss:0.462889\nstep 2860,loss:0.483023\nstep 2870,loss:0.463562\nstep 2880,loss:0.478818\nstep 2890,loss:0.46129\nstep 2900,loss:0.453424\nstep 2910,loss:0.476357\nstep 2920,loss:0.512366\nstep 2930,loss:0.477843\nstep 2940,loss:0.470442\nstep 2950,loss:0.491793\nstep 2960,loss:0.464153\nstep 2970,loss:0.491539\nstep 2980,loss:0.470639\nstep 2990,loss:0.480877\nstep 3000,loss:0.485663\n距离训练完一整轮剩余时间： 528.6756991904228  分钟\nper_train_loss_avg: 0.468773298740387 **********\nvaild_loss: 0.46691078\nstep 3010,loss:0.449276\nstep 3020,loss:0.486323\nstep 3030,loss:0.484768\nstep 3040,loss:0.468624\nstep 3050,loss:0.462889\nstep 3060,loss:0.460175\nstep 3070,loss:0.487935\nstep 3080,loss:0.475379\nstep 3090,loss:0.478642\nstep 3100,loss:0.434417\nstep 3110,loss:0.456589\nstep 3120,loss:0.479985\nstep 3130,loss:0.512075\nstep 3140,loss:0.480271\nstep 3150,loss:0.500205\nstep 3160,loss:0.52213\nstep 3170,loss:0.48266\nstep 3180,loss:0.454998\nstep 3190,loss:0.463544\nstep 3200,loss:0.472449\nstep 3210,loss:0.490774\nstep 3220,loss:0.475208\nstep 3230,loss:0.479757\nstep 3240,loss:0.452254\nstep 3250,loss:0.432564\nstep 3260,loss:0.461612\nstep 3270,loss:0.451309\nstep 3280,loss:0.473359\nstep 3290,loss:0.460605\nstep 3300,loss:0.468137\nstep 3310,loss:0.463933\nstep 3320,loss:0.466816\nstep 3330,loss:0.478641\nstep 3340,loss:0.464892\nstep 3350,loss:0.432135\nstep 3360,loss:0.444334\nstep 3370,loss:0.478658\nstep 3380,loss:0.458163\nstep 3390,loss:0.501571\nstep 3400,loss:0.456419\nstep 3410,loss:0.467631\nstep 3420,loss:0.490327\nstep 3430,loss:0.441213\nstep 3440,loss:0.491077\nstep 3450,loss:0.494505\nstep 3460,loss:0.451539\nstep 3470,loss:0.463475\nstep 3480,loss:0.453543\nstep 3490,loss:0.446233\nstep 3500,loss:0.499997\n距离训练完一整轮剩余时间： 525.4388507835567  分钟\nper_train_loss_avg: 0.4682221249938011 **********\nvaild_loss: 0.47084033\nstep 3510,loss:0.448993\nstep 3520,loss:0.457136\nstep 3530,loss:0.45108\nstep 3540,loss:0.46933\nstep 3550,loss:0.482631\nstep 3560,loss:0.484877\nstep 3570,loss:0.441332\nstep 3580,loss:0.462695\nstep 3590,loss:0.459264\nstep 3600,loss:0.476625\nstep 3610,loss:0.491846\nstep 3620,loss:0.468104\nstep 3630,loss:0.475694\nstep 3640,loss:0.472676\nstep 3650,loss:0.465888\nstep 3660,loss:0.476718\nstep 3670,loss:0.493143\nstep 3680,loss:0.48188\nstep 3690,loss:0.470809\nstep 3700,loss:0.444644\nstep 3710,loss:0.463885\nstep 3720,loss:0.4464\nstep 3730,loss:0.462871\nstep 3740,loss:0.484254\nstep 3750,loss:0.457136\nstep 3760,loss:0.451167\nstep 3770,loss:0.461896\nstep 3780,loss:0.455006\nstep 3790,loss:0.495364\nstep 3800,loss:0.477474\nstep 3810,loss:0.452185\nstep 3820,loss:0.431279\nstep 3830,loss:0.465256\nstep 3840,loss:0.449591\nstep 3850,loss:0.469536\nstep 3860,loss:0.44631\nstep 3870,loss:0.4779\nstep 3880,loss:0.476449\nstep 3890,loss:0.467468\nstep 3900,loss:0.436605\nstep 3910,loss:0.464895\nstep 3920,loss:0.440302\nstep 3930,loss:0.474697\nstep 3940,loss:0.487871\nstep 3950,loss:0.475892\nstep 3960,loss:0.462578\nstep 3970,loss:0.460984\nstep 3980,loss:0.462414\nstep 3990,loss:0.488098\nstep 4000,loss:0.452896\n距离训练完一整轮剩余时间： 515.3595712631941  分钟\nper_train_loss_avg: 0.4642634689807892 **********\nvaild_loss: 0.4764783\nstep 4010,loss:0.466482\nstep 4020,loss:0.472703\nstep 4030,loss:0.466132\nstep 4040,loss:0.458814\nstep 4050,loss:0.469069\nstep 4060,loss:0.486903\nstep 4070,loss:0.448255\nstep 4080,loss:0.473771\nstep 4090,loss:0.469758\nstep 4100,loss:0.452416\nstep 4110,loss:0.500492\nstep 4120,loss:0.456981\nstep 4130,loss:0.502476\nstep 4140,loss:0.451336\nstep 4150,loss:0.467942\nstep 4160,loss:0.469832\nstep 4170,loss:0.437821\nstep 4180,loss:0.461912\nstep 4190,loss:0.433809\nstep 4200,loss:0.474077\nstep 4210,loss:0.482567\nstep 4220,loss:0.493176\nstep 4230,loss:0.452513\nstep 4240,loss:0.489919\nstep 4250,loss:0.456772\nstep 4260,loss:0.471217\nstep 4270,loss:0.463036\nstep 4280,loss:0.457078\nstep 4290,loss:0.457425\nstep 4300,loss:0.455333\nstep 4310,loss:0.47032\nstep 4320,loss:0.47436\nstep 4330,loss:0.441583\nstep 4340,loss:0.471644\nstep 4350,loss:0.505268\nstep 4360,loss:0.467097\nstep 4370,loss:0.469117\nstep 4380,loss:0.453747\nstep 4390,loss:0.470338\nstep 4400,loss:0.475989\nstep 4410,loss:0.468777\nstep 4420,loss:0.47437\nstep 4430,loss:0.447379\nstep 4440,loss:0.485676\nstep 4450,loss:0.480455\nstep 4460,loss:0.467005\nstep 4470,loss:0.481018\nstep 4480,loss:0.44802\nstep 4490,loss:0.434746\nstep 4500,loss:0.466003\n距离训练完一整轮剩余时间： 514.5649104764065  分钟\nper_train_loss_avg: 0.4644102423787117 **********\nvaild_loss: 0.46756485\nstep 4510,loss:0.445859\nstep 4520,loss:0.458798\nstep 4530,loss:0.481734\nstep 4540,loss:0.472992\nstep 4550,loss:0.460063\nstep 4560,loss:0.47936\nstep 4570,loss:0.464171\nstep 4580,loss:0.457313\nstep 4590,loss:0.470319\nstep 4600,loss:0.460052\nstep 4610,loss:0.452134\nstep 4620,loss:0.475399\nstep 4630,loss:0.462033\nstep 4640,loss:0.486404\nstep 4650,loss:0.487997\nstep 4660,loss:0.404349\nstep 4670,loss:0.466913\nstep 4680,loss:0.494603\nstep 4690,loss:0.476612\nstep 4700,loss:0.463111\nstep 4710,loss:0.451179\nstep 4720,loss:0.461967\nstep 4730,loss:0.450821\nstep 4740,loss:0.48814\nstep 4750,loss:0.46259\nstep 4760,loss:0.482125\nstep 4770,loss:0.463535\nstep 4780,loss:0.457266\nstep 4790,loss:0.468718\nstep 4800,loss:0.48274\nstep 4810,loss:0.459495\nstep 4820,loss:0.479461\nstep 4830,loss:0.46338\nstep 4840,loss:0.476361\nstep 4850,loss:0.460295\nstep 4860,loss:0.483057\nstep 4870,loss:0.481839\nstep 4880,loss:0.43064\nstep 4890,loss:0.44535\nstep 4900,loss:0.494797\nstep 4910,loss:0.479938\nstep 4920,loss:0.447459\nstep 4930,loss:0.479815\nstep 4940,loss:0.462113\nstep 4950,loss:0.484886\nstep 4960,loss:0.471095\nstep 4970,loss:0.457717\nstep 4980,loss:0.474592\nstep 4990,loss:0.489995\nstep 5000,loss:0.432144\n距离训练完一整轮剩余时间： 508.57325015217066  分钟\nper_train_loss_avg: 0.46480650985240934 **********\nvaild_loss: 0.47609493\nper_vaild_loss_avg: 0.4827133923768997 ************\nstep 5010,loss:0.485424\nstep 5020,loss:0.468644\nstep 5030,loss:0.473379\nstep 5040,loss:0.466685\nstep 5050,loss:0.469752\nstep 5060,loss:0.471277\nstep 5070,loss:0.492474\nstep 5080,loss:0.496955\nstep 5090,loss:0.455219\nstep 5100,loss:0.489498\nstep 5110,loss:0.476176\nstep 5120,loss:0.447057\nstep 5130,loss:0.440627\nstep 5140,loss:0.48886\nstep 5150,loss:0.448326\nstep 5160,loss:0.482744\nstep 5170,loss:0.449169\nstep 5180,loss:0.476484\nstep 5190,loss:0.478523\nstep 5200,loss:0.506539\nstep 5210,loss:0.451476\nstep 5220,loss:0.481567\nstep 5230,loss:0.474714\nstep 5240,loss:0.48681\nstep 5250,loss:0.485185\nstep 5260,loss:0.451004\nstep 5270,loss:0.480565\nstep 5280,loss:0.47726\nstep 5290,loss:0.44341\nstep 5300,loss:0.45455\nstep 5310,loss:0.447416\nstep 5320,loss:0.447453\nstep 5330,loss:0.470787\nstep 5340,loss:0.45048\nstep 5350,loss:0.466333\nstep 5360,loss:0.474217\nstep 5370,loss:0.417265\nstep 5380,loss:0.444393\nstep 5390,loss:0.469722\nstep 5400,loss:0.464263\nstep 5410,loss:0.476506\nstep 5420,loss:0.468907\nstep 5430,loss:0.442776\nstep 5440,loss:0.500297\nstep 5450,loss:0.495714\nstep 5460,loss:0.456379\nstep 5470,loss:0.4766\nstep 5480,loss:0.461002\nstep 5490,loss:0.465574\nstep 5500,loss:0.472196\n距离训练完一整轮剩余时间： 507.26520714412135  分钟\nper_train_loss_avg: 0.46578767722845077 **********\nvaild_loss: 0.46621948\nstep 5510,loss:0.439242\nstep 5520,loss:0.457464\nstep 5530,loss:0.451077\nstep 5540,loss:0.469073\nstep 5550,loss:0.491129\nstep 5560,loss:0.485983\nstep 5570,loss:0.474923\nstep 5580,loss:0.479607\nstep 5590,loss:0.448448\nstep 5600,loss:0.468906\nstep 5610,loss:0.482077\nstep 5620,loss:0.47464\nstep 5630,loss:0.444941\nstep 5640,loss:0.468125\nstep 5650,loss:0.461141\nstep 5660,loss:0.452563\nstep 5670,loss:0.41922\nstep 5680,loss:0.454763\nstep 5690,loss:0.448486\nstep 5700,loss:0.435414\nstep 5710,loss:0.437545\nstep 5720,loss:0.442708\nstep 5730,loss:0.467097\nstep 5740,loss:0.460239\nstep 5750,loss:0.471842\nstep 5760,loss:0.48562\nstep 5770,loss:0.465771\nstep 5780,loss:0.443188\nstep 5790,loss:0.454446\nstep 5800,loss:0.429277\nstep 5810,loss:0.477325\nstep 5820,loss:0.451867\nstep 5830,loss:0.437637\nstep 5840,loss:0.46354\nstep 5850,loss:0.488651\nstep 5860,loss:0.440123\nstep 5870,loss:0.477939\nstep 5880,loss:0.482646\nstep 5890,loss:0.485651\nstep 5900,loss:0.463265\nstep 5910,loss:0.465375\nstep 5920,loss:0.495124\nstep 5930,loss:0.458358\nstep 5940,loss:0.47098\nstep 5950,loss:0.457387\nstep 5960,loss:0.426632\nstep 5970,loss:0.452753\nstep 5980,loss:0.462121\nstep 5990,loss:0.442025\nstep 6000,loss:0.458797\n距离训练完一整轮剩余时间： 501.62719111976526  分钟\nper_train_loss_avg: 0.4628032082915306 **********\nvaild_loss: 0.46096495\nstep 6010,loss:0.41902\nstep 6020,loss:0.469224\nstep 6030,loss:0.459079\nstep 6040,loss:0.457835\nstep 6050,loss:0.477296\nstep 6060,loss:0.453534\nstep 6070,loss:0.444238\nstep 6080,loss:0.491758\nstep 6090,loss:0.448437\nstep 6100,loss:0.452155\nstep 6110,loss:0.431136\nstep 6120,loss:0.448069\nstep 6130,loss:0.484478\nstep 6140,loss:0.448384\nstep 6150,loss:0.480584\nstep 6160,loss:0.438262\nstep 6170,loss:0.438429\nstep 6180,loss:0.459569\nstep 6190,loss:0.471645\nstep 6200,loss:0.473607\nstep 6210,loss:0.462397\nstep 6220,loss:0.501227\nstep 6230,loss:0.486458\nstep 6240,loss:0.476692\nstep 6250,loss:0.492686\nstep 6260,loss:0.46953\nstep 6270,loss:0.46357\nstep 6280,loss:0.460863\nstep 6290,loss:0.452209\nstep 6300,loss:0.468215\nstep 6310,loss:0.460379\nstep 6320,loss:0.508386\nstep 6330,loss:0.471931\nstep 6340,loss:0.454521\nstep 6350,loss:0.456831\nstep 6360,loss:0.477367\nstep 6370,loss:0.474955\nstep 6380,loss:0.449837\nstep 6390,loss:0.507341\nstep 6400,loss:0.488388\nstep 6410,loss:0.4416\nstep 6420,loss:0.490131\nstep 6430,loss:0.483127\nstep 6440,loss:0.467158\nstep 6450,loss:0.429976\nstep 6460,loss:0.462609\nstep 6470,loss:0.475873\nstep 6480,loss:0.440726\nstep 6490,loss:0.492599\nstep 6500,loss:0.436962\n距离训练完一整轮剩余时间： 494.4416314456612  分钟\nper_train_loss_avg: 0.4644523940682411 **********\nvaild_loss: 0.4713122\nstep 6510,loss:0.455752\nstep 6520,loss:0.471879\nstep 6530,loss:0.443589\nstep 6540,loss:0.453612\nstep 6550,loss:0.500332\nstep 6560,loss:0.478153\nstep 6570,loss:0.455397\nstep 6580,loss:0.446969\nstep 6590,loss:0.468593\nstep 6600,loss:0.439313\nstep 6610,loss:0.413548\nstep 6620,loss:0.446887\nstep 6630,loss:0.424299\nstep 6640,loss:0.465441\nstep 6650,loss:0.467821\nstep 6660,loss:0.431033\nstep 6670,loss:0.469128\nstep 6680,loss:0.47844\nstep 6690,loss:0.461538\nstep 6700,loss:0.472957\nstep 6710,loss:0.485087\nstep 6720,loss:0.470515\nstep 6730,loss:0.467761\nstep 6740,loss:0.459572\n","name":"stdout"}],"source":"# -*- coding:utf-8 -*-\r\n\r\nimport os\r\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import rnn\r\n\r\nimport tensorflow.contrib.layers as layers\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\n\r\nimport tensorflow as tf\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom gensim.models import Word2Vec\r\n\r\n\"\"\"wd_5_bigru_cnn\r\n\r\n在论文 Recurrent Convolutional Neural Networks for Text Classification 中。\r\n\r\n使用 BiRNN 处理，将每个时刻的隐藏状态和原输入拼起来，在进行 max_pooling 操作。\r\n\r\n这里有些不同，首先也是使用 bigru 得到每个时刻的，将每个时刻的隐藏状态和原输入拼起来；\r\n\r\n然后使用输入到 TextCNN 网络中。\r\n\r\n\"\"\"\r\n\r\n\r\n\r\n\r\n\r\nclass Settings(object):\r\n\r\n    def __init__(self):\r\n\r\n\r\n        self.model_name = 'wd_1_2_cnn_max'\r\n        self.query_len = 20\r\n        self.title_len = 20\r\n        self.filter_sizes = 3\r\n        self.n_filter = 200\r\n        self.fc_hidden_size = 512\r\n        self.n_class = 1\r\n        self.train_batch_size=1024\r\n        self.vaild_batch_size=5000\r\n        self.lr=0.01\r\n        self.decay_step=7000\r\n        self.decay_rate=0.8\r\n        self.rnn_hidden_units_hrcn=120\r\n\r\n\r\n\r\n\r\n\r\n\r\nclass TEXTCNN(object):\r\n\r\n    def __init__(self,  settings):\r\n\r\n\r\n        self.model_name = settings.model_name\r\n        self.query_len = settings.query_len\r\n        self.title_len = settings.title_len\r\n        self.filter_sizes = settings.filter_sizes\r\n        self.n_filter = settings.n_filter\r\n        self.n_filter_total = self.n_filter \r\n        self.n_class = settings.n_class\r\n        self.rnn_hidden_units_hrcn=settings.rnn_hidden_units_hrcn\r\n        self.fc_hidden_size = settings.fc_hidden_size\r\n        self._global_step = tf.Variable(0, trainable=False, name='Global_Step')\r\n        self.update_emas = list()\r\n        # placeholders\r\n        self.embedding_size=100\r\n        self.initializer=tf.contrib.layers.xavier_initializer()\r\n        self.hidden_size=120\r\n        self.n_layer_bigru=1\r\n        \r\n\r\n        with tf.name_scope('Inputs'):\r\n            self.embedded_x_query=tf.placeholder(tf.float32,[None,self.query_len,self.embedding_size])#h r\r\n            self.embedded_x_title=tf.placeholder(tf.float32,[None,self.title_len,self.embedding_size])# r\r\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name='y_input')\r\n            self._keep_prob = tf.placeholder(tf.float32, [])\r\n            self._batch_size = tf.placeholder(tf.int32, [])\r\n            \r\n            self.keep_prob_embed = tf.placeholder(name='keep_prob', dtype=tf.float32)\r\n            self.keep_prob_fully = tf.placeholder(name='keep_prob', dtype=tf.float32)\r\n            self.keep_prob_ae = tf.placeholder(name='keep_prob', dtype=tf.float32)\r\n            self.training = tf.placeholder(name='training', dtype=tf.bool)\r\n\r\n        p = self.dropout(self.embedded_x_query, self.keep_prob_embed)\r\n        h = self.dropout(self.embedded_x_title, self.keep_prob_embed)\r\n        \r\n\r\n        with tf.variable_scope('bigru_text'):\r\n            output_title = self.bigru_inference(self.embedded_x_query)\r\n\r\n        with tf.variable_scope('bigru_content'):\r\n            output_content = self.bigru_inference(self.embedded_x_title)\r\n        \r\n        with tf.name_scope(\"rnn_output\"):\r\n            for i in range(2):\r\n                # BiLSTM\r\n                p_state, h_state = p, h\r\n                for j in range(2):\r\n                    with tf.variable_scope(f'p_lstm_{i}_{j}', reuse=None):\r\n                        p_state, _ = self.BiLSTM(tf.concat(p_state, axis=-1))\r\n                    with tf.variable_scope(f'p_lstm_{i}_{j}' + str(i), reuse=None):\r\n                        h_state, _ = self.BiLSTM(tf.concat(h_state, axis=-1))\r\n    \r\n                p_state = tf.concat(p_state, axis=-1)\r\n                h_state = tf.concat(h_state, axis=-1)\r\n                # attention\r\n                cosine = tf.divide(tf.matmul(p_state, tf.matrix_transpose(h_state)),\r\n                                   (tf.norm(p_state, axis=-1, keep_dims=True) * tf.norm(h_state, axis=-1, keep_dims=True)))\r\n                att_matrix = tf.nn.softmax(cosine)\r\n                p_attention = tf.matmul(att_matrix, h_state)\r\n                h_attention = tf.matmul(att_matrix, p_state)\r\n    \r\n                # DesNet\r\n                p = tf.concat((p, p_state, p_attention), axis=-1)\r\n                h = tf.concat((h, h_state, h_attention), axis=-1)\r\n    \r\n                # auto_encoder\r\n                p = tf.layers.dense(p, 200)\r\n                h = tf.layers.dense(h, 200)\r\n    \r\n                p = self.dropout(p, self.keep_prob_ae)\r\n                h = self.dropout(h, self.keep_prob_ae)\r\n    \r\n            # interaction and prediction layer\r\n            add = p + h\r\n            sub = p - h\r\n            norm = tf.norm(sub, axis=-1)\r\n            out = tf.concat((p, h, add, sub, tf.expand_dims(norm, axis=-1)), axis=-1)\r\n            out = tf.reshape(out, shape=(-1, out.shape[1] * out.shape[2]))\r\n            out = self.dropout(out, self.keep_prob_fully)\r\n    \r\n            out = tf.layers.dense(out, 500, activation='relu')\r\n            out = tf.layers.batch_normalization(out, training=self.training)\r\n            out = tf.layers.dense(out, 500, activation='relu')\r\n            out = tf.layers.batch_normalization(out, training=self.training)\r\n            out = tf.layers.dense(out, 500)\r\n            out = tf.layers.batch_normalization(out, training=self.training)\r\n            self.distance = tf.layers.dense(out, 1)\r\n\r\n        with tf.variable_scope('fc-bn-layer'):\r\n            output = tf.concat([output_title, output_content], axis=1)\r\n            W_fc = self.weight_variable([self.hidden_size * 4, self.fc_hidden_size], name='Weight_fc')\r\n            h_fc = tf.matmul(output, W_fc, name='h_fc')\r\n            beta_fc = tf.Variable(tf.constant(0.1, tf.float32, shape=[self.fc_hidden_size], name=\"beta_fc\"))\r\n            fc_bn=tf.layers.batch_normalization(h_fc, training=self.training)\r\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=\"relu\")\r\n\r\n        with tf.variable_scope('out_layer'):\r\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name='Weight_out')\r\n            b_out = self.bias_variable([self.n_class], name='bias_out')\r\n            self._y_pred = tf.nn.xw_plus_b(self.fc_bn_relu, W_out, b_out, name='y_pred')  # 每个类别的分数 scores\r\n            self._y_pred = self._y_pred+self.distance\r\n\r\n\r\n        with tf.name_scope('loss'):\r\n            self._loss = tf.reduce_mean(\r\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\r\n            self._y_pred = tf.sigmoid(self._y_pred)\r\n\r\n\r\n    @property\r\n    def keep_prob(self):\r\n        return self._keep_prob\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self._batch_size\r\n\r\n    @property\r\n    def global_step(self):\r\n        return self._global_step\r\n\r\n\r\n    @property\r\n    def y_inputs(self):\r\n        return self._y_inputs\r\n\r\n    @property\r\n    def y_pred(self):\r\n        return self._y_pred\r\n\r\n    @property\r\n    def loss(self):\r\n        return self._loss\r\n\r\n\r\n    def weight_variable(self, shape, name):\r\n        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial, name=name)\r\n\r\n    def bias_variable(self, shape, name):\r\n        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n        initial = tf.constant(0.1, shape=shape)\r\n        return tf.Variable(initial, name=name)\r\n\r\n\r\n    def BiLSTM(self, x):\r\n        fw_cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_hidden_units_hrcn)\r\n        bw_cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_hidden_units_hrcn)\r\n        return tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, x, dtype=tf.float32)\r\n        \r\n    def dropout(self, x, keep_prob):\r\n        return tf.nn.dropout(x, keep_prob)\r\n        \r\n    def gru_cell(self):\r\n        with tf.name_scope('gru_cell'):\r\n            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\r\n        return rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\r\n\r\n    def bi_gru(self, inputs):\r\n        \"\"\"build the bi-GRU network. 返回个所有层的隐含状态。\"\"\"\r\n        cells_fw = [self.gru_cell() for _ in range(self.n_layer_bigru)]\r\n        cells_bw = [self.gru_cell() for _ in range(self.n_layer_bigru)]\r\n        initial_states_fw = [cell_fw.zero_state(self.batch_size, tf.float32) for cell_fw in cells_fw]\r\n        initial_states_bw = [cell_bw.zero_state(self.batch_size, tf.float32) for cell_bw in cells_bw]\r\n        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, inputs,\r\n                                                            initial_states_fw=initial_states_fw,\r\n                                                            initial_states_bw=initial_states_bw, dtype=tf.float32)\r\n        return outputs\r\n\r\n    def task_specific_attention(self, inputs, output_size,\r\n                                initializer=layers.xavier_initializer(),\r\n                                activation_fn=tf.tanh, scope=None):\r\n        assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\r\n        with tf.variable_scope(scope or 'attention') as scope:\r\n            # u_w, attention 向量\r\n            attention_context_vector = tf.get_variable(name='attention_context_vector', shape=[output_size],\r\n                                                       initializer=initializer, dtype=tf.float32)\r\n            # 全连接层，把 h_i 转为 u_i ， shape= [batch_size, units, input_size] -> [batch_size, units, output_size]\r\n            input_projection = layers.fully_connected(inputs, output_size, activation_fn=activation_fn, scope=scope)\r\n            # 输出 [batch_size, units]\r\n            vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\r\n            attention_weights = tf.nn.softmax(vector_attn, dim=1)\r\n            tf.summary.histogram('attention_weigths', attention_weights)\r\n            weighted_projection = tf.multiply(inputs, attention_weights)\r\n            outputs = tf.reduce_sum(weighted_projection, axis=1)\r\n            return outputs  # 输出 [batch_size, hidden_size*2]\r\n\r\n\r\n    def bigru_inference(self, X_inputs):\r\n        inputs=X_inputs\r\n        output_bigru = self.bi_gru(inputs)\r\n        output_att = self.task_specific_attention(output_bigru, self.hidden_size*2)\r\n        return output_att\r\n\r\n\r\n\r\ndef loader(data_store_path='first_zzp/'):############\r\n    # all_data=pd.read_csv(data_store_path+\"data_sets/train_feature_second_no_query_and_title_data.csv\")\r\n    # query_title=pd.read_csv(data_store_path+\"data_sets/train_data.csv\",usecols=['query','title'])\r\n    print(\"开始加载数据\")\r\n    all_data=pd.read_pickle(data_store_path+\"train_data.pickle\")\r\n    data_split=StratifiedShuffleSplit(n_splits=2,test_size=0.001,random_state=1)\r\n    train_index,vaild_index=data_split.split(all_data['label'],all_data['label']).__next__()\r\n    train_data=all_data.iloc[train_index]\r\n    vaild_data=all_data.iloc[vaild_index]\r\n    print(\"加载数据完成\")\r\n    return train_data,vaild_data\r\n\r\ndef batch_iter(data_x,data_y, batch_size, num_epochs, shuffle=True):#这个就是产生batch的，可以直接用\r\n    \"\"\"\r\n    Generates a batch iterator for a dataset.\r\n    \"\"\"\r\n    # data_x=data_x.tolist()\r\n    data=list(zip(data_x,data_y))\r\n    data=np.array(data)\r\n    data_size = len(data)\r\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\r\n    for epoch in range(num_epochs):\r\n        # Shuffle the data at each epoch\r\n        if shuffle:#如果为true 则代表允许随机取数据\r\n            shuffle_indices = np.random.permutation(np.arange(data_size))\r\n            shuffled_data = data[shuffle_indices]#随机取数据\r\n        else:\r\n            shuffled_data = data\r\n        for batch_num in range(num_batches_per_epoch):\r\n            start_index = batch_num * batch_size\r\n            end_index = min((batch_num + 1) * batch_size, data_size)\r\n            yield shuffled_data[start_index:end_index]#而不用一个个加到列表里了。这是一个batch，嗯哼\r\n            #取到的维度为（batchsize,2） y为(batchsize,1) 里面存的是序号\r\n\r\ndef get_w2v_array(word_list,max_len):\r\n    array = np.zeros((max_len, 100))\r\n    if len(word_list)<=max_len:\r\n        for i in range(len(word_list)):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    else:\r\n        for i in range(max_len):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n    return array\r\n\r\n\r\nsettings = Settings()\r\n\r\ntrain_batches,vaild_data=loader()\r\n\r\ntrain_batches=np.array(train_batches)\r\ntrain_batches_y=train_batches[:,2]\r\ntrain_batches_x=np.delete(train_batches, 2, axis=1)######这里是除了label以外的维数\r\n\r\n\r\nbatches=batch_iter(train_batches_x,train_batches_y,settings.train_batch_size,4)\r\n\r\nvaild_batches=np.array(vaild_data)\r\nvaild_batches_y=vaild_batches[:,2]\r\nvaild_batches_x=np.delete(vaild_batches, 2, axis=1)\r\nvaild_data=batch_iter(vaild_batches_x,vaild_batches_y,settings.vaild_batch_size,1000)\r\n\r\nprint(\"train starting\")\r\nw2v_model = Word2Vec.load('w2v_model/w2v_all_data_model.txt')\r\nword_wv= w2v_model.wv\r\n\r\n\r\nsettings = Settings()\r\n\r\nsettings.all_features_num=train_batches_x.shape[1]-2###########自动设置数值型特征数量\r\nmodel = TEXTCNN( settings)\r\n\r\nlearning_rate = tf.train.exponential_decay(settings.lr, model.global_step, settings.decay_step,\r\n                                       settings.decay_rate, staircase=True)\r\noptimizer = tf.train.AdamOptimizer(learning_rate)\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    grads_and_vars=optimizer.compute_gradients(model.loss)\r\n    train_op=optimizer.apply_gradients(grads_and_vars,global_step=model.global_step)\r\nsess = tf.Session() #启动创建的模型\r\nsess.run(tf.initialize_all_variables())\r\n\r\n\r\n# saver.restore(sess, tf.train.latest_checkpoint('tf_model_rcnn/'))\r\nper_train_loss_avg_list=[]\r\nper_vaild_loss_avg_list=[]\r\nparts_time=[]\r\nfrom time import time\r\nfor batch in batches:\r\n    time1=time()\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=[i.tolist() for i in batch_x]\r\n    batch_x=pd.DataFrame(batch_x)\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\r\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \r\n    feed_dict = {model.embedded_x_query: batch_x_query,\r\n                 model.embedded_x_title: batch_x_title,\r\n                 model.y_inputs: batch_y,\r\n                 model.batch_size: batch_x.shape[0], \r\n                 model.keep_prob: 0.8,\r\n        \r\n                model.keep_prob_embed:0.8,\r\n                model.keep_prob_fully:0.8,\r\n                model.keep_prob_ae:0.5,\r\n                model.training:True,\r\n        \r\n    }\r\n\r\n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\r\n    time2=time()\r\n    parts_time.append(time2-time1)\r\n    if step%10==0:\r\n        print(\"step {},loss:{:g}\".format(step,loss_out))              \r\n    per_train_loss_avg_list.append(loss_out)\r\n    \r\n    \r\n    if step%(int(50000000/1004))==0:\r\n        print(\"batch ended\")\r\n    if step%500==0:\r\n        time_single_part=sum(parts_time)\r\n        ed_time=(50000000/settings.train_batch_size-step)/500*time_single_part\r\n        print(\"距离训练完一整轮剩余时间：\",ed_time/60,\" 分钟\")\r\n        parts_time=[]\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=[i.tolist() for i in vaild_x]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\r\n\r\n        feed_dict = {model.embedded_x_query: vaild_x_query,\r\n                     model.embedded_x_title: vaild_x_title,\r\n                     model.y_inputs: vaild_y,\r\n                     model.batch_size: vaild_x.shape[0], \r\n                     model.keep_prob: 1,\r\n\r\n                    model.keep_prob_embed:1,\r\n                    model.keep_prob_fully:1,\r\n                    model.keep_prob_ae:1,\r\n                    model.training:False,\r\n            \r\n            \r\n        }\r\n    \r\n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%5000==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n        saver = tf.train.Saver(max_to_keep = 3,var_list=tf.global_variables())\r\n        saver = saver.save(sess, \"/home/kesci/work/first_zzp/tf_model_bigru_hrcn/rnn\", global_step=step)","execution_count":1},{"metadata":{"id":"A95E0BDE60E643688A942199BF498D34","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"                     USER        PID ACCESS COMMAND\r\n/dev/nvidia-uvm:     root     kernel mount /dev/nvidia-uvm\r\n                     kesci      2501 F.... python\r\n/dev/nvidia-uvm-tools:\r\n                     root     kernel mount /dev/nvidia-uvm-tools\r\n/dev/nvidia0:        root     kernel mount /dev/nvidia0\r\n                     kesci      2501 F...m python\r\n/dev/nvidiactl:      root     kernel mount /dev/nvidiactl\r\n                     kesci      2501 F...m python\r\n","name":"stdout"}],"source":"!fuser -v /dev/nvidia*","execution_count":2},{"metadata":{"id":"1746DE75B91A4460ACEE1D25B13D5444","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"!kill -9 2501","execution_count":3},{"metadata":{"id":"BF4CEF9A4AB34E76996DA664BD8DD8A6","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"step 7300,loss:0.475182\nstep 7310,loss:0.48214\nstep 7320,loss:0.442797\nstep 7330,loss:0.464975\nstep 7340,loss:0.47957\nstep 7350,loss:0.463268\nstep 7360,loss:0.451333\nstep 7370,loss:0.443255\nstep 7380,loss:0.472878\nstep 7390,loss:0.475085\nstep 7400,loss:0.4738\nstep 7410,loss:0.512576\nstep 7420,loss:0.449731\nstep 7430,loss:0.427682\nstep 7440,loss:0.490056\nstep 7450,loss:0.432677\nstep 7460,loss:0.444967\nstep 7470,loss:0.457567\nstep 7480,loss:0.463213\nstep 7490,loss:0.465898\nstep 7500,loss:0.45297\n距离训练完一整轮剩余时间： 200.7374793558071  分钟\nper_train_loss_avg: 0.4633075876799757 **********\nvaild_loss: 0.45274818\nstep 7510,loss:0.484436\nstep 7520,loss:0.466174\nstep 7530,loss:0.452982\nstep 7540,loss:0.439087\nstep 7550,loss:0.494937\nstep 7560,loss:0.443573\nstep 7570,loss:0.467521\nstep 7580,loss:0.49258\nstep 7590,loss:0.443348\nstep 7600,loss:0.452766\nstep 7610,loss:0.498642\nstep 7620,loss:0.478847\nstep 7630,loss:0.495396\nstep 7640,loss:0.482614\nstep 7650,loss:0.444225\nstep 7660,loss:0.477744\nstep 7670,loss:0.497053\nstep 7680,loss:0.437524\nstep 7690,loss:0.463843\nstep 7700,loss:0.485017\nstep 7710,loss:0.502246\nstep 7720,loss:0.441151\nstep 7730,loss:0.452592\nstep 7740,loss:0.455205\nstep 7750,loss:0.436242\nstep 7760,loss:0.452925\nstep 7770,loss:0.457765\nstep 7780,loss:0.473674\nstep 7790,loss:0.452353\nstep 7800,loss:0.448685\nstep 7810,loss:0.473958\nstep 7820,loss:0.461711\nstep 7830,loss:0.467734\nstep 7840,loss:0.466025\nstep 7850,loss:0.427155\nstep 7860,loss:0.499794\nstep 7870,loss:0.461531\nstep 7880,loss:0.447334\nstep 7890,loss:0.460844\nstep 7900,loss:0.474236\nstep 7910,loss:0.43628\nstep 7920,loss:0.459185\nstep 7930,loss:0.439238\nstep 7940,loss:0.475876\nstep 7950,loss:0.449822\nstep 7960,loss:0.462231\nstep 7970,loss:0.471639\nstep 7980,loss:0.487447\nstep 7990,loss:0.483568\nstep 8000,loss:0.489338\n距离训练完一整轮剩余时间： 431.0908120065927  分钟\nper_train_loss_avg: 0.46392501455545426 **********\nvaild_loss: 0.44764346\nstep 8010,loss:0.480204\nstep 8020,loss:0.45999\nstep 8030,loss:0.451231\nstep 8040,loss:0.4789\nstep 8050,loss:0.453187\nstep 8060,loss:0.470598\nstep 8070,loss:0.484793\nstep 8080,loss:0.482523\nstep 8090,loss:0.482264\nstep 8100,loss:0.487997\nstep 8110,loss:0.483476\nstep 8120,loss:0.485893\nstep 8130,loss:0.449908\nstep 8140,loss:0.453562\nstep 8150,loss:0.447442\nstep 8160,loss:0.471509\nstep 8170,loss:0.450831\nstep 8180,loss:0.462714\nstep 8190,loss:0.492774\nstep 8200,loss:0.469702\nstep 8210,loss:0.470455\nstep 8220,loss:0.498206\nstep 8230,loss:0.447112\nstep 8240,loss:0.481067\nstep 8250,loss:0.455557\nstep 8260,loss:0.494277\nstep 8270,loss:0.459746\nstep 8280,loss:0.435428\nstep 8290,loss:0.449527\nstep 8300,loss:0.463328\nstep 8310,loss:0.478952\nstep 8320,loss:0.454591\nstep 8330,loss:0.450945\nstep 8340,loss:0.447597\nstep 8350,loss:0.449185\nstep 8360,loss:0.455526\nstep 8370,loss:0.486943\nstep 8380,loss:0.47764\nstep 8390,loss:0.482754\nstep 8400,loss:0.47328\nstep 8410,loss:0.474998\nstep 8420,loss:0.448851\nstep 8430,loss:0.466703\nstep 8440,loss:0.443707\nstep 8450,loss:0.461979\nstep 8460,loss:0.46628\nstep 8470,loss:0.449507\nstep 8480,loss:0.445202\nstep 8490,loss:0.456844\nstep 8500,loss:0.462244\n距离训练完一整轮剩余时间： 470.82542531204723  分钟\nper_train_loss_avg: 0.4643370591402054 **********\nvaild_loss: 0.47435078\nstep 8510,loss:0.45109\nstep 8520,loss:0.455754\nstep 8530,loss:0.485572\nstep 8540,loss:0.477128\nstep 8550,loss:0.4555\nstep 8560,loss:0.443042\nstep 8570,loss:0.488817\nstep 8580,loss:0.441604\nstep 8590,loss:0.460076\nstep 8600,loss:0.492567\nstep 8610,loss:0.44859\nstep 8620,loss:0.455635\nstep 8630,loss:0.496365\nstep 8640,loss:0.477772\nstep 8650,loss:0.478386\nstep 8660,loss:0.456972\nstep 8670,loss:0.477976\nstep 8680,loss:0.441446\nstep 8690,loss:0.461532\nstep 8700,loss:0.472037\nstep 8710,loss:0.458341\nstep 8720,loss:0.487647\nstep 8730,loss:0.464852\nstep 8740,loss:0.447338\nstep 8750,loss:0.465049\nstep 8760,loss:0.44081\nstep 8770,loss:0.490109\nstep 8780,loss:0.425366\nstep 8790,loss:0.483375\nstep 8800,loss:0.474136\nstep 8810,loss:0.461496\nstep 8820,loss:0.459155\nstep 8830,loss:0.455859\nstep 8840,loss:0.470473\nstep 8850,loss:0.477129\nstep 8860,loss:0.456566\nstep 8870,loss:0.478853\nstep 8880,loss:0.490238\nstep 8890,loss:0.436065\nstep 8900,loss:0.456069\nstep 8910,loss:0.478957\nstep 8920,loss:0.467597\nstep 8930,loss:0.450952\nstep 8940,loss:0.47037\nstep 8950,loss:0.44959\nstep 8960,loss:0.489255\nstep 8970,loss:0.459121\nstep 8980,loss:0.437327\nstep 8990,loss:0.479287\nstep 9000,loss:0.456733\n距离训练完一整轮剩余时间： 463.60397987626493  分钟\nper_train_loss_avg: 0.4630352481007576 **********\nvaild_loss: 0.46869463\nstep 9010,loss:0.460437\nstep 9020,loss:0.436848\nstep 9030,loss:0.456678\nstep 9040,loss:0.453763\nstep 9050,loss:0.456268\nstep 9060,loss:0.459124\nstep 9070,loss:0.4639\nstep 9080,loss:0.471066\nstep 9090,loss:0.520471\nstep 9100,loss:0.481557\nstep 9110,loss:0.453114\nstep 9120,loss:0.448123\nstep 9130,loss:0.455842\nstep 9140,loss:0.4505\nstep 9150,loss:0.482238\nstep 9160,loss:0.47259\nstep 9170,loss:0.448884\nstep 9180,loss:0.457596\nstep 9190,loss:0.449621\nstep 9200,loss:0.478297\nstep 9210,loss:0.435325\nstep 9220,loss:0.454608\nstep 9230,loss:0.463158\nstep 9240,loss:0.478158\nstep 9250,loss:0.454812\nstep 9260,loss:0.486094\nstep 9270,loss:0.487146\nstep 9280,loss:0.449383\nstep 9290,loss:0.459941\nstep 9300,loss:0.456814\nstep 9310,loss:0.452156\nstep 9320,loss:0.449324\nstep 9330,loss:0.450738\nstep 9340,loss:0.468483\nstep 9350,loss:0.474835\nstep 9360,loss:0.459343\nstep 9370,loss:0.513172\nstep 9380,loss:0.448121\nstep 9390,loss:0.472288\nstep 9400,loss:0.467749\nstep 9410,loss:0.487775\nstep 9420,loss:0.467472\nstep 9430,loss:0.458633\nstep 9440,loss:0.472198\nstep 9450,loss:0.442468\nstep 9460,loss:0.465158\nstep 9470,loss:0.465327\nstep 9480,loss:0.454075\nstep 9490,loss:0.465147\nstep 9500,loss:0.482556\n距离训练完一整轮剩余时间： 458.4848039764911  分钟\nper_train_loss_avg: 0.46184473860263825 **********\nvaild_loss: 0.47264028\nstep 9510,loss:0.45805\nstep 9520,loss:0.477718\nstep 9530,loss:0.46127\nstep 9540,loss:0.444397\nstep 9550,loss:0.442538\nstep 9560,loss:0.457239\nstep 9570,loss:0.461272\nstep 9580,loss:0.484707\nstep 9590,loss:0.462784\nstep 9600,loss:0.444106\nstep 9610,loss:0.443122\nstep 9620,loss:0.474896\nstep 9630,loss:0.482449\nstep 9640,loss:0.459928\nstep 9650,loss:0.466801\nstep 9660,loss:0.442463\nstep 9670,loss:0.453536\nstep 9680,loss:0.434561\nstep 9690,loss:0.438317\nstep 9700,loss:0.47489\nstep 9710,loss:0.462846\nstep 9720,loss:0.462698\nstep 9730,loss:0.466025\nstep 9740,loss:0.48477\nstep 9750,loss:0.449991\nstep 9760,loss:0.46371\nstep 9770,loss:0.483208\nstep 9780,loss:0.455101\nstep 9790,loss:0.441807\nstep 9800,loss:0.477167\nstep 9810,loss:0.48928\nstep 9820,loss:0.44414\nstep 9830,loss:0.459024\nstep 9840,loss:0.506569\nstep 9850,loss:0.457639\nstep 9860,loss:0.444546\nstep 9870,loss:0.469715\nstep 9880,loss:0.438335\nstep 9890,loss:0.46298\nstep 9900,loss:0.455332\nstep 9910,loss:0.446702\nstep 9920,loss:0.480525\nstep 9930,loss:0.476187\nstep 9940,loss:0.439884\nstep 9950,loss:0.460314\nstep 9960,loss:0.495515\nstep 9970,loss:0.465573\nstep 9980,loss:0.477491\nstep 9990,loss:0.49426\nstep 10000,loss:0.465128\n距离训练完一整轮剩余时间： 454.43649962730706  分钟\nper_train_loss_avg: 0.4632273114323616 **********\nvaild_loss: 0.47424117\nper_vaild_loss_avg: 0.4659743636846542 ************\nstep 10010,loss:0.485413\nstep 10020,loss:0.444617\nstep 10030,loss:0.447271\nstep 10040,loss:0.479558\nstep 10050,loss:0.451653\nstep 10060,loss:0.463704\nstep 10070,loss:0.462425\nstep 10080,loss:0.486497\nstep 10090,loss:0.472138\nstep 10100,loss:0.43511\nstep 10110,loss:0.469643\nstep 10120,loss:0.455921\nstep 10130,loss:0.45747\nstep 10140,loss:0.457209\nstep 10150,loss:0.466757\nstep 10160,loss:0.449014\nstep 10170,loss:0.461189\nstep 10180,loss:0.477145\nstep 10190,loss:0.448008\nstep 10200,loss:0.471702\nstep 10210,loss:0.447101\nstep 10220,loss:0.438986\nstep 10230,loss:0.465668\nstep 10240,loss:0.473886\nstep 10250,loss:0.471491\nstep 10260,loss:0.493143\nstep 10270,loss:0.447481\nstep 10280,loss:0.497457\nstep 10290,loss:0.464034\nstep 10300,loss:0.482169\nstep 10310,loss:0.470191\nstep 10320,loss:0.462163\nstep 10330,loss:0.433958\nstep 10340,loss:0.455016\nstep 10350,loss:0.447622\nstep 10360,loss:0.463879\nstep 10370,loss:0.469102\nstep 10380,loss:0.4717\nstep 10390,loss:0.442625\nstep 10400,loss:0.437358\nstep 10410,loss:0.473588\nstep 10420,loss:0.469785\nstep 10430,loss:0.450996\nstep 10440,loss:0.418989\nstep 10450,loss:0.466254\nstep 10460,loss:0.462054\nstep 10470,loss:0.478148\nstep 10480,loss:0.471959\nstep 10490,loss:0.449188\nstep 10500,loss:0.447083\n距离训练完一整轮剩余时间： 446.83924853292604  分钟\nper_train_loss_avg: 0.4640379847288132 **********\nvaild_loss: 0.46356162\nstep 10510,loss:0.46959\nstep 10520,loss:0.498378\nstep 10530,loss:0.472822\nstep 10540,loss:0.430755\nstep 10550,loss:0.459638\nstep 10560,loss:0.440285\nstep 10570,loss:0.42956\nstep 10580,loss:0.449527\nstep 10590,loss:0.430055\nstep 10600,loss:0.475923\nstep 10610,loss:0.46365\nstep 10620,loss:0.460812\nstep 10630,loss:0.480949\nstep 10640,loss:0.456917\nstep 10650,loss:0.447651\nstep 10660,loss:0.447647\nstep 10670,loss:0.447518\nstep 10680,loss:0.470661\nstep 10690,loss:0.468369\nstep 10700,loss:0.471856\nstep 10710,loss:0.463086\nstep 10720,loss:0.467143\nstep 10730,loss:0.493339\nstep 10740,loss:0.478476\nstep 10750,loss:0.481781\nstep 10760,loss:0.46748\nstep 10770,loss:0.470267\nstep 10780,loss:0.440319\nstep 10790,loss:0.441411\nstep 10800,loss:0.453986\nstep 10810,loss:0.479353\nstep 10820,loss:0.449234\nstep 10830,loss:0.452087\nstep 10840,loss:0.462618\nstep 10850,loss:0.451433\nstep 10860,loss:0.473296\nstep 10870,loss:0.48241\nstep 10880,loss:0.474018\nstep 10890,loss:0.485924\nstep 10900,loss:0.454511\nstep 10910,loss:0.466918\nstep 10920,loss:0.458073\nstep 10930,loss:0.474373\nstep 10940,loss:0.488343\nstep 10950,loss:0.409231\nstep 10960,loss:0.433867\nstep 10970,loss:0.486483\nstep 10980,loss:0.472679\nstep 10990,loss:0.452624\nstep 11000,loss:0.454363\n距离训练完一整轮剩余时间： 440.79143596328794  分钟\nper_train_loss_avg: 0.4638241785764694 **********\nvaild_loss: 0.47291395\nstep 11010,loss:0.452456\nstep 11020,loss:0.454515\nstep 11030,loss:0.458285\nstep 11040,loss:0.500314\nstep 11050,loss:0.460627\nstep 11060,loss:0.488527\nstep 11070,loss:0.453465\nstep 11080,loss:0.475333\nstep 11090,loss:0.472711\nstep 11100,loss:0.452878\nstep 11110,loss:0.448182\nstep 11120,loss:0.475204\nstep 11130,loss:0.474384\nstep 11140,loss:0.502478\nstep 11150,loss:0.453261\nstep 11160,loss:0.462738\nstep 11170,loss:0.442452\nstep 11180,loss:0.48111\nstep 11190,loss:0.482882\nstep 11200,loss:0.481572\nstep 11210,loss:0.492591\nstep 11220,loss:0.421434\nstep 11230,loss:0.479724\nstep 11240,loss:0.450626\nstep 11250,loss:0.471304\nstep 11260,loss:0.480286\nstep 11270,loss:0.437875\nstep 11280,loss:0.459533\nstep 11290,loss:0.44923\nstep 11300,loss:0.432722\nstep 11310,loss:0.455597\nstep 11320,loss:0.473933\nstep 11330,loss:0.484976\nstep 11340,loss:0.485532\nstep 11350,loss:0.450723\nstep 11360,loss:0.467459\nstep 11370,loss:0.481892\nstep 11380,loss:0.492579\nstep 11390,loss:0.470656\nstep 11400,loss:0.48101\nstep 11410,loss:0.48103\nstep 11420,loss:0.463817\nstep 11430,loss:0.478348\nstep 11440,loss:0.482827\nstep 11450,loss:0.47278\nstep 11460,loss:0.459912\nstep 11470,loss:0.494943\nstep 11480,loss:0.452599\nstep 11490,loss:0.445867\nstep 11500,loss:0.500339\n距离训练完一整轮剩余时间： 436.6508226808161  分钟\nper_train_loss_avg: 0.46295232433080674 **********\nvaild_loss: 0.46440738\nstep 11510,loss:0.436366\nstep 11520,loss:0.479734\nstep 11530,loss:0.482965\nstep 11540,loss:0.440255\nstep 11550,loss:0.465161\nstep 11560,loss:0.460385\nstep 11570,loss:0.459617\nstep 11580,loss:0.490279\nstep 11590,loss:0.464036\nstep 11600,loss:0.443964\nstep 11610,loss:0.446929\nstep 11620,loss:0.484647\nstep 11630,loss:0.449641\nstep 11640,loss:0.45804\nstep 11650,loss:0.492764\nstep 11660,loss:0.455979\nstep 11670,loss:0.429618\nstep 11680,loss:0.489779\nstep 11690,loss:0.44694\nstep 11700,loss:0.455956\nstep 11710,loss:0.468352\nstep 11720,loss:0.45851\nstep 11730,loss:0.450835\nstep 11740,loss:0.440717\nstep 11750,loss:0.444866\nstep 11760,loss:0.457202\nstep 11770,loss:0.455244\nstep 11780,loss:0.448058\nstep 11790,loss:0.473594\n","name":"stdout"}],"source":"parts_time=[]\r\nfrom time import time\r\nfor batch in batches:\r\n    time1=time()\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=[i.tolist() for i in batch_x]\r\n    batch_x=pd.DataFrame(batch_x)\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\r\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \r\n    feed_dict = {model.embedded_x_query: batch_x_query,\r\n                 model.embedded_x_title: batch_x_title,\r\n                 model.y_inputs: batch_y,\r\n                 model.batch_size: batch_x.shape[0], \r\n                 model.keep_prob: 0.8,\r\n        \r\n                model.keep_prob_embed:0.8,\r\n                model.keep_prob_fully:0.8,\r\n                model.keep_prob_ae:0.5,\r\n                model.training:True,\r\n        \r\n    }\r\n\r\n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\r\n    time2=time()\r\n    parts_time.append(time2-time1)\r\n    if step%10==0:\r\n        print(\"step {},loss:{:g}\".format(step,loss_out))              \r\n    per_train_loss_avg_list.append(loss_out)\r\n    \r\n    \r\n    if step%(int(50000000/1004))==0:\r\n        print(\"batch ended\")\r\n    if step%500==0:\r\n        time_single_part=sum(parts_time)\r\n        ed_time=(50000000/settings.train_batch_size-step)/500*time_single_part\r\n        print(\"距离训练完一整轮剩余时间：\",ed_time/60,\" 分钟\")\r\n        parts_time=[]\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=[i.tolist() for i in vaild_x]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\r\n\r\n        feed_dict = {model.embedded_x_query: vaild_x_query,\r\n                     model.embedded_x_title: vaild_x_title,\r\n                     model.y_inputs: vaild_y,\r\n                     model.batch_size: vaild_x.shape[0], \r\n                     model.keep_prob: 1,\r\n\r\n                    model.keep_prob_embed:1,\r\n                    model.keep_prob_fully:1,\r\n                    model.keep_prob_ae:1,\r\n                    model.training:False,\r\n            \r\n            \r\n        }\r\n    \r\n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%5000==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n        saver = tf.train.Saver(max_to_keep = 3,var_list=tf.global_variables())\r\n        saver = saver.save(sess, \"/home/kesci/work/first_zzp/tf_model_bigru_hrcn/rnn\", global_step=step)","execution_count":2},{"metadata":{"id":"FD753F5F0A8740F18A9D525C5DF24D3C"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}