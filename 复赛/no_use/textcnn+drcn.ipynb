{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"6FA9EE0A8C3945B183CB364C1BC55F75"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"E38B51923102401697D8380468B0A474"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"FB4C093CCC0A4411BC6939A6144E36D1"}},{"outputs":[],"execution_count":null,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"02B7BEFFABA4411D887DC3207BD54FBF"}},{"metadata":{"id":"0BEE36574D824A68862E81F2F1311544","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"开始加载数据\n加载数据完成\ntrain starting\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","name":"stderr"},{"output_type":"stream","text":"WARNING:tensorflow:From <ipython-input-1-4665bb70f9bf>:248: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\nWARNING:tensorflow:From <ipython-input-1-4665bb70f9bf>:117: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\nInstructions for updating:\nUse `tf.global_variables_initializer` instead.\nstart\nstep 1,loss:1.24884\nstep 2,loss:7.94893\nstep 3,loss:11.1769\nstep 4,loss:7.45909\nstep 5,loss:3.62681\nstep 6,loss:2.00468\nstep 7,loss:2.09596\nstep 8,loss:2.82304\nstep 9,loss:3.84512\nstep 10,loss:3.55188\nstep 11,loss:3.75298\nstep 12,loss:3.15512\nstep 13,loss:2.67905\nstep 14,loss:2.52603\nstep 15,loss:2.43207\nstep 16,loss:2.50855\nstep 17,loss:2.22003\nstep 18,loss:1.85318\nstep 19,loss:1.73492\nstep 20,loss:1.65507\nstep 21,loss:1.47723\nstep 22,loss:1.75411\nstep 23,loss:1.37164\nstep 24,loss:1.04718\nstep 25,loss:1.77719\nstep 26,loss:1.13742\nstep 27,loss:0.795003\nstep 28,loss:0.735636\nstep 29,loss:0.674342\nstep 30,loss:0.66117\nstep 31,loss:0.675892\nstep 32,loss:0.715318\nstep 33,loss:0.524459\nstep 34,loss:0.710225\nstep 35,loss:0.581973\nstep 36,loss:0.487676\nstep 37,loss:0.544356\nstep 38,loss:0.595928\nstep 39,loss:0.592641\nstep 40,loss:0.641123\nstep 41,loss:0.641489\nstep 42,loss:0.615753\nstep 43,loss:0.651401\nstep 44,loss:0.58698\nstep 45,loss:0.632078\nstep 46,loss:0.542601\nstep 47,loss:0.503209\nstep 48,loss:0.506911\nstep 49,loss:0.481174\nstep 50,loss:0.51861\nper_train_loss_avg: 1.8695789194107055 **********\nvaild_loss: 5.1307254\nstep 51,loss:0.52827\nstep 52,loss:0.494918\nstep 53,loss:0.522421\nstep 54,loss:0.523861\nstep 55,loss:0.478773\nstep 56,loss:0.489493\nstep 57,loss:0.517284\nstep 58,loss:0.511714\nstep 59,loss:0.495047\nstep 60,loss:0.519245\nstep 61,loss:0.491507\nstep 62,loss:0.515328\nstep 63,loss:0.499486\nstep 64,loss:0.491476\nstep 65,loss:0.537013\nstep 66,loss:0.494512\nstep 67,loss:0.550529\nstep 68,loss:0.505398\nstep 69,loss:0.499714\nstep 70,loss:0.479906\nstep 71,loss:0.476283\nstep 72,loss:0.517523\nstep 73,loss:0.491254\nstep 74,loss:0.476801\nstep 75,loss:0.468451\nstep 76,loss:0.453796\nstep 77,loss:0.479139\nstep 78,loss:0.501246\nstep 79,loss:0.489339\nstep 80,loss:0.53059\nstep 81,loss:0.514567\nstep 82,loss:0.486473\nstep 83,loss:0.47709\nstep 84,loss:0.466124\nstep 85,loss:0.482803\nstep 86,loss:0.427798\nstep 87,loss:0.502469\nstep 88,loss:0.496473\nstep 89,loss:0.503669\nstep 90,loss:0.498069\nstep 91,loss:0.506869\nstep 92,loss:0.513848\nstep 93,loss:0.471044\nstep 94,loss:0.510963\nstep 95,loss:0.502138\nstep 96,loss:0.487942\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4665bb70f9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0mbatch_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                     mgr = _arrays_to_mgr(arrays, columns, index, columns,\n\u001b[0;32m--> 400\u001b[0;31m                                          dtype=dtype)\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                     mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7362\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7364\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4871\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4872\u001b[0;31m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4873\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4874\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mform_blocks\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4937\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ObjectBlock'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4938\u001b[0;31m         \u001b[0mobject_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_simple_blockify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ObjectBlock'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4939\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_simple_blockify\u001b[0;34m(tuples, dtype)\u001b[0m\n\u001b[1;32m   4980\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4982\u001b[0;31m     \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4983\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3198\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3199\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_block_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3201\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDatetimeTZBlock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetimetz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget_block_type\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m   3174\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetimetz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3175\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3176\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetimetz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3177\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeTZBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_datetimetz\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# It seems like a repeat of is_datetime64tz_dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     return ((isinstance(arr, ABCDatetimeIndex) and\n\u001b[0m\u001b[1;32m    269\u001b[0m              getattr(arr, 'tz', None) is not None) or\n\u001b[1;32m    270\u001b[0m             is_datetime64tz_dtype(arr))\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_pandas_abc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_typ'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":"# -*- coding:utf-8 -*-\n\nimport os\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib import rnn\n\nimport tensorflow.contrib.layers as layers\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport tensorflow as tf\n\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\n\n\"\"\"wd_5_bigru_cnn\n\n在论文 Recurrent Convolutional Neural Networks for Text Classification 中。\n\n使用 BiRNN 处理，将每个时刻的隐藏状态和原输入拼起来，在进行 max_pooling 操作。\n\n这里有些不同，首先也是使用 bigru 得到每个时刻的，将每个时刻的隐藏状态和原输入拼起来；\n\n然后使用输入到 TextCNN 网络中。\n\n\"\"\"\n\n\n\n\n\nclass Settings(object):\n\n    def __init__(self):\n\n\n        self.model_name = 'wd_1_2_cnn_max'\n        self.query_len = 20\n        self.title_len = 20\n        self.filter_sizes = [2, 3, 4, 5, 7]\n        self.n_filter = 256\n        self.fc_hidden_size = 512\n        self.n_class = 1\n        self.train_batch_size=1024\n        self.vaild_batch_size=5000\n        self.lr=0.01\n        self.decay_step=7000\n        self.decay_rate=0.8\n        self.rnn_hidden_units=100\n\n\n\n\n\n\nclass TEXTCNN(object):\n\n    def __init__(self,  settings):\n\n\n        self.model_name = settings.model_name\n        self.query_len = settings.query_len\n        self.title_len = settings.title_len\n        self.filter_sizes = settings.filter_sizes\n        self.n_filter = settings.n_filter\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\n        self.n_class = settings.n_class\n        self.rnn_hidden_units=settings.rnn_hidden_units\n        self.fc_hidden_size = settings.fc_hidden_size\n        self._global_step = tf.Variable(0, trainable=False, name='Global_Step')\n        self.update_emas = list()\n        # placeholders\n        self._keep_prob = tf.placeholder(tf.float32, [])\n        self._batch_size = tf.placeholder(tf.int32, [])\n        self.embedding_size=100\n        \n\n        with tf.name_scope('Inputs'):\n            self.embedded_x_query=tf.placeholder(tf.float32,[None,self.query_len,self.embedding_size])#h r\n            self.embedded_x_title=tf.placeholder(tf.float32,[None,self.title_len,self.embedding_size])# r\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name='y_input')\n            \n            self.keep_prob_embed = tf.placeholder(name='keep_prob', dtype=tf.float32)\n            self.keep_prob_fully = tf.placeholder(name='keep_prob', dtype=tf.float32)\n            self.keep_prob_ae = tf.placeholder(name='keep_prob', dtype=tf.float32)\n            self.bn_training = tf.placeholder(name='bn_training', dtype=tf.bool)\n\n        p = self.dropout(self.embedded_x_query, self.keep_prob_embed)\n        h = self.dropout(self.embedded_x_title, self.keep_prob_embed)\n        \n        output_title = self.cnn_inference(self.embedded_x_query, self.query_len,'query')\n        output_title = tf.expand_dims(output_title, 0)\n\n        output_content = self.cnn_inference(self.embedded_x_title, self.title_len,'title')\n        output_content = tf.expand_dims(output_content, 0)\n        \n        with tf.name_scope(\"rnn_output\"):\n            for i in range(3):\n                # BiLSTM\n                p_state, h_state = p, h\n                for j in range(3):\n                    with tf.variable_scope(f'p_lstm_{i}_{j}', reuse=None):\n                        p_state, _ = self.BiLSTM(tf.concat(p_state, axis=-1))\n                    with tf.variable_scope(f'p_lstm_{i}_{j}' + str(i), reuse=None):\n                        h_state, _ = self.BiLSTM(tf.concat(h_state, axis=-1))\n    \n                p_state = tf.concat(p_state, axis=-1)\n                h_state = tf.concat(h_state, axis=-1)\n                # attention\n                cosine = tf.divide(tf.matmul(p_state, tf.matrix_transpose(h_state)),\n                                   (tf.norm(p_state, axis=-1, keep_dims=True) * tf.norm(h_state, axis=-1, keep_dims=True)))\n                att_matrix = tf.nn.softmax(cosine)\n                p_attention = tf.matmul(att_matrix, h_state)\n                h_attention = tf.matmul(att_matrix, p_state)\n    \n                # DesNet\n                p = tf.concat((p, p_state, p_attention), axis=-1)\n                h = tf.concat((h, h_state, h_attention), axis=-1)\n    \n                # auto_encoder\n                p = tf.layers.dense(p, 200)\n                h = tf.layers.dense(h, 200)\n    \n                p = self.dropout(p, self.keep_prob_ae)\n                h = self.dropout(h, self.keep_prob_ae)\n    \n            # interaction and prediction layer\n            add = p + h\n            sub = p - h\n            norm = tf.norm(sub, axis=-1)\n            out = tf.concat((p, h, add, sub, tf.expand_dims(norm, axis=-1)), axis=-1)\n            out = tf.reshape(out, shape=(-1, out.shape[1] * out.shape[2]))\n            out = self.dropout(out, self.keep_prob_fully)\n    \n            out = tf.layers.dense(out, 1000, activation='relu')\n            out = tf.layers.batch_normalization(out, training=self.bn_training)\n            out = tf.layers.dense(out, 1000, activation='relu')\n            out = tf.layers.batch_normalization(out, training=self.bn_training)\n            out = tf.layers.dense(out, 1000)\n            out = tf.layers.batch_normalization(out, training=self.bn_training)\n            self.distance = tf.layers.dense(out, 1)\n\n        with tf.variable_scope('fc-bn-layer'):\n            output = tf.concat([output_title, output_content], axis=0)\n            output = tf.reduce_max(output, axis=0)\n            W_fc = self.weight_variable([self.n_filter_total, self.fc_hidden_size], name='Weight_fc')\n            h_fc = tf.matmul(output, W_fc, name='h_fc')\n            fc_bn = tf.layers.batch_normalization(h_fc, training=self.bn_training)\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=\"relu\")\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\n\n        with tf.variable_scope('out_layer'):\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name='Weight_out')\n            b_out = self.bias_variable([self.n_class], name='bias_out')\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name='y_pred')  # 每个类别的分数 scores\n            self._y_pred = self._y_pred+self.distance\n        with tf.name_scope('loss'):\n            self._loss = tf.reduce_mean(\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\n            self._y_pred = tf.sigmoid(self._y_pred)\n\n\n\n    @property\n    def keep_prob(self):\n        return self._keep_prob\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @property\n    def X1_inputs(self):\n        return self._X1_inputs\n\n    @property\n    def X2_inputs(self):\n        return self._X2_inputs\n\n    @property\n    def y_inputs(self):\n        return self._y_inputs\n\n    @property\n    def y_pred(self):\n        return self._y_pred\n\n    @property\n    def loss(self):\n        return self._loss\n\n    def weight_variable(self, shape, name):\n        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(self, shape, name):\n        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name=name)\n\n\n\n    def cnn_inference(self, X_inputs, n_step,name):\n        \"\"\"TextCNN 模型。\n        Args:\n            X_inputs: tensor.shape=(batch_size, n_step)\n        Returns:\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\n        \"\"\"\n        inputs = X_inputs\n        inputs = tf.expand_dims(inputs, -1)\n        pooled_outputs = list()\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.variable_scope((\"conv-maxpool-%s\" % filter_size)+'_'+name):\n                # Convolution Layer\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\n                W_filter = self.weight_variable(shape=filter_shape, name='W_filter')\n                beta = self.bias_variable(shape=[self.n_filter], name='beta_filter')\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n                conv_bn=tf.layers.batch_normalization(conv,training=self.bn_training,name=name)\n                # conv_bn, update_ema = self.batchnorm(conv, beta, convolutional=True)  # 在激活层前面加 BN\n                # Apply nonlinearity, batch norm scaling is not useful with relus\n                # batch norm offsets are used instead of biases,使用 BN 层的 offset，不要 biases\n                h = tf.nn.relu(conv_bn, name=\"relu\")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n                pooled_outputs.append(pooled)\n                # self.update_emas.append(update_ema)\n        h_pool = tf.concat(pooled_outputs, 3)\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\n\n\n\n    def BiLSTM(self, x):\n        fw_cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_hidden_units)\n        bw_cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_hidden_units)\n        return tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, x, dtype=tf.float32)\n        \n    def dropout(self, x, keep_prob):\n        return tf.nn.dropout(x, keep_prob)\n        \n    \n\n\n\n\n\n\n\n\n\ndef loader(data_store_path='first_zzp/'):############\n    # all_data=pd.read_csv(data_store_path+\"data_sets/train_feature_second_no_query_and_title_data.csv\")\n    # query_title=pd.read_csv(data_store_path+\"data_sets/train_data.csv\",usecols=['query','title'])\n    print(\"开始加载数据\")\n    all_data=pd.read_pickle(data_store_path+\"train_data.pickle\")\n    data_split=StratifiedShuffleSplit(n_splits=2,test_size=0.001,random_state=1)\n    train_index,vaild_index=data_split.split(all_data['label'],all_data['label']).__next__()\n    train_data=all_data.iloc[train_index]\n    vaild_data=all_data.iloc[vaild_index]\n    print(\"加载数据完成\")\n    return train_data,vaild_data\n\ndef batch_iter(data_x,data_y, batch_size, num_epochs, shuffle=True):#这个就是产生batch的，可以直接用\n    \"\"\"\n    Generates a batch iterator for a dataset.\n    \"\"\"\n    # data_x=data_x.tolist()\n    data=list(zip(data_x,data_y))\n    data=np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:#如果为true 则代表允许随机取数据\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]#随机取数据\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]#而不用一个个加到列表里了。这是一个batch，嗯哼\n            #取到的维度为（batchsize,2） y为(batchsize,1) 里面存的是序号\n\ndef get_w2v_array(word_list,max_len):\n    array = np.zeros((max_len, 100))\n    if len(word_list)<=max_len:\n        for i in range(len(word_list)):\n            if str(word_list[i]) in word_wv.vocab.keys():\n                array[i][:] = word_wv[str(word_list[i])]\n    else:\n        for i in range(max_len):\n            if str(word_list[i]) in word_wv.vocab.keys():\n                array[i][:] = word_wv[str(word_list[i])]\n    return array\n\n\nsettings = Settings()\n\ntrain_batches,vaild_data=loader()\n\ntrain_batches=np.array(train_batches)\ntrain_batches_y=train_batches[:,2]\ntrain_batches_x=np.delete(train_batches, 2, axis=1)######这里是除了label以外的维数\n\n\nbatches=batch_iter(train_batches_x,train_batches_y,settings.train_batch_size,4)\n\nvaild_batches=np.array(vaild_data)\nvaild_batches_y=vaild_batches[:,2]\nvaild_batches_x=np.delete(vaild_batches, 2, axis=1)\nvaild_data=batch_iter(vaild_batches_x,vaild_batches_y,settings.vaild_batch_size,1000)\n\nprint(\"train starting\")\nw2v_model = Word2Vec.load('w2v_model/w2v_all_data_model.txt')\nword_wv= w2v_model.wv\n\n\nsettings = Settings()\n\nsettings.all_features_num=train_batches_x.shape[1]-2###########自动设置数值型特征数量\nmodel = TEXTCNN( settings)\n\nlearning_rate = tf.train.exponential_decay(settings.lr, model.global_step, settings.decay_step,\n                                       settings.decay_rate, staircase=True)\noptimizer = tf.train.AdamOptimizer(learning_rate)\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    grads_and_vars=optimizer.compute_gradients(model.loss)\n    train_op=optimizer.apply_gradients(grads_and_vars,global_step=model.global_step)\nsess = tf.Session() #启动创建的模型\nsess.run(tf.initialize_all_variables())\n\n\n# saver.restore(sess, tf.train.latest_checkpoint('tf_model_rcnn/'))\nper_train_loss_avg_list=[]\nper_vaild_loss_avg_list=[]\nparts_time=[]\nprint(\"start\")\nfor batch in batches:\n    batch_x,batch_y=zip(*batch)\n    batch_y=np.array(batch_y)[:,None]\n    batch_x=[i.tolist() for i in batch_x]\n    batch_x=pd.DataFrame(batch_x)\n\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\n    batch_x=np.array(batch_x)\n    #####[None,title_len,embedding_size]\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\n    #query   title   title_count         title_ctr      label\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\n            # self.keep_prob_embed = tf.placeholder(name='keep_prob', dtype=tf.float32)\n            # self.keep_prob_fully = tf.placeholder(name='keep_prob', dtype=tf.float32)\n            # self.keep_prob_ae = tf.placeholder(name='keep_prob', dtype=tf.float32)\n            # self.bn_training = tf.placeholder(name='bn_training', dtype=tf.bool)\n        \n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \n    feed_dict = {model.embedded_x_query: batch_x_query,\n                 model.embedded_x_title: batch_x_title,\n                 model.y_inputs: batch_y,\n                 model.batch_size: batch_x.shape[0], \n                 model.keep_prob: 0.8,\n                model.keep_prob_embed:0.8,\n                model.keep_prob_fully:0.8,\n                model.keep_prob_ae:0.5,\n                model.bn_training:True,\n    }\n    \n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\n    print(\"step {},loss:{:g}\".format(step,loss_out))              \n    per_train_loss_avg_list.append(loss_out)\n    \n    if step%(int(50000000/1004))==0:\n        print(\"batch ended\")\n    if step%50==0:\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\n        vaild_y=np.array(vaild_y)[:,None]\n        vaild_x=[i.tolist() for i in vaild_x]\n        vaild_x=pd.DataFrame(vaild_x)\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\n        vaild_x=np.array(vaild_x)\n    \n        #####[None,title_len,embedding_size]\n        #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\n        #query   title   title_count         title_ctr      label\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\n\n\n        # #########################这是连续型特征\n        # vaild_x_ctr_fea=vaild_x[:,2:]######这里需要到时候调整，按label的位置和特征位置\n        # vaild_x_ctr_fea=np.delete(vaild_x_ctr_fea, -1, axis=1)######需要把类别型特征去掉\n        # ##########################\n        # ###########################这是类别型特征\n        # vaild_x_tfidf=vaild_x[:,-1][:,None]\n        # #########################################\n\n\n        feed_dict = {model.embedded_x_query: vaild_x_query,\n                     model.embedded_x_title: vaild_x_title,\n                     model.y_inputs: vaild_y,\n                     model.batch_size: vaild_x.shape[0], \n                     model.keep_prob: 1,\n                    model.keep_prob_embed:1,\n                    model.keep_prob_fully:1,\n                    model.keep_prob_ae:1,\n                    model.bn_training:False,\n        }\n        \n        \n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\n        per_vaild_loss_avg_list.append(loss_out)\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\n        per_train_loss_avg_list=[]\n        print(\"vaild_loss:\",loss_out)\n    if step%5000==0:\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\n        per_vaild_loss_avg_list=[]\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\n        saver = tf.train.Saver(max_to_keep = 3)\n        saver = saver.save(sess, \"/home/kesci/work/td_model_drcn\", global_step=step)","execution_count":1},{"metadata":{"id":"A337159ED5EF4CC4B84D153E0ACA77D3","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"step 120,loss:0.472395\nstep 130,loss:0.463998\nstep 140,loss:0.506809\nstep 150,loss:0.496877\nstep 160,loss:0.480746\nstep 170,loss:0.499878\nstep 180,loss:0.521097\nstep 190,loss:0.475982\nstep 200,loss:0.467037\nstep 210,loss:0.453355\nstep 220,loss:0.444587\nstep 230,loss:0.466765\nstep 240,loss:0.493508\nstep 250,loss:0.514931\nstep 260,loss:0.51912\nstep 270,loss:0.467393\nstep 280,loss:0.44461\nstep 290,loss:0.503186\nstep 300,loss:0.500033\nstep 310,loss:0.488276\nstep 320,loss:0.490603\nstep 330,loss:0.496466\nstep 340,loss:0.501055\nstep 350,loss:0.486557\nstep 360,loss:0.476719\nstep 370,loss:0.484332\nstep 380,loss:0.485746\nstep 390,loss:0.501963\nstep 400,loss:0.469364\nstep 410,loss:0.436967\nstep 420,loss:0.55591\nstep 430,loss:0.471293\nstep 440,loss:0.504407\nstep 450,loss:0.608281\nstep 460,loss:0.557874\nstep 470,loss:0.516888\nstep 480,loss:0.484384\nstep 490,loss:0.483864\nstep 500,loss:0.474109\n距离训练完一整轮剩余时间： 767.2611294154078  分钟\nper_train_loss_avg: 0.48840725056843665 **********\nvaild_loss: 0.5826657\nstep 510,loss:0.468049\nstep 520,loss:0.4808\nstep 530,loss:0.493085\nstep 540,loss:0.450416\nstep 550,loss:0.524913\nstep 560,loss:0.481231\nstep 570,loss:0.467341\nstep 580,loss:0.462757\nstep 590,loss:0.453697\nstep 600,loss:0.498075\nstep 610,loss:0.487845\nstep 620,loss:0.468275\nstep 630,loss:0.515749\nstep 640,loss:0.47929\nstep 650,loss:0.523054\nstep 660,loss:0.531044\nstep 670,loss:0.44981\nstep 680,loss:0.461067\nstep 690,loss:0.468417\nstep 700,loss:0.480703\nstep 710,loss:0.484582\nstep 720,loss:0.477982\nstep 730,loss:0.493235\nstep 740,loss:0.494243\nstep 750,loss:0.471655\nstep 760,loss:0.487379\nstep 770,loss:0.493562\nstep 780,loss:0.490628\nstep 790,loss:0.489329\nstep 800,loss:0.507147\nstep 810,loss:0.45618\nstep 820,loss:0.433563\nstep 830,loss:0.473912\nstep 840,loss:0.497862\nstep 850,loss:0.459539\nstep 860,loss:0.499757\nstep 870,loss:0.476422\nstep 880,loss:0.469149\nstep 890,loss:0.492633\nstep 900,loss:0.496168\nstep 910,loss:0.477992\nstep 920,loss:0.518006\nstep 930,loss:0.474916\nstep 940,loss:0.458691\nstep 950,loss:0.497661\nstep 960,loss:0.444276\nstep 970,loss:0.514513\nstep 980,loss:0.49264\nstep 990,loss:0.477474\nstep 1000,loss:0.46927\n距离训练完一整轮剩余时间： 985.7845298127581  分钟\nper_train_loss_avg: 0.4835477374196053 **********\nvaild_loss: 0.4863197\nstep 1010,loss:0.485955\nstep 1020,loss:0.500275\nstep 1030,loss:0.484535\nstep 1040,loss:0.486053\nstep 1050,loss:0.459597\nstep 1060,loss:0.484854\nstep 1070,loss:0.502355\nstep 1080,loss:0.440989\nstep 1090,loss:0.453345\nstep 1100,loss:0.504219\nstep 1110,loss:0.483393\nstep 1120,loss:0.472755\nstep 1130,loss:0.494566\nstep 1140,loss:0.444392\nstep 1150,loss:0.46597\nstep 1160,loss:0.476691\nstep 1170,loss:0.464136\nstep 1180,loss:0.494233\nstep 1190,loss:0.431217\nstep 1200,loss:0.509216\nstep 1210,loss:0.473809\nstep 1220,loss:0.481353\nstep 1230,loss:0.490359\nstep 1240,loss:0.483592\nstep 1250,loss:0.47047\nstep 1260,loss:0.480711\nstep 1270,loss:0.503472\nstep 1280,loss:0.483701\nstep 1290,loss:0.483799\nstep 1300,loss:0.476711\nstep 1310,loss:0.462345\nstep 1320,loss:0.466436\nstep 1330,loss:0.471169\nstep 1340,loss:0.45232\nstep 1350,loss:0.475379\nstep 1360,loss:0.480391\nstep 1370,loss:0.444933\nstep 1380,loss:0.462524\nstep 1390,loss:0.519954\nstep 1400,loss:0.451205\nstep 1410,loss:0.463391\nstep 1420,loss:0.460853\nstep 1430,loss:0.477863\nstep 1440,loss:0.488138\nstep 1450,loss:0.467981\nstep 1460,loss:0.533716\nstep 1470,loss:0.467085\nstep 1480,loss:0.517458\nstep 1490,loss:0.485691\nstep 1500,loss:0.453646\n距离训练完一整轮剩余时间： 978.2873305051277  分钟\nper_train_loss_avg: 0.4760953410863876 **********\nvaild_loss: 0.48283124\nstep 1510,loss:0.460159\nstep 1520,loss:0.472551\nstep 1530,loss:0.449372\nstep 1540,loss:0.497113\nstep 1550,loss:0.496954\nstep 1560,loss:0.493353\nstep 1570,loss:0.479041\nstep 1580,loss:0.48818\nstep 1590,loss:0.481775\nstep 1600,loss:0.484197\nstep 1610,loss:0.460824\nstep 1620,loss:0.486579\nstep 1630,loss:0.492822\nstep 1640,loss:0.437857\nstep 1650,loss:0.462732\nstep 1660,loss:0.495459\nstep 1670,loss:0.459319\nstep 1680,loss:0.44302\nstep 1690,loss:0.485563\nstep 1700,loss:0.476932\nstep 1710,loss:0.487192\nstep 1720,loss:0.493299\nstep 1730,loss:0.447139\nstep 1740,loss:0.476715\nstep 1750,loss:0.466724\nstep 1760,loss:0.442771\nstep 1770,loss:0.450644\nstep 1780,loss:0.456286\nstep 1790,loss:0.49282\nstep 1800,loss:0.483979\nstep 1810,loss:0.473845\n","name":"stdout"}],"source":"parts_time=[]\nfrom time import time\nfor batch in batches:\n    time1=time()\n    batch_x,batch_y=zip(*batch)\n    batch_y=np.array(batch_y)[:,None]\n    batch_x=[i.tolist() for i in batch_x]\n    batch_x=pd.DataFrame(batch_x)\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\n    batch_x=np.array(batch_x)\n    #####[None,title_len,embedding_size]\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\n    #query   title   title_count         title_ctr      label\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \n    feed_dict = {model.embedded_x_query: batch_x_query,\n                 model.embedded_x_title: batch_x_title,\n                 model.y_inputs: batch_y,\n                 model.batch_size: batch_x.shape[0], \n                 model.keep_prob: 0.8,\n        \n                model.keep_prob_embed:0.8,\n                model.keep_prob_fully:0.8,\n                model.keep_prob_ae:0.5,\n                model.bn_training:True,\n        \n    }\n\n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\n    time2=time()\n    parts_time.append(time2-time1)\n    if step%10==0:\n        print(\"step {},loss:{:g}\".format(step,loss_out))              \n    per_train_loss_avg_list.append(loss_out)\n    \n    \n    if step%(int(50000000/1004))==0:\n        print(\"batch ended\")\n    if step%500==0:\n        time_single_part=sum(parts_time)\n        ed_time=(50000000/settings.train_batch_size-step)/500*time_single_part\n        print(\"距离训练完一整轮剩余时间：\",ed_time/60,\" 分钟\")\n        parts_time=[]\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\n        vaild_y=np.array(vaild_y)[:,None]\n        vaild_x=[i.tolist() for i in vaild_x]\n        vaild_x=pd.DataFrame(vaild_x)\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\n        vaild_x=np.array(vaild_x)\n    \n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\n\n        feed_dict = {model.embedded_x_query: vaild_x_query,\n                     model.embedded_x_title: vaild_x_title,\n                     model.y_inputs: vaild_y,\n                     model.batch_size: vaild_x.shape[0], \n                     model.keep_prob: 1,\n\n                    model.keep_prob_embed:1,\n                    model.keep_prob_fully:1,\n                    model.keep_prob_ae:1,\n                    model.bn_training:False,\n            \n            \n        }\n    \n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\n        per_vaild_loss_avg_list.append(loss_out)\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\n        per_train_loss_avg_list=[]\n        print(\"vaild_loss:\",loss_out)\n    if step%5000==0:\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\n        per_vaild_loss_avg_list=[]\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\n        saver = tf.train.Saver(max_to_keep = 3,var_list=tf.global_variables())\n        saver = saver.save(sess, \"/home/kesci/work/first_zzp/tf_model_hrcn_textcnn/rnn\", global_step=step)","execution_count":2},{"metadata":{"id":"71339D1DBC534D579CF899437BF5A81C"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}