{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"FDD23076F5CE4A408BE9F416FE2E83A5"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"7F37F720D2A04A13B790042E8414516C"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"16A2E345E91B4AF8835F46CA88117001"}},{"outputs":[{"output_type":"stream","text":"开始加载数据\n加载数据完成\ntrain starting\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","name":"stderr"},{"output_type":"stream","text":"start\nstep 10,loss:1.18032\nstep 20,loss:0.971862\nstep 30,loss:0.537315\nstep 40,loss:0.425747\nstep 50,loss:0.302233\nstep 60,loss:0.351071\nstep 70,loss:0.21517\nstep 80,loss:0.234336\n","name":"stdout"}],"execution_count":1,"source":"# -*- coding:utf-8 -*-\r\n\r\nimport os\r\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import rnn\r\n\r\nimport tensorflow.contrib.layers as layers\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\n\r\nimport tensorflow as tf\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom gensim.models import Word2Vec\r\n\r\n\"\"\"wd_5_bigru_cnn\r\n\r\n在论文 Recurrent Convolutional Neural Networks for Text Classification 中。\r\n\r\n使用 BiRNN 处理，将每个时刻的隐藏状态和原输入拼起来，在进行 max_pooling 操作。\r\n\r\n这里有些不同，首先也是使用 bigru 得到每个时刻的，将每个时刻的隐藏状态和原输入拼起来；\r\n\r\n然后使用输入到 TextCNN 网络中。\r\n\r\n\"\"\"\r\n\r\n\r\n\r\n\r\n\r\nclass Settings(object):\r\n\r\n    def __init__(self):\r\n\r\n\r\n        self.model_name = 'wd_1_2_cnn_max'\r\n        self.query_len = 20\r\n        self.title_len = 20\r\n        self.filter_sizes = [2, 3, 4, 5]\r\n        self.n_filter = 256\r\n        self.fc_hidden_size = 1024\r\n        self.n_class = 1\r\n        self.train_batch_size=512\r\n        self.vaild_batch_size=512\r\n        self.lr=0.0025\r\n        self.decay_step=20000\r\n        self.decay_rate=0.8\r\n\r\n\r\n\r\n\r\n\r\nclass TEXTCNN(object):\r\n\r\n    def __init__(self,  settings):\r\n\r\n\r\n        self.model_name = settings.model_name\r\n        self.query_len = settings.query_len\r\n        self.title_len = settings.title_len\r\n        self.filter_sizes = settings.filter_sizes\r\n        self.n_filter = settings.n_filter\r\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\r\n        self.n_class = settings.n_class\r\n        self.fc_hidden_size = settings.fc_hidden_size\r\n        self._global_step = tf.Variable(0, trainable=False, name='Global_Step')\r\n        self.update_emas = list()\r\n        # placeholders\r\n        self.training = tf.placeholder(tf.bool)\r\n        self._keep_prob = tf.placeholder(tf.float32, [])\r\n        self._batch_size = tf.placeholder(tf.int32, [])\r\n        self.embedding_size=100\r\n        self.w=4\r\n        self.di = 16\r\n        self.num_layers = 2\r\n        self.model_type=\"ABCNN3\"\r\n        self.eclipse = 1e-10\r\n        \r\n\r\n        with tf.name_scope('Inputs'):\r\n            self.embedded_x_query=tf.placeholder(tf.float32,[None,self.query_len,self.embedding_size])#h r\r\n            self.embedded_x_title=tf.placeholder(tf.float32,[None,self.title_len,self.embedding_size])# r\r\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name='y_input')\r\n\r\n        output_title = self.cnn_inference(self.embedded_x_query, self.query_len,'query')\r\n        output_title = tf.expand_dims(output_title, 0)\r\n\r\n        output_content = self.cnn_inference(self.embedded_x_title, self.title_len,'title')\r\n        output_content = tf.expand_dims(output_content, 0)\r\n\r\n\r\n        with tf.variable_scope('fc-bn-layer'):\r\n            output = tf.concat([output_title, output_content], axis=0)\r\n            output = tf.reduce_max(output, axis=0)\r\n            W_fc = self.weight_variable([self.n_filter_total, self.fc_hidden_size], name='Weight_fc')\r\n            h_fc = tf.matmul(output, W_fc, name='h_fc')\r\n            fc_bn=tf.layers.batch_normalization(h_fc,training=self.training)\r\n            # self.update_emas.append(update_ema_fc)\r\n            self.fc_bn_relu = tf.nn.relu(fc_bn, name=\"relu\")\r\n            fc_bn_drop = tf.nn.dropout(self.fc_bn_relu, self.keep_prob)\r\n\r\n        with tf.variable_scope('out_layer'):\r\n            W_out = self.weight_variable([self.fc_hidden_size, self.n_class], name='Weight_out')\r\n            b_out = self.bias_variable([self.n_class], name='bias_out')\r\n            self.sim = self.lcnn_foward()\r\n            self._y_pred = tf.nn.xw_plus_b(fc_bn_drop, W_out, b_out, name='y_pred')  # 每个类别的分数 scores\r\n            self._y_pred = self._y_pred+self.sim \r\n        \r\n\r\n        with tf.name_scope('loss'):\r\n            self._loss = tf.reduce_mean(\r\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\r\n\r\n    @property\r\n    def tst(self):\r\n        return self._tst\r\n\r\n    @property\r\n    def keep_prob(self):\r\n        return self._keep_prob\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self._batch_size\r\n\r\n    @property\r\n    def global_step(self):\r\n        return self._global_step\r\n\r\n    @property\r\n    def X1_inputs(self):\r\n        return self._X1_inputs\r\n\r\n    @property\r\n    def X2_inputs(self):\r\n        return self._X2_inputs\r\n\r\n    @property\r\n    def y_inputs(self):\r\n        return self._y_inputs\r\n\r\n    @property\r\n    def y_pred(self):\r\n        return self._y_pred\r\n\r\n    @property\r\n    def loss(self):\r\n        return self._loss\r\n\r\n    def weight_variable(self, shape, name):\r\n        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial, name=name)\r\n\r\n    def bias_variable(self, shape, name):\r\n        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n        initial = tf.constant(0.1, shape=shape)\r\n        return tf.Variable(initial, name=name)\r\n\r\n\r\n    def lcnn_foward(self):\r\n\r\n        question_inputs = tf.transpose(self.embedded_x_query, perm=[0, 2, 1])\r\n        answer_inputs = tf.transpose(self.embedded_x_title, perm=[0, 2, 1])\r\n\r\n        question_expanded = tf.expand_dims(question_inputs, -1)\r\n        answer_expanded = tf.expand_dims(answer_inputs, -1)\r\n\r\n        with tf.name_scope('all_pooling'):\r\n            question_ap_0 = self.all_pool(variable_scope='input-question', x=question_expanded)\r\n            answer_ap_0 = self.all_pool(variable_scope='input-answer', x=answer_expanded)\r\n\r\n        question_wp_1, question_ap_1, answer_wp_1, answer_ap_1 = self.CNN_layer(variable_scope='CNN-1', x1=question_expanded, x2=answer_expanded, d=self.embedding_size)\r\n        sims = [self.cos_sim(question_ap_0, answer_ap_0), self.cos_sim(question_ap_1, answer_ap_1)]\r\n        # sims.append(self.get_sim(question_ap_0, answer_ap_0, self.embedding_size, 'ap_0'))\r\n        # sims.append(self.get_sim(question_ap_1, answer_ap_1, self.di, 'ap_1'))\r\n\r\n\r\n        if self.num_layers > 1:\r\n            _, question_ap_2, _, answer_ap_2 = self.CNN_layer(variable_scope=\"CNN-2\", x1=question_wp_1, x2=answer_wp_1, d=self.di)\r\n            self.question_test = question_ap_2\r\n            self.answer_test = answer_ap_2\r\n            sims.append(self.cos_sim(question_ap_2, answer_ap_2))\r\n            # sims.append(self.get_sim(question_ap_2, answer_ap_2, self.di, 'ap_2'))\r\n\r\n\r\n        with tf.variable_scope('output_layer'):\r\n            self.output_features = tf.stack(sims, axis=1, name='output_features')\r\n\r\n            self.output_features = tf.layers.batch_normalization(self.output_features,training=self.training,name='lc_cnn_out')\r\n\r\n            self.output_features = tf.nn.dropout(self.output_features, self.keep_prob, name='hidden_output_drop')\r\n\r\n            self.fc = tf.contrib.layers.fully_connected(\r\n                inputs = self.output_features,\r\n                num_outputs= 256,\r\n                activation_fn = tf.nn.tanh,\r\n                weights_initializer=tf.contrib.layers.xavier_initializer(),\r\n                biases_initializer=tf.constant_initializer(1e-02),\r\n                scope=\"FC\"\r\n            )\r\n            self.fc_bn = tf.layers.batch_normalization(self.fc,training=self.training,name='lc_fc_bn_')\r\n            self.hidden_drop = tf.nn.dropout(self.fc, self.keep_prob, name='fc_drop')\r\n\r\n            sim = tf.contrib.layers.fully_connected(\r\n                inputs = self.hidden_drop,\r\n                num_outputs= self.n_class,\r\n                activation_fn = None,\r\n                weights_initializer=tf.contrib.layers.xavier_initializer(),\r\n                biases_initializer=tf.constant_initializer(1e-04),\r\n                scope=\"FC_2\"\r\n            )\r\n\r\n\r\n        return sim\r\n\r\n\r\n\r\n    def CNN_layer(self, variable_scope, x1, x2, d):\r\n        with tf.variable_scope(variable_scope):\r\n            # 在输入层加入注意力\r\n            if self.model_type == 'ABCNN1' or self.model_type == 'ABCNN3':\r\n                with tf.name_scope('att_mat'):\r\n                    # [query_len, d]\r\n                    # question和answer共享同一个矩阵aW\r\n                    aW = tf.get_variable(name = 'aW',\r\n                                         shape = (self.query_len, d),\r\n                                         initializer = tf.contrib.layers.xavier_initializer())\r\n                    # [batch_size, query_len, query_len]\r\n                    att_mat_A = self.make_attention_mat(x1, x2)\r\n\r\n                    # tf.einsum(\"ijk,kl->ijl\", att_mat_A, aW) [batch_size, query_len, d]\r\n                    # tf.matrix_transpose(_____)  [batch_size, d, query_len]\r\n                    # tf.expand_dims(_____)  [batch_size, d, query_len, 1]\r\n                    x1_a = tf.expand_dims(tf.matrix_transpose(tf.einsum(\"ijk,kl->ijl\", att_mat_A, aW)), -1)\r\n                    x2_a = tf.expand_dims(tf.matrix_transpose(tf.einsum(\"ijk,kl->ijl\", tf.matrix_transpose(att_mat_A), aW)), -1)\r\n                    # [batch_size, d, query_len, 2]\r\n                    x1 = tf.concat([x1, x1_a], axis=-1)\r\n                    x2 = tf.concat([x2, x2_a], axis=-1)\r\n\r\n            # 这个reuse很迷\r\n            question_conv = self.convolution(x=self.pad_for_wide_conv(x1), d=d, reuse=False, name_scope='question')\r\n            answer_conv = self.convolution(x=self.pad_for_wide_conv(x2), d=d, reuse=True, name_scope='answer')\r\n\r\n            question_attention, answer_attention = None, None\r\n\r\n            if self.model_type == 'ABCNN2' or self.model_type == 'ABCNN3':\r\n                # matrix A [batch_size, query_len + w - 1, query_len + w - 1]\r\n                att_mat_A = self.make_attention_mat(question_conv, answer_conv)\r\n                # [batch_size, query_len + w - 1]\r\n                question_attention, answer_attention = tf.reduce_sum(att_mat_A, axis=2), tf.reduce_sum(att_mat_A, axis=1)\r\n\r\n            question_wp = self.w_pool(variable_scope='question', x=question_conv, attention=question_attention)\r\n            question_ap = self.all_pool(variable_scope='question', x=question_conv)\r\n            answer_wp = self.w_pool(variable_scope='answer', x=answer_conv, attention=answer_attention)\r\n            answer_ap = self.all_pool(variable_scope='answer', x=answer_conv)\r\n\r\n            return question_wp,question_ap,answer_wp,answer_ap\r\n\r\n    def get_sim(self, x1, x2, d, name_scope):\r\n        with tf.variable_scope(\"similarity_\" + name_scope):\r\n            M = tf.get_variable(\r\n                name = 'M',\r\n                shape = [d, d],\r\n                initializer = tf.contrib.layers.xavier_initializer()\r\n            )\r\n\r\n            x1_trans = tf.matmul(x1, M)\r\n            sims = tf.reduce_sum(tf.multiply(x1_trans, x2), axis=1)\r\n            return sims\r\n\r\n\r\n    def w_pool(self, variable_scope, x, attention):\r\n        '''\r\n        :param viriable_scope:\r\n        :param x: [batch_size, di, query_len + w - 1, 1 or 2]\r\n        :param attention: [batch_size, query_len + w -1]\r\n        :return:\r\n        '''\r\n        with tf.variable_scope(variable_scope + '-w_pool'):\r\n            if self.model_type == 'ABCNN2' or self.model_type == 'ABCNN3':\r\n                pools = []\r\n                # [batch, s+w-1] => [batch, s+w-1, 1, 1] => [batch, 1, s+w-1, 1]\r\n                attention = tf.transpose(tf.expand_dims(tf.expand_dims(attention, -1), -1), [0, 2, 1, 3])\r\n                for i in range(self.query_len):\r\n                    pools.append(tf.reduce_sum(\r\n                        x[:, :, i: i+self.w, :] * attention[:, :, i: i+self.w, :],\r\n                        axis=2,\r\n                        keepdims=True\r\n                    ))\r\n                w_ap = tf.concat(pools, axis=2, name='w_ap')\r\n            else:\r\n                w_ap = tf.layers.average_pooling2d(\r\n                    inputs=x,\r\n                    pool_size=(1, self.w),   #用w作为卷积窗口可以还原句子长度\r\n                    strides=1,\r\n                    padding='VALID',\r\n                    name='w_ap'\r\n                )\r\n            # w_ap [batch_size, di, query_len, 1 or 2]\r\n            return w_ap\r\n\r\n\r\n    def all_pool(self, variable_scope, x):\r\n        # al-op\r\n        with tf.variable_scope(variable_scope + '-all_pool'):\r\n            if variable_scope.startswith('input'):\r\n                # 对输入层做all-pooling\r\n                pool_width = self.query_len\r\n                d = self.embedding_size\r\n            else:\r\n                # 对最后的巻积层做all-pooling\r\n                pool_width = self.query_len + self.w - 1\r\n                d = self.di\r\n\r\n            all_ap = tf.layers.average_pooling2d(\r\n                inputs = x,\r\n                pool_size = (1, pool_width),\r\n                strides = 1,\r\n                padding= 'VALID',\r\n                name = 'all_ap'\r\n            )\r\n            # [batch_size, di, 1, 1]\r\n\r\n            # [batch_size, di]\r\n            all_ap_reshaped = tf.reshape(all_ap, [-1, d])\r\n            return all_ap_reshaped\r\n\r\n\r\n    def make_attention_mat(self, x1, x2):\r\n        # x1  [batch_size, embedding_size, query_len, 1]\r\n        # tf.matrix_transpose(x2) [batch_size, embedding_size, 1, query_len]\r\n\r\n        # 广播产生一个 [query_len_0, query_len_1]的矩阵\r\n        # x1 - tf.matrix_transpose(x2)  [batch_size, embedding_size, query_len, query_len]\r\n        # euclidean [bath_size, query_len, query_len]\r\n        euclidean = tf.sqrt(tf.reduce_sum(tf.square(x1 - tf.matrix_transpose(x2)), axis=1) + self.eclipse)\r\n        return 1 / (1 + euclidean)\r\n\r\n    def pad_for_wide_conv(self, x):\r\n        # 左右各填充self.w - 1个0\r\n\r\n        # 填充前 [batch_size, d, query_len, 1] or [batch_size, d, query_len, 2]\r\n        # 填充后 [batch_size, d, query_len - 2*(w-1), 1] or [batch_size, d, query_len - 2*(w-1), 2]\r\n        return tf.pad(x, np.array([[0, 0], [0, 0], [self.w - 1, self.w - 1], [0, 0]]), \"CONSTANT\", name=\"pad_wide_conv\")\r\n\r\n\r\n    def convolution(self, x, d, reuse, name_scope):\r\n        # 滑窗卷积\r\n        with tf.name_scope(name_scope + '-conv'):\r\n            with tf.variable_scope(\"conv\") as scope:\r\n                conv = tf.contrib.layers.conv2d(\r\n                    inputs = x,\r\n                    num_outputs = self.di,\r\n                    kernel_size = (d, self.w),\r\n                    stride = 1,\r\n                    padding = 'VALID',\r\n                    activation_fn = tf.nn.tanh,\r\n                    weights_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\r\n\r\n                    biases_initializer = tf.constant_initializer(1e-4),\r\n                    reuse = reuse,\r\n                    trainable = True,\r\n                    scope = scope,\r\n                )\r\n            # output [batch_size, 1, query_len + w - 1, di]\r\n\r\n            # conv_trans: [batch_size, di, query_len + w - 1, 1]\r\n            conv_trans = tf.transpose(conv, [0, 3, 2, 1], name = 'conv_trans')\r\n            return conv_trans\r\n\r\n    def cos_sim(self, v1, v2):\r\n        norm1 = tf.sqrt(tf.reduce_sum(tf.square(v1), axis=1))\r\n        norm2 = tf.sqrt(tf.reduce_sum(tf.square(v2), axis=1))\r\n        dot_products = tf.reduce_sum(v1*v2, axis=1, name='cos_sim')\r\n\r\n        return dot_products / (norm1 * norm2)\r\n    \r\n\r\n\r\n    def cnn_inference(self, X_inputs, n_step,name):\r\n        \"\"\"TextCNN 模型。\r\n        Args:\r\n            X_inputs: tensor.shape=(batch_size, n_step)\r\n        Returns:\r\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\r\n        \"\"\"\r\n        inputs = X_inputs\r\n        inputs = tf.expand_dims(inputs, -1)\r\n        pooled_outputs = list()\r\n        for i, filter_size in enumerate(self.filter_sizes):\r\n            with tf.variable_scope((\"conv-maxpool-%s\" % filter_size)+name):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\r\n                W_filter = self.weight_variable(shape=filter_shape, name='W_filter')\r\n                beta = self.bias_variable(shape=[self.n_filter], name='beta_filter')\r\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\r\n                conv_bn=tf.layers.batch_normalization(conv,training=self.training,name=name)\r\n                # Apply nonlinearity, batch norm scaling is not useful with relus\r\n                # batch norm offsets are used instead of biases,使用 BN 层的 offset，不要 biases\r\n                h = tf.nn.relu(conv_bn, name=\"relu\")\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\r\n                                        strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n        h_pool = tf.concat(pooled_outputs, 3)\r\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\r\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\r\n\r\n\r\n\r\ndef loader(data_store_path=''):############\r\n    # all_data=pd.read_csv(data_store_path+\"data_sets/train_feature_second_no_query_and_title_data.csv\")\r\n    # query_title=pd.read_csv(data_store_path+\"data_sets/train_data.csv\",usecols=['query','title'])\r\n    print(\"开始加载数据\")\r\n    all_data=pd.read_pickle(data_store_path+\"/home/kesci/train_data0-5000w.pickle\")\r\n    data_split=StratifiedShuffleSplit(n_splits=2,test_size=0.001,random_state=1)\r\n    train_index,vaild_index=data_split.split(all_data['label'],all_data['label']).__next__()\r\n    train_data=all_data.iloc[train_index]\r\n    vaild_data=all_data.iloc[vaild_index]\r\n    print(\"加载数据完成\")\r\n    return train_data,vaild_data\r\n\r\ndef batch_iter(data_x,data_y, batch_size, num_epochs, shuffle=True):#这个就是产生batch的，可以直接用\r\n    \"\"\"\r\n    Generates a batch iterator for a dataset.\r\n    \"\"\"\r\n    # data_x=data_x.tolist()\r\n    data=list(zip(data_x,data_y))\r\n    data=np.array(data)\r\n    data_size = len(data)\r\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\r\n    for epoch in range(num_epochs):\r\n        # Shuffle the data at each epoch\r\n        if shuffle:#如果为true 则代表允许随机取数据\r\n            shuffle_indices = np.random.permutation(np.arange(data_size))\r\n            shuffled_data = data[shuffle_indices]#随机取数据\r\n        else:\r\n            shuffled_data = data\r\n        for batch_num in range(num_batches_per_epoch):\r\n            start_index = batch_num * batch_size\r\n            end_index = min((batch_num + 1) * batch_size, data_size)\r\n            yield shuffled_data[start_index:end_index]#而不用一个个加到列表里了。这是一个batch，嗯哼\r\n            #取到的维度为（batchsize,2） y为(batchsize,1) 里面存的是序号\r\n\r\nfrom gensim import models\r\nimport pickle\r\nfw=open('dictionary/dictionary.pickle','rb')\r\ndictionary=pickle.load(fw)\r\nimport pickle\r\nfw=open('tfidf_model/tfidf_model.pickle','rb')\r\ntfidf_model=pickle.load(fw)\r\n\r\ndef get_w2v_array(word_list,max_len):\r\n    single_sentence_tfidf=tfidf_model[dictionary.doc2bow(word_list)]\r\n    # single_sentence_tfidf={dictionary[word_id_tfidf[0]]:word_id_tfidf[1] for word_id_tfidf in single_sentence_tfidf}\r\n    array = np.zeros((max_len, 100))\r\n    if len(word_list)<=max_len:\r\n        for i in range(len(word_list)):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n                # try:\r\n                #     array[i][:]*=single_sentence_tfidf[str(word_list[i])]\r\n\r\n                # except:\r\n                #     pass\r\n                \r\n    else:\r\n        for i in range(max_len):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n                # try:\r\n                #     array[i][:]*=single_sentence_tfidf[str(word_list[i])]\r\n                # except:\r\n                #     pass\r\n    return array\r\n\r\nsettings = Settings()\r\n\r\ntrain_batches,vaild_data=loader()\r\n\r\ntrain_batches=np.array(train_batches)\r\ntrain_batches_y=train_batches[:,2]\r\ntrain_batches_x=np.delete(train_batches, 2, axis=1)######这里是除了label以外的维数\r\n\r\n\r\nbatches=batch_iter(train_batches_x,train_batches_y,settings.train_batch_size,4)\r\n\r\nvaild_batches=np.array(vaild_data)\r\nvaild_batches_y=vaild_batches[:,2]\r\nvaild_batches_x=np.delete(vaild_batches, 2, axis=1)\r\nvaild_data=batch_iter(vaild_batches_x,vaild_batches_y,settings.vaild_batch_size,1000)\r\n\r\nprint(\"train starting\")\r\nw2v_model = Word2Vec.load('w2v_model3/w2v_all_data_model.txt')\r\nword_wv= w2v_model.wv\r\n\r\n\r\nsettings = Settings()\r\n\r\nsettings.all_features_num=train_batches_x.shape[1]-2###########自动设置数值型特征数量\r\nmodel = TEXTCNN( settings)\r\n\r\nlearning_rate = tf.train.exponential_decay(settings.lr, model.global_step, settings.decay_step,\r\n                                       settings.decay_rate, staircase=True)\r\noptimizer = tf.train.AdamOptimizer(learning_rate)\r\n\r\nupdate_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    grads_and_vars=optimizer.compute_gradients(model.loss)\r\n    train_op=optimizer.apply_gradients(grads_and_vars,global_step=model.global_step)\r\n\r\nconfig = tf.ConfigProto() \r\nconfig.gpu_options.allow_growth = True \r\nsess = tf.Session(config=config) #启动创建的模型\r\n# sess.run(tf.initialize_all_variables())\r\n\r\n# saver = tf.train.Saver(tf.global_variables())\r\nsaver = tf.train.Saver()\r\nsess.run(tf.global_variables_initializer())\r\n# saver.restore(sess, tf.train.latest_checkpoint('/home/kesci/work/tf_model_textcnn/'))\r\nper_train_loss_avg_list=[]\r\nper_vaild_loss_avg_list=[]\r\nprint(\"start\")\r\nparts_time=[]\r\nfor batch in batches:\r\n    from time import time\r\n    time1=time()\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=[i.tolist() for i in batch_x]\r\n    batch_x=pd.DataFrame(batch_x)\r\n\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\r\n\r\n        \r\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \r\n    feed_dict = {model.embedded_x_query: batch_x_query,\r\n                 model.embedded_x_title: batch_x_title,\r\n                 model.y_inputs: batch_y,\r\n                 model.batch_size: batch_x.shape[0], \r\n                 model.training: True, \r\n                 model.keep_prob: 0.8}\r\n    \r\n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\r\n    time2=time()\r\n    parts_time.append(time2-time1)\r\n    if step%10==0:\r\n        print(\"step {},loss:{:g}\".format(step,loss_out))              \r\n    per_train_loss_avg_list.append(loss_out)\r\n    \r\n    if step%(int(50000000/1004))==0:\r\n        print(\"batch ended\")\r\n    if step%500==0:\r\n        time_single_part=sum(parts_time)\r\n        ed_time=(50000000/settings.train_batch_size-step)/500*time_single_part\r\n        print(\"距离训练完一整轮剩余时间：\",ed_time/60,\" 分钟\")\r\n        parts_time=[]\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=[i.tolist() for i in vaild_x]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        #####[None,title_len,embedding_size]\r\n        #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n        #query   title   title_count         title_ctr      label\r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\r\n\r\n\r\n        # #########################这是连续型特征\r\n        # vaild_x_ctr_fea=vaild_x[:,2:]######这里需要到时候调整，按label的位置和特征位置\r\n        # vaild_x_ctr_fea=np.delete(vaild_x_ctr_fea, -1, axis=1)######需要把类别型特征去掉\r\n        # ##########################\r\n        # ###########################这是类别型特征\r\n        # vaild_x_tfidf=vaild_x[:,-1][:,None]\r\n        # #########################################\r\n\r\n\r\n        feed_dict = {model.embedded_x_query: vaild_x_query,\r\n                     model.embedded_x_title: vaild_x_title,\r\n                     model.y_inputs: vaild_y,\r\n                     model.batch_size: vaild_x.shape[0], \r\n                     model.training: False, \r\n                     model.keep_prob: 1}\r\n        \r\n        \r\n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%5000==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n        saver = tf.train.Saver(max_to_keep = 3,var_list=tf.global_variables())\r\n        saver = saver.save(sess, \"/home/kesci/work/tf_model_textcnn_rcnn/cnn\", global_step=step)","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"35F24CA4E70747E5AC53E7A3618245B6","scrolled":false}},{"metadata":{"id":"4E9D4C7625524EB2920AA5029CE124DC"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}