{"cells":[{"outputs":[],"execution_count":null,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"19C9445B4CF44D7B980D31E23C6B1EBC"}},{"outputs":[],"execution_count":null,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"E097F1C2A9F047CDBC7388BDC8565517"}},{"outputs":[],"execution_count":null,"source":"# 查看当前kernerl下的package\n!pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"AA72A55608EA4E9C880D184A33539F94"}},{"outputs":[],"execution_count":null,"source":"!killall python","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"A6E1E6D09CC34C7B900E56E56EC2753A","scrolled":false}},{"metadata":{"id":"8E216CD8D8564ED09280EF967E9C71F8","collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"开始加载数据\n加载数据完成\ntrain starting\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","name":"stderr"},{"output_type":"stream","text":"start\nstep 10,loss:0.478304\nstep 20,loss:0.482207\nstep 30,loss:0.48416\nstep 40,loss:0.48709\nstep 50,loss:0.489043\nstep 60,loss:0.457793\nstep 70,loss:0.480254\nstep 80,loss:0.48709\nstep 90,loss:0.483184\nstep 100,loss:0.486113\nstep 110,loss:0.477324\nstep 120,loss:0.470488\nstep 130,loss:0.483184\nstep 140,loss:0.486113\nstep 150,loss:0.463652\nstep 160,loss:0.470488\nstep 170,loss:0.495879\nstep 180,loss:0.469512\nstep 190,loss:0.508574\nstep 200,loss:0.469512\nstep 210,loss:0.491973\nstep 220,loss:0.488066\nstep 230,loss:0.483184\nstep 240,loss:0.49002\nstep 250,loss:0.478301\nstep 260,loss:0.485137\nstep 270,loss:0.463652\nstep 280,loss:0.475371\nstep 290,loss:0.48416\nstep 300,loss:0.468535\nstep 310,loss:0.480254\nstep 320,loss:0.48709\nstep 330,loss:0.477324\nstep 340,loss:0.479277\nstep 350,loss:0.461699\nstep 360,loss:0.48709\nstep 370,loss:0.479277\nstep 380,loss:0.500762\nstep 390,loss:0.48416\nstep 400,loss:0.48416\nstep 410,loss:0.472442\nstep 420,loss:0.48416\nstep 430,loss:0.488066\nstep 440,loss:0.475372\nstep 450,loss:0.470488\nstep 460,loss:0.485137\nstep 470,loss:0.48123\nstep 480,loss:0.48123\nstep 490,loss:0.473418\nstep 500,loss:0.460723\n距离训练完一整轮剩余时间： 557.4273482713849  分钟\nper_train_loss_avg: 0.48190095436573027 **********\nvaild_loss: 0.4860618\nstep 510,loss:0.476348\nstep 520,loss:0.48123\nstep 530,loss:0.489043\nstep 540,loss:0.486113\nstep 550,loss:0.478301\nstep 560,loss:0.516387\nstep 570,loss:0.485137\nstep 580,loss:0.462676\nstep 590,loss:0.500762\nstep 600,loss:0.482207\nstep 610,loss:0.478301\nstep 620,loss:0.489043\nstep 630,loss:0.500762\nstep 640,loss:0.494902\nstep 650,loss:0.488066\nstep 660,loss:0.485137\nstep 670,loss:0.479277\nstep 680,loss:0.471465\nstep 690,loss:0.465605\nstep 700,loss:0.462676\nstep 710,loss:0.471465\nstep 720,loss:0.490996\nstep 730,loss:0.482207\nstep 740,loss:0.474395\nstep 750,loss:0.48709\nstep 760,loss:0.469512\nstep 770,loss:0.468535\nstep 780,loss:0.477324\nstep 790,loss:0.48416\nstep 800,loss:0.494902\nstep 810,loss:0.483184\nstep 820,loss:0.49002\nstep 830,loss:0.478301\nstep 840,loss:0.479277\nstep 850,loss:0.482207\nstep 860,loss:0.48123\nstep 870,loss:0.456816\nstep 880,loss:0.476348\nstep 890,loss:0.469512\nstep 900,loss:0.476348\nstep 910,loss:0.493926\nstep 920,loss:0.496855\nstep 930,loss:0.48123\nstep 940,loss:0.482207\nstep 950,loss:0.482207\nstep 960,loss:0.499785\nstep 970,loss:0.48123\nstep 980,loss:0.501738\nstep 990,loss:0.472441\nstep 1000,loss:0.495879\n距离训练完一整轮剩余时间： 552.0027325636397  分钟\nper_train_loss_avg: 0.481871121108532 **********\nvaild_loss: 0.4862618\nstep 1010,loss:0.485137\nstep 1020,loss:0.490996\nstep 1030,loss:0.483184\nstep 1040,loss:0.485137\nstep 1050,loss:0.490996\nstep 1060,loss:0.493926\nstep 1070,loss:0.497832\nstep 1080,loss:0.473418\nstep 1090,loss:0.483184\nstep 1100,loss:0.501738\nstep 1110,loss:0.478301\nstep 1120,loss:0.475371\nstep 1130,loss:0.504668\nstep 1140,loss:0.482207\nstep 1150,loss:0.492949\nstep 1160,loss:0.48123\nstep 1170,loss:0.478301\nstep 1180,loss:0.461699\nstep 1190,loss:0.471465\nstep 1200,loss:0.491973\nstep 1210,loss:0.49002\nstep 1220,loss:0.453887\nstep 1230,loss:0.468535\nstep 1240,loss:0.489043\nstep 1250,loss:0.477324\nstep 1260,loss:0.465605\nstep 1270,loss:0.494902\nstep 1280,loss:0.488066\nstep 1290,loss:0.486113\nstep 1300,loss:0.474395\nstep 1310,loss:0.49002\nstep 1320,loss:0.48123\nstep 1330,loss:0.485137\nstep 1340,loss:0.472441\nstep 1350,loss:0.490996\nstep 1360,loss:0.482207\nstep 1370,loss:0.473418\nstep 1380,loss:0.485137\nstep 1390,loss:0.48416\nstep 1400,loss:0.486113\nstep 1410,loss:0.493926\nstep 1420,loss:0.472441\nstep 1430,loss:0.472441\nstep 1440,loss:0.471465\nstep 1450,loss:0.473418\nstep 1460,loss:0.473418\nstep 1470,loss:0.475371\nstep 1480,loss:0.471465\nstep 1490,loss:0.466582\nstep 1500,loss:0.48416\n距离训练完一整轮剩余时间： 540.9265773177146  分钟\nper_train_loss_avg: 0.4826269806623459 **********\nvaild_loss: 0.48226172\nstep 1510,loss:0.471465\nstep 1520,loss:0.500762\nstep 1530,loss:0.469512\nstep 1540,loss:0.457793\nstep 1550,loss:0.499785\nstep 1560,loss:0.489043\nstep 1570,loss:0.469512\nstep 1580,loss:0.483184\nstep 1590,loss:0.49002\nstep 1600,loss:0.482207\nstep 1610,loss:0.465605\nstep 1620,loss:0.478301\nstep 1630,loss:0.49002\nstep 1640,loss:0.463652\nstep 1650,loss:0.496855\nstep 1660,loss:0.468535\nstep 1670,loss:0.472441\nstep 1680,loss:0.489043\nstep 1690,loss:0.485137\nstep 1700,loss:0.476348\nstep 1710,loss:0.491973\nstep 1720,loss:0.473418\nstep 1730,loss:0.477324\nstep 1740,loss:0.477324\nstep 1750,loss:0.504668\nstep 1760,loss:0.460723\nstep 1770,loss:0.469512\nstep 1780,loss:0.494902\nstep 1790,loss:0.499785\nstep 1800,loss:0.513457\nstep 1810,loss:0.489043\nstep 1820,loss:0.471465\nstep 1830,loss:0.501738\nstep 1840,loss:0.496855\nstep 1850,loss:0.469512\nstep 1860,loss:0.466582\nstep 1870,loss:0.475371\nstep 1880,loss:0.478301\nstep 1890,loss:0.48416\nstep 1900,loss:0.461699\nstep 1910,loss:0.494902\nstep 1920,loss:0.483184\nstep 1930,loss:0.476348\nstep 1940,loss:0.48123\nstep 1950,loss:0.468535\nstep 1960,loss:0.495879\nstep 1970,loss:0.465605\nstep 1980,loss:0.471465\nstep 1990,loss:0.501738\nstep 2000,loss:0.497832\n距离训练完一整轮剩余时间： 549.382040501386  分钟\nper_train_loss_avg: 0.48233010280132294 **********\nvaild_loss: 0.47306177\nstep 2010,loss:0.488066\nstep 2020,loss:0.485137\nstep 2030,loss:0.461699\nstep 2040,loss:0.471465\nstep 2050,loss:0.482207\nstep 2060,loss:0.482207\nstep 2070,loss:0.511504\nstep 2080,loss:0.479277\nstep 2090,loss:0.48709\nstep 2100,loss:0.474395\nstep 2110,loss:0.472441\nstep 2120,loss:0.477324\nstep 2130,loss:0.459746\nstep 2140,loss:0.459746\nstep 2150,loss:0.48123\nstep 2160,loss:0.474395\nstep 2170,loss:0.498809\nstep 2180,loss:0.473418\nstep 2190,loss:0.505645\nstep 2200,loss:0.482207\nstep 2210,loss:0.483184\nstep 2220,loss:0.48416\nstep 2230,loss:0.469512\nstep 2240,loss:0.494902\nstep 2250,loss:0.465605\nstep 2260,loss:0.491973\nstep 2270,loss:0.485137\nstep 2280,loss:0.496855\nstep 2290,loss:0.483184\nstep 2300,loss:0.483184\nstep 2310,loss:0.506621\nstep 2320,loss:0.488066\nstep 2330,loss:0.494902\nstep 2340,loss:0.465605\nstep 2350,loss:0.476348\nstep 2360,loss:0.479277\nstep 2370,loss:0.488066\nstep 2380,loss:0.486113\nstep 2390,loss:0.48123\nstep 2400,loss:0.470488\nstep 2410,loss:0.490996\nstep 2420,loss:0.499785\nstep 2430,loss:0.48709\nstep 2440,loss:0.457793\nstep 2450,loss:0.48416\nstep 2460,loss:0.470488\nstep 2470,loss:0.495879\nstep 2480,loss:0.468535\nstep 2490,loss:0.48123\nstep 2500,loss:0.466582\n距离训练完一整轮剩余时间： 548.3467682749033  分钟\nper_train_loss_avg: 0.48200002443790435 **********\nvaild_loss: 0.4674617\nstep 2510,loss:0.482207\nstep 2520,loss:0.475371\nstep 2530,loss:0.473418\nstep 2540,loss:0.48709\nstep 2550,loss:0.470488\nstep 2560,loss:0.471465\nstep 2570,loss:0.491973\nstep 2580,loss:0.491973\nstep 2590,loss:0.49002\nstep 2600,loss:0.478301\nstep 2610,loss:0.482207\nstep 2620,loss:0.473418\n","name":"stdout"}],"source":"# -*- coding:utf-8 -*-\r\n\r\nimport os\r\n# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import rnn\r\n\r\nimport tensorflow.contrib.layers as layers\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import StratifiedShuffleSplit\r\n\r\n\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib.rnn import LSTMCell, DropoutWrapper\r\n\r\n\r\n\r\nimport tensorflow as tf\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom gensim.models import Word2Vec\r\n\r\n\"\"\"wd_5_bigru_cnn\r\n\r\n在论文 Recurrent Convolutional Neural Networks for Text Classification 中。\r\n\r\n使用 BiRNN 处理，将每个时刻的隐藏状态和原输入拼起来，在进行 max_pooling 操作。\r\n\r\n这里有些不同，首先也是使用 bigru 得到每个时刻的，将每个时刻的隐藏状态和原输入拼起来；\r\n\r\n然后使用输入到 TextCNN 网络中。\r\n\r\n\"\"\"\r\n\r\n\r\n\r\n\r\n\r\nclass Settings(object):\r\n\r\n    def __init__(self):\r\n\r\n\r\n        self.model_name = 'wd_1_2_cnn_max'\r\n        self.query_len = 15\r\n        self.title_len = 15\r\n        self.filter_sizes = [2, 3, 4, 5]\r\n        self.n_filter = 200\r\n        self.fc_hidden_size = 512\r\n        self.n_class = 1\r\n        self.train_batch_size=1024\r\n        self.vaild_batch_size=5000\r\n        self.lr=0.0025\r\n        self.decay_step=20000\r\n        self.decay_rate=0.8\r\n        self.rnn_hidden_units=100\r\n        self.rnn_hidden_size=256\r\n\r\n\r\n\r\n\r\n\r\nclass TEXTCNN(object):\r\n\r\n    def __init__(self,  settings):\r\n\r\n\r\n        self.model_name = settings.model_name\r\n        self.query_len = settings.query_len\r\n        self.title_len = settings.title_len\r\n        self.filter_sizes = settings.filter_sizes\r\n        self.n_filter = settings.n_filter\r\n        self.n_filter_total = self.n_filter * len(self.filter_sizes)\r\n        self.n_class = settings.n_class\r\n        self.fc_hidden_size = settings.fc_hidden_size\r\n        self.rnn_hidden_units = settings.rnn_hidden_units\r\n        self._global_step = tf.Variable(0, trainable=False, name='Global_Step')\r\n        self.update_emas = list()\r\n        # placeholders\r\n        self.training = tf.placeholder(tf.bool)\r\n        self._keep_prob = tf.placeholder(tf.float32, [])\r\n        self._batch_size = tf.placeholder(tf.int32, [])\r\n        self.embedding_size=100\r\n        self.w=0\r\n        self.rnn_hidden_size=settings.rnn_hidden_size\r\n\r\n        with tf.name_scope('Inputs'):\r\n            self.embedded_x_query=tf.placeholder(tf.float32,[None,self.query_len,self.embedding_size])#h r\r\n            self.embedded_x_title=tf.placeholder(tf.float32,[None,self.title_len,self.embedding_size])# r\r\n            self.query_real_length = tf.placeholder(tf.int32, [None], name='a_length')\r\n            self.title_real_length = tf.placeholder(tf.int32, [None], name='b_length')\r\n            self._y_inputs = tf.placeholder(tf.float32, [None, self.n_class], name='y_input')\r\n\r\n\r\n        with tf.variable_scope('out_layer'):\r\n\r\n            self.sim = self.esim_foward()\r\n\r\n            self._y_pred = self.sim \r\n        \r\n\r\n        with tf.name_scope('loss'):\r\n            self._loss = tf.reduce_mean(\r\n                tf.nn.sigmoid_cross_entropy_with_logits(logits=self._y_pred, labels=self._y_inputs))\r\n\r\n    @property\r\n    def tst(self):\r\n        return self._tst\r\n\r\n    @property\r\n    def keep_prob(self):\r\n        return self._keep_prob\r\n\r\n    @property\r\n    def batch_size(self):\r\n        return self._batch_size\r\n\r\n    @property\r\n    def global_step(self):\r\n        return self._global_step\r\n\r\n    @property\r\n    def X1_inputs(self):\r\n        return self._X1_inputs\r\n\r\n    @property\r\n    def X2_inputs(self):\r\n        return self._X2_inputs\r\n\r\n    @property\r\n    def y_inputs(self):\r\n        return self._y_inputs\r\n\r\n    @property\r\n    def y_pred(self):\r\n        return self._y_pred\r\n\r\n    @property\r\n    def loss(self):\r\n        return self._loss\r\n\r\n    def weight_variable(self, shape, name):\r\n        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        return tf.Variable(initial, name=name)\r\n    def bias_variable(self, shape, name):\r\n        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n        initial = tf.constant(0.1, shape=shape)\r\n        return tf.Variable(initial, name=name)\r\n\r\n    def esim_foward(self):\r\n        a_bar, b_bar = self._inputEncodingBlock('input_encoding')\r\n        m_a, m_b = self._localInferenceBlock(a_bar, b_bar, 'local_inference')\r\n        logits = self._compositionBlock(m_a, m_b, self.rnn_hidden_units, 'composition')\r\n        return logits\r\n\r\n    # feed forward unit\r\n\r\n    def _feedForwardBlock(self, inputs, hidden_dims, num_units, scope, isReuse = False, initializer = None):\r\n        \"\"\"\r\n        :param inputs: tensor with shape (batch_size, 4 * 2 * hidden_size)\r\n        :param scope: scope name\r\n        :return: output: tensor with shape (batch_size, num_units)\r\n        \"\"\"\r\n        with tf.variable_scope(scope, reuse = isReuse):\r\n            if initializer is None:\r\n                initializer = tf.random_normal_initializer(0.0, 0.1)\r\n\r\n            with tf.variable_scope('feed_foward_layer1'):\r\n                inputs = tf.nn.dropout(inputs, self.keep_prob)\r\n                outputs = tf.layers.dense(inputs, hidden_dims, tf.nn.relu, kernel_initializer = initializer)\r\n            with tf.variable_scope('feed_foward_layer2'):\r\n                outputs = tf.nn.dropout(outputs, self.keep_prob)\r\n                results = tf.layers.dense(outputs, num_units, tf.nn.tanh, kernel_initializer = initializer)\r\n                return results\r\n\r\n    # biLSTM unit\r\n    def _biLSTMBlock(self, inputs, num_units, scope, seq_len = None, isReuse = False):\r\n        with tf.variable_scope(scope, reuse = isReuse):\r\n            lstmCell = LSTMCell(num_units = num_units)\r\n            dropLSTMCell = lambda: DropoutWrapper(lstmCell, output_keep_prob = self.keep_prob)\r\n            fwLSTMCell, bwLSTMCell = dropLSTMCell(), dropLSTMCell()\r\n            output = tf.nn.bidirectional_dynamic_rnn(cell_fw = fwLSTMCell,\r\n                                                     cell_bw = bwLSTMCell,\r\n                                                     inputs = inputs,\r\n                                                     sequence_length = seq_len,\r\n                                                     dtype = tf.float32)\r\n            return output\r\n    # input encoding block (\"3.1 Input Encoding\" in paper)\r\n    def _inputEncodingBlock(self, scope):\r\n        \"\"\"\r\n        :param scope: scope name\r\n\r\n        embedded_x_query, embedded_x_title: tensor with shape (batch_size, seq_length, embedding_size)\r\n\r\n        :return: a_bar: tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n                 b_bar: tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n        \"\"\"\r\n\r\n        with tf.variable_scope(scope):\r\n            # a_bar = BiLSTM(a, i) (1)\r\n            # b_bar = BiLSTM(b, i) (2)\r\n            outputsPremise, finalStatePremise = self._biLSTMBlock(self.embedded_x_query, self.rnn_hidden_units,\r\n                                                                  'biLSTM', self.query_real_length)\r\n            outputsHypothesis, finalStateHypothesis = self._biLSTMBlock(self.embedded_x_title, self.rnn_hidden_units,\r\n                                                              'biLSTM', self.title_real_length,\r\n                                                              isReuse = True)\r\n            a_bar = tf.concat(outputsPremise, axis=2)\r\n            b_bar = tf.concat(outputsHypothesis, axis=2)\r\n            return a_bar, b_bar\r\n\r\n\r\n\r\n    # local inference block (\"3.2 Local Inference Modeling\" in paper)\r\n    def _localInferenceBlock(self, a_bar, b_bar, scope):\r\n        \"\"\"\r\n        :param a_bar: tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n        :param b_bar: tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n        :param scope: scope name\r\n\r\n        attentionWeights: attention matrix, tensor with shape (batch_size, seq_length, seq_length)\r\n        attentionSoft_a, attentionSoft_b: using Softmax at two directions, tensor with shape (batch_size, seq_length, seq_length)\r\n        a_hat, b_hat: context vectors, tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n        a_diff, b_diff: difference of a_bar and a_hat, b_bar and b_hat, tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n        a_mul, b_mul: hadamard product of a_bar and a_hat, b_bar and b_hat, tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n        :return: m_a: concat of [a_bar, a_hat, a_diff, a_mul], tensor with shape (batch_size, seq_length, 4 * 2 * hidden_size)\r\n                 m_b: concat of [b_bar, b_hat, b_diff, b_mul], tensor with shape (batch_size, seq_length, 4 * 2 * hidden_size)\r\n        \"\"\"\r\n        with tf.variable_scope(scope):\r\n            # e = a_bar.T * b_bar (11)\r\n            attentionWeights = tf.matmul(a_bar, tf.transpose(b_bar, [0, 2, 1]))\r\n\r\n\r\n            # a_hat = softmax(e) * b_bar (12)\r\n            # b_hat = softmax(e) * a_bar (13)\r\n            attentionSoft_a = tf.nn.softmax(attentionWeights)\r\n            attentionSoft_b = tf.nn.softmax(tf.transpose(attentionWeights))\r\n            attentionSoft_b = tf.transpose(attentionSoft_b)\r\n\r\n\r\n            a_hat = tf.matmul(attentionSoft_a, b_bar)\r\n            b_hat = tf.matmul(attentionSoft_b, a_bar)\r\n\r\n            a_diff = tf.subtract(a_bar, a_hat)\r\n            a_mul = tf.multiply(a_bar, a_hat)\r\n\r\n\r\n            b_diff = tf.subtract(b_bar, b_hat)\r\n            b_mul = tf.multiply(b_bar, b_hat)\r\n\r\n            # m_a = [a_bar, a_hat, a_bar - a_hat, a_bar 'dot' a_hat] (14)\r\n            # m_b = [b_bar, b_hat, b_bar - b_hat, b_bar 'dot' b_hat] (15)\r\n            m_a = tf.concat([a_bar, a_hat, a_diff, a_mul], axis = 2)\r\n            m_b = tf.concat([b_bar, b_hat, b_diff, b_mul], axis = 2)\r\n\r\n            return m_a, m_b\r\n\r\n    # composition block (\"3.3 Inference Composition\" in paper)\r\n    def _compositionBlock(self, m_a, m_b, hiddenSize, scope):\r\n        \"\"\"\r\n        :param m_a: concat of [a_bar, a_hat, a_diff, a_mul], tensor with shape (batch_size, seq_length, 4 * 2 * hidden_size)\r\n        :param m_b: concat of [b_bar, b_hat, b_diff, b_mul], tensor with shape (batch_size, seq_length, 4 * 2 * hidden_size)\r\n        :param hiddenSize: biLSTM cell's hidden states size\r\n        :param scope: scope name\r\n        outputV_a, outputV_b: hidden states of biLSTM, tuple (forward LSTM cell, backward LSTM cell)\r\n        v_a, v_b: concate of biLSTM hidden states, tensor with shape (batch_size, seq_length, 2 * hidden_size)\r\n        v_a_avg, v_b_avg: timestep (axis = seq_length) average of v_a, v_b, tensor with shape (batch_size, 2 * hidden_size)\r\n        v_a_max, v_b_max: timestep (axis = seq_length) max value of v_a, v_b, tensor with shape (batch_size, 2 * hidden_size)\r\n        v: concat of [v_a_avg, v_b_avg, v_a_max, v_b_max], tensor with shape (batch_size, 4 * 2 * hidden_size)\r\n\r\n        :return: y_hat: output of feed forward layer, tensor with shape (batch_size, n_class)\r\n        \"\"\"\r\n        with tf.variable_scope(scope):\r\n            outputV_a, finalStateV_a = self._biLSTMBlock(m_a, hiddenSize, 'biLSTM')\r\n            outputV_b, finalStateV_b = self._biLSTMBlock(m_b, hiddenSize, 'biLSTM', isReuse = True)\r\n            v_a = tf.concat(outputV_a, axis = 2)\r\n            v_b = tf.concat(outputV_b, axis = 2)\r\n\r\n\r\n\r\n            # v_{a,avg} = \\sum_{i=1}^l_a \\frac{v_a,i}{l_a}, v_{a,max} = \\max_{i=1} ^ l_a v_{a,i} (18)\r\n            # v_{b,avg} = \\sum_{j=1}^l_b \\frac{v_b,j}{l_b}, v_{b,max} = \\max_{j=1} ^ l_b v_{b,j} (19)\r\n            v_a_avg = tf.reduce_mean(v_a, axis = 1)\r\n            v_b_avg = tf.reduce_mean(v_b, axis = 1)\r\n            v_a_max = tf.reduce_max(v_a, axis = 1)\r\n            v_b_max = tf.reduce_max(v_b, axis = 1)\r\n\r\n            # v = [v_{a,avg}; v_{a,max}; v_{b,avg}; v_{b_max}] (20)\r\n            v = tf.concat([v_a_avg, v_a_max, v_b_avg, v_b_max], axis = 1)\r\n\r\n            y_hat = self._feedForwardBlock(v, self.rnn_hidden_size, self.n_class, 'feed_forward')\r\n            return y_hat\r\n\r\n\r\n    def cnn_inference(self, X_inputs, n_step,name):\r\n        \"\"\"TextCNN 模型。\r\n        Args:\r\n            X_inputs: tensor.shape=(batch_size, n_step)\r\n        Returns:\r\n            title_outputs: tensor.shape=(batch_size, self.n_filter_total)\r\n        \"\"\"\r\n        inputs = X_inputs\r\n        inputs = tf.expand_dims(inputs, -1)\r\n        pooled_outputs = list()\r\n        for i, filter_size in enumerate(self.filter_sizes):\r\n            with tf.variable_scope((\"conv-maxpool-%s\" % filter_size)+name):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, self.embedding_size, 1, self.n_filter]\r\n                W_filter = self.weight_variable(shape=filter_shape, name='W_filter')\r\n                beta = self.bias_variable(shape=[self.n_filter], name='beta_filter')\r\n                conv = tf.nn.conv2d(inputs, W_filter, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\r\n                conv_bn=tf.layers.batch_normalization(conv,training=self.training,name=name)\r\n                # Apply nonlinearity, batch norm scaling is not useful with relus\r\n                # batch norm offsets are used instead of biases,使用 BN 层的 offset，不要 biases\r\n                h = tf.nn.relu(conv_bn, name=\"relu\")\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(h, ksize=[1, n_step - filter_size + 1, 1, 1],\r\n                                        strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n        h_pool = tf.concat(pooled_outputs, 3)\r\n        h_pool_flat = tf.reshape(h_pool, [-1, self.n_filter_total])\r\n        return h_pool_flat  # shape = [batch_size, self.n_filter_total]\r\n\r\n\r\n\r\ndef loader(data_store_path=''):############\r\n    # all_data=pd.read_csv(data_store_path+\"data_sets/train_feature_second_no_query_and_title_data.csv\")\r\n    # query_title=pd.read_csv(data_store_path+\"data_sets/train_data.csv\",usecols=['query','title'])\r\n    print(\"开始加载数据\")\r\n    all_data=pd.read_pickle(data_store_path+\"/home/kesci/train_data30000w-32000w.pickle\")\r\n    data_split=StratifiedShuffleSplit(n_splits=2,test_size=0.001,random_state=1)\r\n    train_index,vaild_index=data_split.split(all_data['label'],all_data['label']).__next__()\r\n    train_data=all_data.iloc[train_index]\r\n    vaild_data=all_data.iloc[vaild_index]\r\n    print(\"加载数据完成\")\r\n    return train_data,vaild_data\r\n\r\ndef batch_iter(data_x,data_y, batch_size, num_epochs, shuffle=True):#这个就是产生batch的，可以直接用\r\n    \"\"\"\r\n    Generates a batch iterator for a dataset.\r\n    \"\"\"\r\n    # data_x=data_x.tolist()\r\n    data=list(zip(data_x,data_y))\r\n    data=np.array(data)\r\n    data_size = len(data)\r\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\r\n    for epoch in range(num_epochs):\r\n        # Shuffle the data at each epoch\r\n        if shuffle:#如果为true 则代表允许随机取数据\r\n            shuffle_indices = np.random.permutation(np.arange(data_size))\r\n            shuffled_data = data[shuffle_indices]#随机取数据\r\n        else:\r\n            shuffled_data = data\r\n        for batch_num in range(num_batches_per_epoch):\r\n            start_index = batch_num * batch_size\r\n            end_index = min((batch_num + 1) * batch_size, data_size)\r\n            yield shuffled_data[start_index:end_index]#而不用一个个加到列表里了。这是一个batch，嗯哼\r\n            #取到的维度为（batchsize,2） y为(batchsize,1) 里面存的是序号\r\n\r\nfrom gensim import models\r\nimport pickle\r\nfw=open('dictionary/dictionary.pickle','rb')\r\ndictionary=pickle.load(fw)\r\nimport pickle\r\nfw=open('tfidf_model/tfidf_model.pickle','rb')\r\ntfidf_model=pickle.load(fw)\r\n\r\ndef get_w2v_array(word_list,max_len):\r\n    single_sentence_tfidf=tfidf_model[dictionary.doc2bow(word_list)]\r\n    # single_sentence_tfidf={dictionary[word_id_tfidf[0]]:word_id_tfidf[1] for word_id_tfidf in single_sentence_tfidf}\r\n    array = np.zeros((max_len, 100))\r\n    if len(word_list)<=max_len:\r\n        for i in range(len(word_list)):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n                # try:\r\n                #     array[i][:]*=single_sentence_tfidf[str(word_list[i])]\r\n\r\n                # except:\r\n                #     pass\r\n                \r\n    else:\r\n        for i in range(max_len):\r\n            if str(word_list[i]) in word_wv.vocab.keys():\r\n                array[i][:] = word_wv[str(word_list[i])]\r\n                # try:\r\n                #     array[i][:]*=single_sentence_tfidf[str(word_list[i])]\r\n                # except:\r\n                #     pass\r\n    return array\r\n\r\nsettings = Settings()\r\n\r\ntrain_batches,vaild_data=loader()\r\n\r\ntrain_batches=np.array(train_batches)\r\ntrain_batches_y=train_batches[:,2]\r\ntrain_batches_x=np.delete(train_batches, 2, axis=1)######这里是除了label以外的维数\r\n\r\n\r\nbatches=batch_iter(train_batches_x,train_batches_y,settings.train_batch_size,4)\r\n\r\nvaild_batches=np.array(vaild_data)\r\nvaild_batches_y=vaild_batches[:,2]\r\nvaild_batches_x=np.delete(vaild_batches, 2, axis=1)\r\nvaild_data=batch_iter(vaild_batches_x,vaild_batches_y,settings.vaild_batch_size,1000)\r\n\r\nprint(\"train starting\")\r\nw2v_model = Word2Vec.load('w2v_model3/w2v_all_data_model.txt')\r\nword_wv= w2v_model.wv\r\n\r\n\r\nsettings = Settings()\r\n\r\nsettings.all_features_num=train_batches_x.shape[1]-2###########自动设置数值型特征数量\r\nmodel = TEXTCNN( settings)\r\n\r\nlearning_rate = tf.train.exponential_decay(settings.lr, model.global_step, settings.decay_step,\r\n                                       settings.decay_rate, staircase=True)\r\noptimizer = tf.train.AdamOptimizer(learning_rate)\r\n\r\nupdate_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    grads_and_vars=optimizer.compute_gradients(model.loss)\r\n    train_op=optimizer.apply_gradients(grads_and_vars,global_step=model.global_step)\r\n\r\nconfig = tf.ConfigProto() \r\nconfig.gpu_options.allow_growth = True \r\nsess = tf.Session(config=config) #启动创建的模型\r\n# sess.run(tf.initialize_all_variables())\r\n\r\n# saver = tf.train.Saver(tf.global_variables())\r\nsaver = tf.train.Saver()\r\nsess.run(tf.global_variables_initializer())\r\n# saver.restore(sess, tf.train.latest_checkpoint('/home/kesci/work/tf_model_textcnn/'))\r\nper_train_loss_avg_list=[]\r\nper_vaild_loss_avg_list=[]\r\nprint(\"start\")\r\nparts_time=[]\r\nfor batch in batches:\r\n    from time import time\r\n    time1=time()\r\n    batch_x,batch_y=zip(*batch)\r\n    batch_y=np.array(batch_y)[:,None]\r\n    batch_x=[i.tolist() for i in batch_x]\r\n    batch_x=pd.DataFrame(batch_x)\r\n\r\n    batch_x[[0,1]]=batch_x[[0,1]].applymap(lambda x:x.split(' '))\r\n    query_real_length = batch_x[[0]].applymap(lambda x:len(x)).values.reshape(-1)\r\n    title_real_length = batch_x[[1]].applymap(lambda x:len(x)).values.reshape(-1)\r\n    batch_x=np.array(batch_x)\r\n    #####[None,title_len,embedding_size]\r\n    #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n    #query   title   title_count         title_ctr      label\r\n    batch_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=batch_x)\r\n    batch_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=batch_x)\r\n\r\n        \r\n    #     train_op=optimizer.apply_gradients(grads_and_vars,global_step=global_step)                              \r\n    feed_dict = {model.embedded_x_query: batch_x_query,\r\n                 model.embedded_x_title: batch_x_title,\r\n                 model.y_inputs: batch_y,\r\n                 model.batch_size: batch_x.shape[0], \r\n                 model.training: True, \r\n                 model.keep_prob: 0.8,\r\n                 model.query_real_length: query_real_length,\r\n                 model.title_real_length: title_real_length,\r\n                 }\r\n    \r\n    _,step,loss_out=sess.run([train_op,model.global_step,model.loss],feed_dict)\r\n    time2=time()\r\n    parts_time.append(time2-time1)\r\n    if step%10==0:\r\n        print(\"step {},loss:{:g}\".format(step,loss_out))              \r\n    per_train_loss_avg_list.append(loss_out)\r\n    \r\n    if step%(int(50000000/1004))==0:\r\n        print(\"batch ended\")\r\n    if step%500==0:\r\n        time_single_part=sum(parts_time)\r\n        ed_time=(50000000/settings.train_batch_size-step)/500*time_single_part\r\n        print(\"距离训练完一整轮剩余时间：\",ed_time/60,\" 分钟\")\r\n        parts_time=[]\r\n        vaild_x,vaild_y=zip(*(vaild_data.__next__()))\r\n        vaild_y=np.array(vaild_y)[:,None]\r\n        vaild_x=[i.tolist() for i in vaild_x]\r\n        vaild_x=pd.DataFrame(vaild_x)\r\n        vaild_x[[0,1]]=vaild_x[[0,1]].applymap(lambda x:x.split(' '))\r\n        query_real_length = vaild_x[[0]].applymap(lambda x:len(x)).values.reshape(-1)\r\n        title_real_length = vaild_x[[1]].applymap(lambda x:len(x)).values.reshape(-1)\r\n        vaild_x=np.array(vaild_x)\r\n    \r\n        #####[None,title_len,embedding_size]\r\n        #np.apply_along_axis(lambda x:[5,5,5,5,5] if x[0]==2 else x,axis=2,arr=a)\r\n        #query   title   title_count         title_ctr      label\r\n        \r\n        vaild_x_query=np.apply_along_axis(lambda x:get_w2v_array(x[0],settings.query_len),axis=1,arr=vaild_x)\r\n        vaild_x_title=np.apply_along_axis(lambda x:get_w2v_array(x[1],settings.title_len),axis=1,arr=vaild_x)\r\n\r\n\r\n        # #########################这是连续型特征\r\n        # vaild_x_ctr_fea=vaild_x[:,2:]######这里需要到时候调整，按label的位置和特征位置\r\n        # vaild_x_ctr_fea=np.delete(vaild_x_ctr_fea, -1, axis=1)######需要把类别型特征去掉\r\n        # ##########################\r\n        # ###########################这是类别型特征\r\n        # vaild_x_tfidf=vaild_x[:,-1][:,None]\r\n        # #########################################\r\n\r\n\r\n        feed_dict = {model.embedded_x_query: vaild_x_query,\r\n                     model.embedded_x_title: vaild_x_title,\r\n                     model.y_inputs: vaild_y,\r\n                     model.batch_size: vaild_x.shape[0], \r\n                     model.training: False, \r\n                     model.keep_prob: 1,\r\n                    model.query_real_length: query_real_length,\r\n                    model.title_real_length: title_real_length,\r\n                    }\r\n        \r\n        \r\n        step,loss_out=sess.run([model.global_step,model.loss],feed_dict)\r\n        per_vaild_loss_avg_list.append(loss_out)\r\n        per_train_loss_avg=sum(per_train_loss_avg_list)/len(per_train_loss_avg_list)\r\n        print(\"per_train_loss_avg:\",per_train_loss_avg,\"**********\")\r\n        per_train_loss_avg_list=[]\r\n        print(\"vaild_loss:\",loss_out)\r\n    if step%5000==0:\r\n        per_vaild_loss_avg=sum(per_vaild_loss_avg_list)/len(per_vaild_loss_avg_list)\r\n        per_vaild_loss_avg_list=[]\r\n        print(\"per_vaild_loss_avg:\",per_vaild_loss_avg,\"************\")\r\n        saver = tf.train.Saver(max_to_keep = 3,var_list=tf.global_variables())\r\n        saver = saver.save(sess, \"/home/kesci/work/tf_model_textcnn_esim/esim\", global_step=step)","execution_count":1},{"metadata":{"id":"32D033181F2F4AE59BA4D1F00ACBEC79","collapsed":true,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"开始加载数据\n加载数据完成\ntrain starting\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n","name":"stderr"},{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\nInstructions for updating:\nUse `tf.global_variables_initializer` instead.\nstep 10,loss:0.482207\nstep 20,loss:0.482207\nstep 30,loss:0.509551\nstep 40,loss:0.51248\nstep 50,loss:0.49002\nstep 60,loss:0.500762\nstep 70,loss:0.516387\nstep 80,loss:0.508574\nstep 90,loss:0.496855\nstep 100,loss:0.509551\nstep 110,loss:0.488066\nstep 120,loss:0.497832\nstep 130,loss:0.511504\nstep 140,loss:0.48416\nstep 150,loss:0.482207\nstep 160,loss:0.469512\nstep 170,loss:0.501738\nstep 180,loss:0.482207\nstep 190,loss:0.485137\nstep 200,loss:0.507598\nstep 210,loss:0.471465\nstep 220,loss:0.514434\nstep 230,loss:0.497832\nstep 240,loss:0.501738\nstep 250,loss:0.500762\nstep 260,loss:0.501738\nstep 270,loss:0.498809\nstep 280,loss:0.483184\nstep 290,loss:0.506621\nstep 300,loss:0.506621\nstep 310,loss:0.482207\nstep 320,loss:0.489043\nstep 330,loss:0.501738\nstep 340,loss:0.477324\nstep 350,loss:0.496855\nstep 360,loss:0.492949\nstep 370,loss:0.506621\nstep 380,loss:0.503691\nstep 390,loss:0.507598\nstep 400,loss:0.478301\nstep 410,loss:0.483184\nstep 420,loss:0.500762\nstep 430,loss:0.491973\nstep 440,loss:0.504668\nstep 450,loss:0.51834\nstep 460,loss:0.502715\nstep 470,loss:0.493926\nstep 480,loss:0.492949\nstep 490,loss:0.510527\nstep 500,loss:0.514434\n距离训练完一整轮剩余时间： 198.3925435256213  分钟\nper_train_loss_avg: 0.4947784979939461 **********\nvaild_loss: 0.49416992\nstep 510,loss:0.508574\nstep 520,loss:0.497832\nstep 530,loss:0.482207\nstep 540,loss:0.486113\nstep 550,loss:0.486113\nstep 560,loss:0.485137\nstep 570,loss:0.494902\nstep 580,loss:0.49002\nstep 590,loss:0.497832\nstep 600,loss:0.497832\nstep 610,loss:0.508574\nstep 620,loss:0.499785\nstep 630,loss:0.503691\nstep 640,loss:0.474395\nstep 650,loss:0.498809\nstep 660,loss:0.485137\nstep 670,loss:0.486113\nstep 680,loss:0.496855\nstep 690,loss:0.491973\nstep 700,loss:0.503691\nstep 710,loss:0.495879\nstep 720,loss:0.497832\nstep 730,loss:0.499785\nstep 740,loss:0.51248\nstep 750,loss:0.508574\nstep 760,loss:0.507598\nstep 770,loss:0.480254\nstep 780,loss:0.500762\nstep 790,loss:0.48709\nstep 800,loss:0.486113\nstep 810,loss:0.504668\nstep 820,loss:0.483184\nstep 830,loss:0.501738\nstep 840,loss:0.497832\nstep 850,loss:0.496855\nstep 860,loss:0.510527\nstep 870,loss:0.513457\nstep 880,loss:0.488066\nstep 890,loss:0.501738\nstep 900,loss:0.51834\nstep 910,loss:0.525176\nstep 920,loss:0.491973\n","name":"stdout"}],"source":"","execution_count":2},{"metadata":{"id":"C34848B956094FECB6B4D17D552E4ACE"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}